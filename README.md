# üß† Knowledge Base Refactoring - Sistema Inteligente

![Architecture Diagram](deepseek_mermaid_20250728_e36e87.png)

Um sistema completo de base de conhecimento com IA generativa usando **Mistral 7B**, arquitetura moderna e princ√≠pios **SOLID** de desenvolvimento de software.

## üöÄ Como Executar o Projeto

### Frontend (Next.js 15 + React 19)
```bash
cd frontend
npm install
npm run dev  # Desenvolvimento com Turbopack
# ou
npm run build && npm start  # Produ√ß√£o
```

### AI Services (Docker Compose)
```bash
# Subir todos os servi√ßos
docker-compose up --build

# Verificar sa√∫de dos servi√ßos
curl http://localhost:8001/health  # Document Processor
curl http://localhost:8002/health  # RAG Service  
curl http://localhost:8003/health  # Mistral Service
```

### Configura√ß√£o Ollama (Necess√°rio)
```bash
# Instalar e configurar Ollama
curl -fsSL https://ollama.ai/install.sh | sh
ollama serve
ollama pull mistral:latest
```

---

# ü§ñ AI Services - Documenta√ß√£o T√©cnica

## üìã Vis√£o Geral da Arquitetura

O sistema AI Services √© composto por **tr√™s microservi√ßos especializados** que trabalham em conjunto para fornecer capacidades de **processamento de documentos**, **busca sem√¢ntica** e **gera√ß√£o de respostas inteligentes**.

```mermaid
graph TB
    Frontend[Frontend Next.js] --> API[API Gateway/Proxy]
    API --> RAG[RAG Service :8002]
    RAG --> DOC[Document Processor :8001]
    RAG --> MISTRAL[Mistral Service :8003]
    DOC --> QDRANT[(Qdrant Vector DB)]
    MISTRAL --> OLLAMA[Ollama + Mistral 7B]
    
    subgraph "AI Services Ecosystem"
        DOC
        RAG
        MISTRAL
        QDRANT
        OLLAMA
    end
```

### üéØ Princ√≠pios Arquiteturais

- **üîß SOLID Principles**: Cada servi√ßo segue responsabilidade √∫nica, aberto/fechado, etc.
- **üîÄ Microservices**: Servi√ßos independentes com APIs bem definidas
- **üèóÔ∏è Clean Architecture**: Separa√ß√£o clara entre dom√≠nio, aplica√ß√£o e infraestrutura
- **üöÄ Async/Await**: Opera√ß√µes n√£o-bloqueantes para alta performance
- **üìä Observabilidade**: Logs estruturados e m√©tricas de performance

---

## üßæ 1. Document Processor Service

**Porta:** `:8001` | **Responsabilidade:** Processamento e indexa√ß√£o de documentos

### üìö Bibliotecas Core

```python
# Core Dependencies
fastapi>=0.116.1          # Web framework async
uvicorn>=0.35.0           # ASGI server
pydantic>=2.11.7          # Data validation
httpx>=0.28.1             # HTTP client async

# Document Processing
pypdf2>=3.0.1             # PDF parsing
torch>=2.4.0              # ML framework
transformers>=4.54.0      # Sentence embeddings
numpy>=1.24               # Numerical operations

# Vector Storage
qdrant-client==1.7.0      # Vector database client
python-multipart>=0.0.20  # File upload support
```

### üèóÔ∏è Arquitetura Interna

```python
class DocumentProcessor:
    """
    SRP: Respons√°vel apenas pelo processamento de documentos
    OCP: Extens√≠vel para novos tipos de documento
    """
    def __init__(self):
        self.embeddings_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.vector_client = QdrantClient(url="http://qdrant:6333")
        self.chunk_size = 512
        self.overlap = 50
```

### üîÑ Fluxo de Processamento

1. **üìÑ Upload**: Recebe PDF via `/upload-pdf`
2. **üî™ Chunking**: Divide texto em chunks de 512 caracteres
3. **üß† Embeddings**: Gera vetores usando Sentence Transformers
4. **üíæ Storage**: Armazena no Qdrant com metadados
5. **üîç Search**: Busca sem√¢ntica via `/search`

### üì° API Endpoints

```python
@router.post("/upload-pdf")
async def upload_pdf(file: UploadFile) -> ProcessingResult:
    """
    Processa PDF e cria embeddings
    - Extrai texto com PyPDF2
    - Divide em chunks sem√¢nticos
    - Gera embeddings com SentenceTransformers
    - Indexa no Qdrant
    """

@router.post("/search")
async def search_documents(query: SearchRequest) -> SearchResults:
    """
    Busca sem√¢ntica nos documentos
    - Gera embedding da query
    - Busca similaridade no Qdrant
    - Filtra por score_threshold
    - Retorna chunks relevantes + metadados
    """

@router.get("/health")
async def health_check() -> HealthStatus:
    """Verifica sa√∫de do servi√ßo e conectividade com Qdrant"""
```

### üõ†Ô∏è Configura√ß√µes T√©cnicas

```python
# Sentence Transformers Model
MODEL = "all-MiniLM-L6-v2"  # 384 dimensions, multilingual
CHUNK_SIZE = 512             # Characters per chunk
OVERLAP = 50                 # Overlap between chunks
SCORE_THRESHOLD = 0.7        # Minimum similarity score

# Qdrant Configuration
COLLECTION_NAME = "documents"
VECTOR_SIZE = 384
DISTANCE_METRIC = "Cosine"
```

---

## üß† 2. RAG Service (Orchestrator)

**Porta:** `:8002` | **Responsabilidade:** Orquestra√ß√£o RAG e gera√ß√£o de respostas contextualizadas

### üìö Bibliotecas Core

```python
# Core Framework
fastapi>=0.116.1          # Async web framework
uvicorn>=0.35.0           # Production ASGI server
pydantic>=2.11.7          # Type validation
httpx>=0.28.1             # Async HTTP client

# Data Processing
numpy>=1.24.0             # Numerical operations
python-multipart>=0.0.6   # Multipart form support
```

### üèóÔ∏è Arquitetura RAG

```python
class RAGService:
    """
    RAG Orchestrator - Combines retrieval + generation
    
    Workflow:
    1. Query ‚Üí Document Search (semantic)
    2. Context + Query ‚Üí Mistral LLM
    3. Response + Sources ‚Üí User
    """
    
    def __init__(self):
        self.document_service = DocumentServiceClient()
        self.mistral_service = MistralServiceClient()
        self.max_context_length = 2048
```

### üîÑ Pipeline RAG

```python
async def process_rag_query(query: str) -> RAGResponse:
    """
    1. üîç RETRIEVAL: Busca documentos relevantes
    2. üìù CONTEXT: Monta contexto com chunks
    3. ü§ñ GENERATION: Gera resposta com Mistral
    4. üìä METRICS: Coleta m√©tricas de performance
    """
    
    # Step 1: Document Retrieval
    search_start = time.time()
    documents = await self.search_documents(
        query=query,
        limit=3,
        score_threshold=0.7
    )
    search_time = time.time() - search_start
    
    # Step 2: Context Assembly
    context = self.build_context(documents, query)
    
    # Step 3: LLM Generation
    generation_start = time.time()
    response = await self.generate_answer(context, query)
    generation_time = time.time() - generation_start
    
    return RAGResponse(
        question=query,
        answer=response.text,
        sources=documents,
        search_time=search_time,
        generation_time=generation_time,
        tokens_used=response.tokens
    )
```

### üì° API Endpoints

```python
@router.post("/ask")
async def rag_query(request: RAGRequest) -> RAGResponse:
    """
    Endpoint principal RAG
    - Busca documentos relevantes
    - Gera contexto estruturado
    - Obt√©m resposta do Mistral
    - Retorna resposta + fontes + m√©tricas
    """

@router.post("/search-only")
async def search_only(query: str) -> SearchResults:
    """Busca documentos sem gera√ß√£o (debug/testing)"""

@router.get("/health")
async def health_check() -> HealthStatus:
    """Verifica conectividade com Document + Mistral services"""
```

### ‚öôÔ∏è Configura√ß√µes RAG

```python
# Service Endpoints
MISTRAL_SERVICE_URL = "http://mistral-service:8003"
DOCUMENT_PROCESSOR_URL = "http://document-processor:8001"

# RAG Parameters
DEFAULT_SEARCH_LIMIT = 3
DEFAULT_SCORE_THRESHOLD = 0.7
MAX_CONTEXT_LENGTH = 2048
DEFAULT_MAX_TOKENS = 512
DEFAULT_TEMPERATURE = 0.7

# Timeouts & Retries
HTTP_TIMEOUT = 60.0
MAX_RETRIES = 3
BACKOFF_FACTOR = 2
```

---

## üöÄ 3. Mistral Service

**Porta:** `:8003` | **Responsabilidade:** Gera√ß√£o de texto com Mistral 7B via Ollama

### üìö Bibliotecas Core

```python
# Web Framework
fastapi>=0.116.1          # Async API framework
uvicorn>=0.35.0           # Production server
pydantic>=2.11.7          # Data validation
httpx>=0.28.1             # Ollama HTTP client
```

### üèóÔ∏è Arquitetura do Modelo

```python
class MistralService:
    """
    Mistral 7B Inference Service
    
    Features:
    - Async inference via Ollama
    - Context-aware responses
    - Token counting and metrics
    - Temperature control
    """
    
    def __init__(self):
        self.ollama_url = "http://172.17.0.1:11434"
        self.model_name = "mistral:latest"
        self.default_system_prompt = self.load_system_prompt()
```

### ü§ñ Sistema de Prompts

```python
SYSTEM_PROMPT = """
Voc√™ √© um assistente especializado em base de conhecimento ICATU.

DIRETRIZES:
1. üìã Use APENAS informa√ß√µes do contexto fornecido
2. üéØ Seja preciso e direto nas respostas
3. üìö Cite fontes quando poss√≠vel
4. ‚ùì Se n√£o souber, diga claramente "N√£o tenho informa√ß√µes suficientes"
5. üîó Mantenha consist√™ncia com procedimentos ICATU

FORMATO DE RESPOSTA:
- Resposta direta √† pergunta
- Informa√ß√µes relevantes do contexto
- Passos ou procedimentos (se aplic√°vel)
"""

def build_prompt(context: str, question: str) -> str:
    """
    Constr√≥i prompt estruturado para Mistral
    
    Template:
    SISTEMA + CONTEXTO + PERGUNTA + INSTRU√á√ïES
    """
    return f"""
{SYSTEM_PROMPT}

CONTEXTO DISPON√çVEL:
{context}

PERGUNTA DO USU√ÅRIO:
{question}

RESPOSTA:
"""
```

### üì° API Endpoints

```python
@router.post("/generate")
async def generate_text(request: GenerationRequest) -> GenerationResponse:
    """
    Gera√ß√£o de texto com Mistral
    - Processa prompt estruturado
    - Controla temperatura e tokens
    - Retorna resposta + m√©tricas
    """

@router.post("/chat")
async def chat_completion(request: ChatRequest) -> ChatResponse:
    """Chat conversacional com hist√≥rico de contexto"""

@router.get("/health")
async def health_check() -> HealthStatus:
    """Verifica conectividade com Ollama + modelo carregado"""
```

### ‚öôÔ∏è Configura√ß√µes do Modelo

```python
# Ollama Configuration
OLLAMA_BASE_URL = "http://172.17.0.1:11434"
MODEL_NAME = "mistral:latest"
MODEL_SIZE = "7B parameters"

# Generation Parameters
DEFAULT_MAX_TOKENS = 512
DEFAULT_TEMPERATURE = 0.7  # Balanced creativity/accuracy
DEFAULT_TOP_P = 0.9
DEFAULT_TOP_K = 40

# Performance Settings
TIMEOUT_SECONDS = 60
STREAM_RESPONSES = False  # Future feature
BATCH_SIZE = 1           # Single requests for now
```

---

## üîß Infraestrutura e DevOps

### üê≥ Docker Compose

```yaml
services:
  # Mistral Inference Service
  mistral-service:
    build: ./ai-services/inference
    ports: ["8003:8003"]
    environment:
      - OLLAMA_BASE_URL=http://172.17.0.1:11434
    networks: [knowledge-base]

  # Document Processing Service  
  document-processor:
    build: ./ai-services/document-processor
    ports: ["8001:8001"]
    volumes:
      - ./data/uploads:/app/uploads
    networks: [knowledge-base]

  # RAG Orchestration Service
  rag-service:
    build: ./ai-services/rag
    ports: ["8002:8002"]
    depends_on: [mistral-service, document-processor]
    networks: [knowledge-base]

networks:
  knowledge-base:
    driver: bridge
```

### üîç Monitoramento e Logging

```python
# Structured Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s'
)

# Performance Metrics
@router.middleware("http")
async def track_performance(request: Request, call_next):
    start_time = time.time()
    response = await call_next(request)
    process_time = time.time() - start_time
    
    logger.info(f"Request: {request.method} {request.url}")
    logger.info(f"Duration: {process_time:.4f}s")
    logger.info(f"Status: {response.status_code}")
    
    return response
```

### üîÑ Health Checks

```python
@router.get("/health")
async def comprehensive_health_check():
    """
    Health check completo para cada servi√ßo:
    
    ‚úÖ Service Status
    ‚úÖ Dependencies (Ollama, Qdrant)
    ‚úÖ Model Availability
    ‚úÖ Disk Space
    ‚úÖ Memory Usage
    """
    return HealthResponse(
        service="healthy",
        dependencies=check_dependencies(),
        timestamp=datetime.utcnow(),
        version="1.0.0"
    )
```

---

## üîó Integra√ß√£o Frontend ‚Üî AI Services

### üåê API Gateway (Next.js)

```typescript
// app/api/ai/[...path]/route.ts
export async function POST(request: Request, { params }: { params: { path: string[] } }) {
  const path = params.path.join('/');
  
  // Route mapping
  const serviceMap = {
    'rag/query': 'http://localhost:8002/ask',
    'upload': 'http://localhost:8001/upload-pdf',
    'search': 'http://localhost:8001/search'
  };
  
  const targetUrl = serviceMap[path];
  return fetch(targetUrl, {
    method: 'POST',
    headers: request.headers,
    body: request.body
  });
}
```

### üéõÔ∏è Cliente HTTP (Frontend)

```typescript
// lib/ai-client.ts
class AIClient {
  private baseUrl = '/api/ai';
  
  async query(question: string): Promise<RAGResponse> {
    const response = await fetch(`${this.baseUrl}/rag/query`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        question,
        max_tokens: 512,
        temperature: 0.7
      })
    });
    
    return response.json();
  }
  
  async uploadDocument(file: File): Promise<ProcessingResult> {
    const formData = new FormData();
    formData.append('file', file);
    
    const response = await fetch(`${this.baseUrl}/upload`, {
      method: 'POST',
      body: formData
    });
    
    return response.json();
  }
}
```

---

## üìä Performance e M√©tricas

### ‚ö° Benchmarks de Performance

Com base nos testes realizados:

| M√©trica | 10 usu√°rios | 30 usu√°rios | 50 usu√°rios | 70 usu√°rios |
|---------|-------------|-------------|-------------|-------------|
| **Taxa de Sucesso** | 100% | 100% | 100% | 100% |
| **Tempo M√©dio** | 2.2s | 5.9s | 10.4s | 14.6s |
| **Throughput** | 2.22 req/s | **2.60 req/s** | 2.46 req/s | 2.29 req/s |
| **P95 Lat√™ncia** | 4.5s | 11.0s | 19.4s | 29.8s |

**üéØ Configura√ß√£o Recomendada:** 30 usu√°rios simult√¢neos (throughput √≥timo)

### üìà Otimiza√ß√µes Implementadas

1. **üîÑ Async/Await**: Opera√ß√µes n√£o-bloqueantes em todos os servi√ßos
2. **‚ö° HTTP/2**: Conex√µes persistentes com httpx
3. **üß† Smart Caching**: Cache de embeddings e respostas
4. **üîç Efficient Search**: Qdrant com √≠ndices otimizados
5. **üéõÔ∏è Resource Management**: Pools de conex√£o e timeouts

---

## üöÄ Como Contribuir

### üõ†Ô∏è Setup de Desenvolvimento

```bash
# 1. Clone e setup
git clone --recursive https://github.com/venysssssssssss/knowledge-base-refac.git
cd knowledge-base-refac

# 2. Instale Ollama
curl -fsSL https://ollama.ai/install.sh | sh
ollama serve
ollama pull mistral:latest

# 3. Suba os servi√ßos
docker-compose up --build

# 4. Teste a integra√ß√£o
cd scripts
node test-connection.js
```

### üß™ Executando Testes

```bash
# Teste de conectividade
node scripts/test-connection.js

# Teste de acur√°cia ICATU
node scripts/icatu-accuracy-test.js

# Teste de performance completo
node scripts/advanced-performance-test.js
```

### üìù Padr√µes de C√≥digo

- **üéØ SOLID Principles**: Responsabilidade √∫nica, interfaces bem definidas
- **üîß Type Safety**: Pydantic para valida√ß√£o, TypeScript no frontend
- **üìä Observabilidade**: Logs estruturados, m√©tricas de performance
- **üß™ Testabilidade**: Dependency injection, mocks para testes

---

## üéØ Roadmap T√©cnico

## üéØ Roadmap T√©cnico

### ‚úÖ FASE 1: Core AI Services (CONCLU√çDA)
- [x] **Mistral Service**: Inference com Ollama + Mistral 7B
- [x] **Document Processor**: PyPDF2 + Sentence Transformers + Qdrant
- [x] **RAG Service**: Orquestra√ß√£o retrieval + generation
- [x] **API Integration**: Proxy Next.js ‚Üî AI Services
- [x] **Performance Testing**: Framework de testes de carga

### üîÑ FASE 2: Arquitetura de Performance (4/6 ‚úÖ)

#### **2.1 Cache distribu√≠do com Redis** ‚úÖ CONCLU√çDA
- [x] Implementar cache Redis para respostas de IA
- [x] Cache local como fallback  
- [x] Sistema de invalida√ß√£o por tags
- [x] Cache sem√¢ntico para perguntas similares

#### **2.2 Sistema de filas inteligentes** ‚úÖ CONCLU√çDA  
- [x] Fila para processamento ass√≠ncrono
- [x] Agrupamento de perguntas similares
- [x] Sistema de prioridades
- [x] Processamento em lote

#### **2.3 Pool de conex√µes** ‚úÖ CONCLU√çDA
- [x] Gerenciamento inteligente de conex√µes
- [x] Load balancing entre servi√ßos
- [x] Retry autom√°tico com backoff
- [x] Health checks dos servi√ßos

#### **2.4 Rate limiting e throttling** ‚úÖ CONCLU√çDA
- [x] Limite de requisi√ß√µes por usu√°rio
- [x] Throttling global do sistema
- [x] Filas de espera inteligentes
- [x] Prote√ß√£o contra spam e DoS

#### **2.5 Paralelismo para m√∫ltiplas sess√µes de chat** üîÑ EM ANDAMENTO
- [ ] Isolamento de sess√µes por usu√°rio
- [ ] Processamento concorrente otimizado
- [ ] Balanceamento de carga din√¢mico
- [ ] Sincroniza√ß√£o de estado entre sess√µes

#### **2.6 Circuit breaker para falhas de IA** üîÑ EM ANDAMENTO  
- [ ] Detec√ß√£o autom√°tica de falhas
- [ ] Fallback para cache ou mensagem de erro
- [ ] Recupera√ß√£o gradual dos servi√ßos
- [ ] M√©tricas de sa√∫de em tempo real

### üíé FASE 3: Frontend Moderno (UI/UX EXCELLENCE)
- [ ] **3.1** Design System completo (Tailwind + CVA)
- [ ] **3.2** Componentes reutiliz√°veis (Radix + Headless UI)
- [ ] **3.3** Anima√ß√µes fluidas (Framer Motion)
- [ ] **3.4** Tema dark/light consistente
- [ ] **3.5** Responsividade mobile-first
- [ ] **3.6** Acessibilidade (WCAG 2.1)
- [ ] **3.7** PWA com service workers

### üîê FASE 4: Autentica√ß√£o e Seguran√ßa (ENTERPRISE)
- [ ] **4.1** Next-Auth com m√∫ltiplos providers
- [ ] **4.2** JWT com refresh tokens
- [ ] **4.3** RBAC (Role-Based Access Control)
- [ ] **4.4** Rate limiting por usu√°rio
- [ ] **4.5** Criptografia de dados sens√≠veis
- [ ] **4.6** Logs de auditoria e compliance

### ‚ö° FASE 5: Cache Inteligente (PERFORMANCE)
- [ ] **5.1** Cache de embeddings (evitar reprocessamento)
- [ ] **5.2** Cache de respostas por similaridade sem√¢ntica
- [ ] **5.3** Cache de sess√µes de chat
- [ ] **5.4** Invalida√ß√£o inteligente de cache
- [ ] **5.5** M√©tricas de cache hit/miss
- [ ] **5.6** Compress√£o de dados de cache

### üîÑ FASE 6: Processamento Paralelo (SCALABILITY)
- [ ] **6.1** Worker pools para document processing
- [ ] **6.2** Queue system para uploads pesados
- [ ] **6.3** Streaming de respostas da IA
- [ ] **6.4** WebSockets para real-time updates
- [ ] **6.5** Load balancing entre AI services
- [ ] **6.6** Auto-scaling baseado em m√©tricas

### üè¢ FASE 7: Princ√≠pios SOLID e Clean Code
- [ ] **7.1** Refatorar para arquitetura hexagonal
- [ ] **7.2** Dependency injection em todos os services
- [ ] **7.3** Interfaces bem definidas
- [ ] **7.4** Testes unit√°rios (Jest + Testing Library)
- [ ] **7.5** Testes de integra√ß√£o
- [ ] **7.6** Documenta√ß√£o t√©cnica completa

### üìä FASE 8: Monitoramento e DevOps (PRODUCTION READY)
- [ ] **8.1** M√©tricas de performance (Prometheus)
- [ ] **8.2** Logging estruturado (ELK Stack)
- [ ] **8.3** Health checks robustos
- [ ] **8.4** CI/CD pipeline
- [ ] **8.5** Docker multi-stage builds
- [ ] **8.6** Kubernetes deployment configs

---

## üèóÔ∏è Stack Tecnol√≥gico Atual

### ü§ñ AI Services Backend
- **üß† LLM**: Mistral 7B via Ollama
- **üîç Vector DB**: Qdrant (busca sem√¢ntica)
- **üìÑ Document Processing**: PyPDF2 + Sentence Transformers
- **üåê Web Framework**: FastAPI (async)
- **üê≥ Containerization**: Docker + Docker Compose
- **üìö ML Libraries**: transformers, torch, numpy

### üíª Frontend Stack
- **‚öõÔ∏è Framework**: Next.js 15 (App Router) + React 19
- **üé® Styling**: Tailwind CSS + CVA (Class Variance Authority)
- **üé≠ Animations**: Framer Motion
- **üíæ Storage**: IndexedDB para cache offline
- **üîó HTTP Client**: Fetch API nativo
- **üìù TypeScript**: Strict mode habilitado

### üîß Infraestrutura
- **üê≥ Orquestra√ß√£o**: Docker Compose
- **üîÑ Proxy**: Next.js API Routes
- **üìä Monitoramento**: Logs estruturados
- **‚ö° Performance**: Async/await, HTTP/2

---

## üöÄ Pr√≥ximos Passos Imediatos

1. **üéØ PRIORIDADE 1**: Implementar circuit breaker para falhas de IA
2. **‚ö° PRIORIDADE 2**: Otimizar paralelismo para m√∫ltiplas sess√µes
3. **üé® PRIORIDADE 3**: Melhorar UI/UX com design system
4. **üîê PRIORIDADE 4**: Implementar autentica√ß√£o robusta

---

## üìà Workflow Operacional

### üìÑ Upload de Documentos
1. **Frontend**: Usu√°rio seleciona PDF via interface React
2. **API Gateway**: Next.js proxy encaminha para Document Processor
3. **Processing**: PyPDF2 extrai texto, Sentence Transformers gera embeddings
4. **Storage**: Qdrant indexa vetores com metadados
5. **Response**: Status de processamento retornado ao usu√°rio

### üîç Consulta RAG
1. **Query**: Usu√°rio faz pergunta no chat
2. **Retrieval**: RAG Service busca documentos relevantes no Qdrant
3. **Context**: Monta contexto estruturado com chunks + metadados
4. **Generation**: Mistral 7B gera resposta contextualizada
5. **Response**: Resposta + fontes + m√©tricas retornadas

### ‚ö° Monitoramento
- **Health Checks**: Verifica√ß√£o autom√°tica dos servi√ßos
- **Metrics**: Tempo de resposta, tokens utilizados, taxa de sucesso
- **Logs**: Rastreamento estruturado de todas as opera√ß√µes
- **Performance**: Benchmarks autom√°ticos de concorr√™ncia

---

## üîß Setup de Desenvolvimento

### üõ†Ô∏è Pr√©-requisitos
```bash
# Instalar Docker
curl -fsSL https://get.docker.com -o get-docker.sh
sh get-docker.sh

# Instalar Ollama
curl -fsSL https://ollama.ai/install.sh | sh
ollama serve
ollama pull mistral:latest

# Instalar Node.js 20+
curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash -
sudo apt-get install -y nodejs
```

### üöÄ In√≠cio R√°pido
```bash
# Clone com submodules
git clone --recursive https://github.com/venysssssssssss/knowledge-base-refac.git
cd knowledge-base-refac

# Setup AI Services
docker-compose up --build -d

# Setup Frontend
cd frontend
npm install
npm run dev

# Verificar sa√∫de dos servi√ßos
cd ../scripts
node test-connection.js
```

### üß™ Executar Testes
```bash
# Teste de conectividade
node scripts/test-connection.js

# Teste de acur√°cia com perguntas ICATU
node scripts/icatu-accuracy-test.js

# Teste de performance (10-70 usu√°rios simult√¢neos)
node scripts/advanced-performance-test.js
```

---

## üìö Recursos e Documenta√ß√£o

### üîó Links √öteis
- **[Ollama Documentation](https://ollama.ai/docs)** - Setup e configura√ß√£o
- **[Qdrant Docs](https://qdrant.tech/documentation/)** - Vector database
- **[FastAPI Guide](https://fastapi.tiangolo.com/)** - API framework
- **[Next.js 15](https://nextjs.org/docs)** - Frontend framework
- **[Sentence Transformers](https://www.sbert.net/)** - Embeddings

### üìñ Arquivos de Configura√ß√£o
- `docker-compose.yml` - Orquestra√ß√£o dos servi√ßos
- `pyproject.toml` - Depend√™ncias Python
- `frontend/package.json` - Depend√™ncias Node.js
- `scripts/` - Ferramentas de teste e monitoramento

### üéØ M√©tricas de Performance
Com base nos testes realizados, o sistema mant√©m:
- **‚úÖ 100% taxa de sucesso** em todos os n√≠veis de concorr√™ncia
- **‚ö° 2.6 req/s throughput √≥timo** com 30 usu√°rios simult√¢neos  
- **üéØ 71% similaridade m√©dia** nas respostas RAG
- **‚è±Ô∏è 2-15s lat√™ncia** dependendo da concorr√™ncia

---

*Documenta√ß√£o t√©cnica criada em 05/08/2025 - Sistema Knowledge Base ICATU v1.0*
