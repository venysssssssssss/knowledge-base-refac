# ğŸ§  Knowledge Base Refactoring - Sistema Inteligente

![Architecture Diagram](deepseek_mermaid_20250728_e36e87.png)

Um sistema completo de base de conhecimento com IA generativa usando **Mistral 7B**, arquitetura moderna e princÃ­pios **SOLID** de desenvolvimento de software.

## ğŸš€ Como Executar o Projeto

### Frontend (Next.js 15 + React 19)
```bash
cd frontend
npm install
npm run dev  # Desenvolvimento com Turbopack
# ou
npm run build && npm start  # ProduÃ§Ã£o
```

### AI Services (Docker Compose)
```bash
# Subir todos os serviÃ§os
docker-compose up --build

# Verificar saÃºde dos serviÃ§os
curl http://localhost:8001/health  # Document Processor
curl http://localhost:8002/health  # RAG Service  
curl http://localhost:8003/health  # Mistral Service
```

### ConfiguraÃ§Ã£o Ollama (NecessÃ¡rio)
```bash
# Instalar e configurar Ollama
curl -fsSL https://ollama.ai/install.sh | sh
ollama serve
ollama pull mistral:latest
```

---

# ğŸ¤– AI Services - DocumentaÃ§Ã£o TÃ©cnica

## ğŸ“‹ VisÃ£o Geral da Arquitetura

O sistema AI Services Ã© composto por **trÃªs microserviÃ§os especializados** que trabalham em conjunto para fornecer capacidades de **processamento de documentos**, **busca semÃ¢ntica** e **geraÃ§Ã£o de respostas inteligentes**.

```mermaid
graph TB
    Frontend[Frontend Next.js] --> API[API Gateway/Proxy]
    API --> RAG[RAG Service :8002]
    RAG --> DOC[Document Processor :8001]
    RAG --> MISTRAL[Mistral Service :8003]
    DOC --> QDRANT[(Qdrant Vector DB)]
    MISTRAL --> OLLAMA[Ollama + Mistral 7B]
    
    subgraph "AI Services Ecosystem"
        DOC
        RAG
        MISTRAL
        QDRANT
        OLLAMA
    end
```

### ğŸ¯ PrincÃ­pios Arquiteturais

- **ğŸ”§ SOLID Principles**: Cada serviÃ§o segue responsabilidade Ãºnica, aberto/fechado, etc.
- **ğŸ”€ Microservices**: ServiÃ§os independentes com APIs bem definidas
- **ğŸ—ï¸ Clean Architecture**: SeparaÃ§Ã£o clara entre domÃ­nio, aplicaÃ§Ã£o e infraestrutura
- **ğŸš€ Async/Await**: OperaÃ§Ãµes nÃ£o-bloqueantes para alta performance
- **ğŸ“Š Observabilidade**: Logs estruturados e mÃ©tricas de performance

---

## ğŸ§¾ 1. Document Processor Service

**Porta:** `:8001` | **Responsabilidade:** Processamento e indexaÃ§Ã£o de documentos

### ğŸ“š Bibliotecas Core

```python
# Core Dependencies
fastapi>=0.116.1          # Web framework async
uvicorn>=0.35.0           # ASGI server
pydantic>=2.11.7          # Data validation
httpx>=0.28.1             # HTTP client async

# Document Processing
pypdf2>=3.0.1             # PDF parsing
torch>=2.4.0              # ML framework
transformers>=4.54.0      # Sentence embeddings
numpy>=1.24               # Numerical operations

# Vector Storage
qdrant-client==1.7.0      # Vector database client
python-multipart>=0.0.20  # File upload support
```

### ğŸ—ï¸ Arquitetura Interna

```python
class DocumentProcessor:
    """
    SRP: ResponsÃ¡vel apenas pelo processamento de documentos
    OCP: ExtensÃ­vel para novos tipos de documento
    """
    def __init__(self):
        self.embeddings_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.vector_client = QdrantClient(url="http://qdrant:6333")
        self.chunk_size = 512
        self.overlap = 50
```

### ğŸ”„ Fluxo de Processamento

1. **ğŸ“„ Upload**: Recebe PDF via `/upload-pdf`
2. **ğŸ”ª Chunking**: Divide texto em chunks de 512 caracteres
3. **ğŸ§  Embeddings**: Gera vetores usando Sentence Transformers
4. **ğŸ’¾ Storage**: Armazena no Qdrant com metadados
5. **ğŸ” Search**: Busca semÃ¢ntica via `/search`

### ğŸ“¡ API Endpoints

```python
@router.post("/upload-pdf")
async def upload_pdf(file: UploadFile) -> ProcessingResult:
    """
    Processa PDF e cria embeddings
    - Extrai texto com PyPDF2
    - Divide em chunks semÃ¢nticos
    - Gera embeddings com SentenceTransformers
    - Indexa no Qdrant
    """

@router.post("/search")
async def search_documents(query: SearchRequest) -> SearchResults:
    """
    Busca semÃ¢ntica nos documentos
    - Gera embedding da query
    - Busca similaridade no Qdrant
    - Filtra por score_threshold
    - Retorna chunks relevantes + metadados
    """

@router.get("/health")
async def health_check() -> HealthStatus:
    """Verifica saÃºde do serviÃ§o e conectividade com Qdrant"""
```

### ğŸ› ï¸ ConfiguraÃ§Ãµes TÃ©cnicas

```python
# Sentence Transformers Model
MODEL = "all-MiniLM-L6-v2"  # 384 dimensions, multilingual
CHUNK_SIZE = 512             # Characters per chunk
OVERLAP = 50                 # Overlap between chunks
SCORE_THRESHOLD = 0.7        # Minimum similarity score

# Qdrant Configuration
COLLECTION_NAME = "documents"
VECTOR_SIZE = 384
DISTANCE_METRIC = "Cosine"
```

---

## ğŸ§  2. RAG Service (Orchestrator)

**Porta:** `:8002` | **Responsabilidade:** OrquestraÃ§Ã£o RAG e geraÃ§Ã£o de respostas contextualizadas

### ğŸ“š Bibliotecas Core

```python
# Core Framework
fastapi>=0.116.1          # Async web framework
uvicorn>=0.35.0           # Production ASGI server
pydantic>=2.11.7          # Type validation
httpx>=0.28.1             # Async HTTP client

# Data Processing
numpy>=1.24.0             # Numerical operations
python-multipart>=0.0.6   # Multipart form support
```

### ğŸ—ï¸ Arquitetura RAG

```python
class RAGService:
    """
    RAG Orchestrator - Combines retrieval + generation
    
    Workflow:
    1. Query â†’ Document Search (semantic)
    2. Context + Query â†’ Mistral LLM
    3. Response + Sources â†’ User
    """
    
    def __init__(self):
        self.document_service = DocumentServiceClient()
        self.mistral_service = MistralServiceClient()
        self.max_context_length = 2048
```

### ğŸ”„ Pipeline RAG

```python
async def process_rag_query(query: str) -> RAGResponse:
    """
    1. ğŸ” RETRIEVAL: Busca documentos relevantes
    2. ğŸ“ CONTEXT: Monta contexto com chunks
    3. ğŸ¤– GENERATION: Gera resposta com Mistral
    4. ğŸ“Š METRICS: Coleta mÃ©tricas de performance
    """
    
    # Step 1: Document Retrieval
    search_start = time.time()
    documents = await self.search_documents(
        query=query,
        limit=3,
        score_threshold=0.7
    )
    search_time = time.time() - search_start
    
    # Step 2: Context Assembly
    context = self.build_context(documents, query)
    
    # Step 3: LLM Generation
    generation_start = time.time()
    response = await self.generate_answer(context, query)
    generation_time = time.time() - generation_start
    
    return RAGResponse(
        question=query,
        answer=response.text,
        sources=documents,
        search_time=search_time,
        generation_time=generation_time,
        tokens_used=response.tokens
    )
```

### ğŸ“¡ API Endpoints

```python
@router.post("/ask")
async def rag_query(request: RAGRequest) -> RAGResponse:
    """
    Endpoint principal RAG
    - Busca documentos relevantes
    - Gera contexto estruturado
    - ObtÃ©m resposta do Mistral
    - Retorna resposta + fontes + mÃ©tricas
    """

@router.post("/search-only")
async def search_only(query: str) -> SearchResults:
    """Busca documentos sem geraÃ§Ã£o (debug/testing)"""

@router.get("/health")
async def health_check() -> HealthStatus:
    """Verifica conectividade com Document + Mistral services"""
```

### âš™ï¸ ConfiguraÃ§Ãµes RAG

```python
# Service Endpoints
MISTRAL_SERVICE_URL = "http://mistral-service:8003"
DOCUMENT_PROCESSOR_URL = "http://document-processor:8001"

# RAG Parameters
DEFAULT_SEARCH_LIMIT = 3
DEFAULT_SCORE_THRESHOLD = 0.7
MAX_CONTEXT_LENGTH = 2048
DEFAULT_MAX_TOKENS = 512
DEFAULT_TEMPERATURE = 0.7

# Timeouts & Retries
HTTP_TIMEOUT = 60.0
MAX_RETRIES = 3
BACKOFF_FACTOR = 2
```

---

## ğŸš€ 3. Mistral Service

**Porta:** `:8003` | **Responsabilidade:** GeraÃ§Ã£o de texto com Mistral 7B via Ollama

### ğŸ“š Bibliotecas Core

```python
# Web Framework
fastapi>=0.116.1          # Async API framework
uvicorn>=0.35.0           # Production server
pydantic>=2.11.7          # Data validation
httpx>=0.28.1             # Ollama HTTP client
```

### ğŸ—ï¸ Arquitetura do Modelo

```python
class MistralService:
    """
    Mistral 7B Inference Service
    
    Features:
    - Async inference via Ollama
    - Context-aware responses
    - Token counting and metrics
    - Temperature control
    """
    
    def __init__(self):
        self.ollama_url = "http://172.17.0.1:11434"
        self.model_name = "mistral:latest"
        self.default_system_prompt = self.load_system_prompt()
```

### ğŸ¤– Sistema de Prompts

```python
SYSTEM_PROMPT = """
VocÃª Ã© um assistente especializado em base de conhecimento ICATU.

DIRETRIZES:
1. ğŸ“‹ Use APENAS informaÃ§Ãµes do contexto fornecido
2. ğŸ¯ Seja preciso e direto nas respostas
3. ğŸ“š Cite fontes quando possÃ­vel
4. â“ Se nÃ£o souber, diga claramente "NÃ£o tenho informaÃ§Ãµes suficientes"
5. ğŸ”— Mantenha consistÃªncia com procedimentos ICATU

FORMATO DE RESPOSTA:
- Resposta direta Ã  pergunta
- InformaÃ§Ãµes relevantes do contexto
- Passos ou procedimentos (se aplicÃ¡vel)
"""

def build_prompt(context: str, question: str) -> str:
    """
    ConstrÃ³i prompt estruturado para Mistral
    
    Template:
    SISTEMA + CONTEXTO + PERGUNTA + INSTRUÃ‡Ã•ES
    """
    return f"""
{SYSTEM_PROMPT}

CONTEXTO DISPONÃVEL:
{context}

PERGUNTA DO USUÃRIO:
{question}

RESPOSTA:
"""
```

### ğŸ“¡ API Endpoints

```python
@router.post("/generate")
async def generate_text(request: GenerationRequest) -> GenerationResponse:
    """
    GeraÃ§Ã£o de texto com Mistral
    - Processa prompt estruturado
    - Controla temperatura e tokens
    - Retorna resposta + mÃ©tricas
    """

@router.post("/chat")
async def chat_completion(request: ChatRequest) -> ChatResponse:
    """Chat conversacional com histÃ³rico de contexto"""

@router.get("/health")
async def health_check() -> HealthStatus:
    """Verifica conectividade com Ollama + modelo carregado"""
```

### âš™ï¸ ConfiguraÃ§Ãµes do Modelo

```python
# Ollama Configuration
OLLAMA_BASE_URL = "http://172.17.0.1:11434"
MODEL_NAME = "mistral:latest"
MODEL_SIZE = "7B parameters"

# Generation Parameters
DEFAULT_MAX_TOKENS = 512
DEFAULT_TEMPERATURE = 0.7  # Balanced creativity/accuracy
DEFAULT_TOP_P = 0.9
DEFAULT_TOP_K = 40

# Performance Settings
TIMEOUT_SECONDS = 60
STREAM_RESPONSES = False  # Future feature
BATCH_SIZE = 1           # Single requests for now
```

---

## ğŸ”§ Infraestrutura e DevOps

### ğŸ³ Docker Compose

```yaml
services:
  # Mistral Inference Service
  mistral-service:
    build: ./ai-services/inference
    ports: ["8003:8003"]
    environment:
      - OLLAMA_BASE_URL=http://172.17.0.1:11434
    networks: [knowledge-base]

  # Document Processing Service  
  document-processor:
    build: ./ai-services/document-processor
    ports: ["8001:8001"]
    volumes:
      - ./data/uploads:/app/uploads
    networks: [knowledge-base]

  # RAG Orchestration Service
  rag-service:
    build: ./ai-services/rag
    ports: ["8002:8002"]
    depends_on: [mistral-service, document-processor]
    networks: [knowledge-base]

networks:
  knowledge-base:
    driver: bridge
```

### ğŸ” Monitoramento e Logging

```python
# Structured Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s'
)

# Performance Metrics
@router.middleware("http")
async def track_performance(request: Request, call_next):
    start_time = time.time()
    response = await call_next(request)
    process_time = time.time() - start_time
    
    logger.info(f"Request: {request.method} {request.url}")
    logger.info(f"Duration: {process_time:.4f}s")
    logger.info(f"Status: {response.status_code}")
    
    return response
```

### ğŸ”„ Health Checks

```python
@router.get("/health")
async def comprehensive_health_check():
    """
    Health check completo para cada serviÃ§o:
    
    âœ… Service Status
    âœ… Dependencies (Ollama, Qdrant)
    âœ… Model Availability
    âœ… Disk Space
    âœ… Memory Usage
    """
    return HealthResponse(
        service="healthy",
        dependencies=check_dependencies(),
        timestamp=datetime.utcnow(),
        version="1.0.0"
    )
```

---

## ğŸ”— IntegraÃ§Ã£o Frontend â†” AI Services

### ğŸŒ API Gateway (Next.js)

```typescript
// app/api/ai/[...path]/route.ts
export async function POST(request: Request, { params }: { params: { path: string[] } }) {
  const path = params.path.join('/');
  
  // Route mapping
  const serviceMap = {
    'rag/query': 'http://localhost:8002/ask',
    'upload': 'http://localhost:8001/upload-pdf',
    'search': 'http://localhost:8001/search'
  };
  
  const targetUrl = serviceMap[path];
  return fetch(targetUrl, {
    method: 'POST',
    headers: request.headers,
    body: request.body
  });
}
```

### ğŸ›ï¸ Cliente HTTP (Frontend)

```typescript
// lib/ai-client.ts
class AIClient {
  private baseUrl = '/api/ai';
  
  async query(question: string): Promise<RAGResponse> {
    const response = await fetch(`${this.baseUrl}/rag/query`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        question,
        max_tokens: 512,
        temperature: 0.7
      })
    });
    
    return response.json();
  }
  
  async uploadDocument(file: File): Promise<ProcessingResult> {
    const formData = new FormData();
    formData.append('file', file);
    
    const response = await fetch(`${this.baseUrl}/upload`, {
      method: 'POST',
      body: formData
    });
    
    return response.json();
  }
}
```

---

## ğŸ“Š Performance e MÃ©tricas

### âš¡ Benchmarks de Performance

Com base nos testes realizados:

| MÃ©trica | 10 usuÃ¡rios | 30 usuÃ¡rios | 50 usuÃ¡rios | 70 usuÃ¡rios |
|---------|-------------|-------------|-------------|-------------|
| **Taxa de Sucesso** | 100% | 100% | 100% | 100% |
| **Tempo MÃ©dio** | 2.2s | 5.9s | 10.4s | 14.6s |
| **Throughput** | 2.22 req/s | **2.60 req/s** | 2.46 req/s | 2.29 req/s |
| **P95 LatÃªncia** | 4.5s | 11.0s | 19.4s | 29.8s |

**ğŸ¯ ConfiguraÃ§Ã£o Recomendada:** 30 usuÃ¡rios simultÃ¢neos (throughput Ã³timo)

### ğŸ“ˆ OtimizaÃ§Ãµes Implementadas

1. **ğŸ”„ Async/Await**: OperaÃ§Ãµes nÃ£o-bloqueantes em todos os serviÃ§os
2. **âš¡ HTTP/2**: ConexÃµes persistentes com httpx
3. **ğŸ§  Smart Caching**: Cache de embeddings e respostas
4. **ğŸ” Efficient Search**: Qdrant com Ã­ndices otimizados
5. **ğŸ›ï¸ Resource Management**: Pools de conexÃ£o e timeouts

---

## ğŸš€ Como Contribuir

### ğŸ› ï¸ Setup de Desenvolvimento

```bash
# 1. Clone e setup
git clone --recursive https://github.com/venysssssssssss/knowledge-base-refac.git
cd knowledge-base-refac

# 2. Instale Ollama
curl -fsSL https://ollama.ai/install.sh | sh
ollama serve
ollama pull mistral:latest

# 3. Suba os serviÃ§os
docker-compose up --build

# 4. Teste a integraÃ§Ã£o
cd scripts
node test-connection.js
```

### ğŸ§ª Executando Testes

```bash
# Teste de conectividade
node scripts/test-connection.js

# Teste de acurÃ¡cia ICATU
node scripts/icatu-accuracy-test.js

# Teste de performance completo
node scripts/advanced-performance-test.js
```

### ğŸ“ PadrÃµes de CÃ³digo

- **ğŸ¯ SOLID Principles**: Responsabilidade Ãºnica, interfaces bem definidas
- **ğŸ”§ Type Safety**: Pydantic para validaÃ§Ã£o, TypeScript no frontend
- **ğŸ“Š Observabilidade**: Logs estruturados, mÃ©tricas de performance
- **ğŸ§ª Testabilidade**: Dependency injection, mocks para testes

---

## ğŸ¯ Roadmap TÃ©cnico

## ğŸ¯ Roadmap TÃ©cnico

### âœ… FASE 1: Core AI Services (CONCLUÃDA)
- [x] **Mistral Service**: Inference com Ollama + Mistral 7B
- [x] **Document Processor**: PyPDF2 + Sentence Transformers + Qdrant
- [x] **RAG Service**: OrquestraÃ§Ã£o retrieval + generation
- [x] **API Integration**: Proxy Next.js â†” AI Services
- [x] **Performance Testing**: Framework de testes de carga

### ğŸ”„ FASE 2: Arquitetura de Performance (4/6 âœ…)

#### **2.1 Cache distribuÃ­do com Redis** âœ… CONCLUÃDA
- [x] Implementar cache Redis para respostas de IA
- [x] Cache local como fallback  
- [x] Sistema de invalidaÃ§Ã£o por tags
- [x] Cache semÃ¢ntico para perguntas similares

#### **2.2 Sistema de filas inteligentes** âœ… CONCLUÃDA  
- [x] Fila para processamento assÃ­ncrono
- [x] Agrupamento de perguntas similares
- [x] Sistema de prioridades
- [x] Processamento em lote

#### **2.3 Pool de conexÃµes** âœ… CONCLUÃDA
- [x] Gerenciamento inteligente de conexÃµes
- [x] Load balancing entre serviÃ§os
- [x] Retry automÃ¡tico com backoff
- [x] Health checks dos serviÃ§os

#### **2.4 Rate limiting e throttling** âœ… CONCLUÃDA
- [x] Limite de requisiÃ§Ãµes por usuÃ¡rio
- [x] Throttling global do sistema
- [x] Filas de espera inteligentes
- [x] ProteÃ§Ã£o contra spam e DoS

#### **2.5 Paralelismo para mÃºltiplas sessÃµes de chat** ğŸ”„ EM ANDAMENTO
- [ ] Isolamento de sessÃµes por usuÃ¡rio
- [ ] Processamento concorrente otimizado
- [ ] Balanceamento de carga dinÃ¢mico
- [ ] SincronizaÃ§Ã£o de estado entre sessÃµes

#### **2.6 Circuit breaker para falhas de IA** ğŸ”„ EM ANDAMENTO  
- [ ] DetecÃ§Ã£o automÃ¡tica de falhas
- [ ] Fallback para cache ou mensagem de erro
- [ ] RecuperaÃ§Ã£o gradual dos serviÃ§os
- [ ] MÃ©tricas de saÃºde em tempo real

### ğŸ’ FASE 3: Frontend Moderno (UI/UX EXCELLENCE)
- [ ] **3.1** Design System completo (Tailwind + CVA)
- [ ] **3.2** Componentes reutilizÃ¡veis (Radix + Headless UI)
- [ ] **3.3** AnimaÃ§Ãµes fluidas (Framer Motion)
- [ ] **3.4** Tema dark/light consistente
- [ ] **3.5** Responsividade mobile-first
- [ ] **3.6** Acessibilidade (WCAG 2.1)
- [ ] **3.7** PWA com service workers

### ğŸ” FASE 4: AutenticaÃ§Ã£o e SeguranÃ§a (ENTERPRISE)
- [ ] **4.1** Next-Auth com mÃºltiplos providers
- [ ] **4.2** JWT com refresh tokens
- [ ] **4.3** RBAC (Role-Based Access Control)
- [ ] **4.4** Rate limiting por usuÃ¡rio
- [ ] **4.5** Criptografia de dados sensÃ­veis
- [ ] **4.6** Logs de auditoria e compliance

### âš¡ FASE 5: Cache Inteligente (PERFORMANCE)
- [ ] **5.1** Cache de embeddings (evitar reprocessamento)
- [ ] **5.2** Cache de respostas por similaridade semÃ¢ntica
- [ ] **5.3** Cache de sessÃµes de chat
- [ ] **5.4** InvalidaÃ§Ã£o inteligente de cache
- [ ] **5.5** MÃ©tricas de cache hit/miss
- [ ] **5.6** CompressÃ£o de dados de cache

### ğŸ”„ FASE 6: Processamento Paralelo (SCALABILITY)
- [ ] **6.1** Worker pools para document processing
- [ ] **6.2** Queue system para uploads pesados
- [ ] **6.3** Streaming de respostas da IA
- [ ] **6.4** WebSockets para real-time updates
- [ ] **6.5** Load balancing entre AI services
- [ ] **6.6** Auto-scaling baseado em mÃ©tricas

### ğŸ¢ FASE 7: PrincÃ­pios SOLID e Clean Code
- [ ] **7.1** Refatorar para arquitetura hexagonal
- [ ] **7.2** Dependency injection em todos os services
- [ ] **7.3** Interfaces bem definidas
- [ ] **7.4** Testes unitÃ¡rios (Jest + Testing Library)
- [ ] **7.5** Testes de integraÃ§Ã£o
- [ ] **7.6** DocumentaÃ§Ã£o tÃ©cnica completa

### ğŸ“Š FASE 8: Monitoramento e DevOps (PRODUCTION READY)
- [ ] **8.1** MÃ©tricas de performance (Prometheus)
- [ ] **8.2** Logging estruturado (ELK Stack)
- [ ] **8.3** Health checks robustos
- [ ] **8.4** CI/CD pipeline
- [ ] **8.5** Docker multi-stage builds
- [ ] **8.6** Kubernetes deployment configs

---

## ğŸ—ï¸ Stack TecnolÃ³gico Atual

### ğŸ¤– AI Services Backend
- **ğŸ§  LLM**: Mistral 7B via Ollama
- **ğŸ” Vector DB**: Qdrant (busca semÃ¢ntica)
- **ğŸ“„ Document Processing**: PyPDF2 + Sentence Transformers
- **ğŸŒ Web Framework**: FastAPI (async)
- **ğŸ³ Containerization**: Docker + Docker Compose
- **ğŸ“š ML Libraries**: transformers, torch, numpy

### ğŸ’» Frontend Stack
- **âš›ï¸ Framework**: Next.js 15 (App Router) + React 19
- **ğŸ¨ Styling**: Tailwind CSS + CVA (Class Variance Authority)
- **ğŸ­ Animations**: Framer Motion
- **ğŸ’¾ Storage**: IndexedDB para cache offline
- **ğŸ”— HTTP Client**: Fetch API nativo
- **ğŸ“ TypeScript**: Strict mode habilitado

### ğŸ”§ Infraestrutura
- **ğŸ³ OrquestraÃ§Ã£o**: Docker Compose
- **ğŸ”„ Proxy**: Next.js API Routes
- **ğŸ“Š Monitoramento**: Logs estruturados
- **âš¡ Performance**: Async/await, HTTP/2

---

## ğŸš€ PrÃ³ximos Passos Imediatos

1. **ğŸ¯ PRIORIDADE 1**: Implementar circuit breaker para falhas de IA
2. **âš¡ PRIORIDADE 2**: Otimizar paralelismo para mÃºltiplas sessÃµes
3. **ğŸ¨ PRIORIDADE 3**: Melhorar UI/UX com design system
4. **ğŸ” PRIORIDADE 4**: Implementar autenticaÃ§Ã£o robusta

---

## ğŸ“ˆ Workflow Operacional

### ğŸ“„ Upload de Documentos
1. **Frontend**: UsuÃ¡rio seleciona PDF via interface React
2. **API Gateway**: Next.js proxy encaminha para Document Processor
3. **Processing**: PyPDF2 extrai texto, Sentence Transformers gera embeddings
4. **Storage**: Qdrant indexa vetores com metadados
5. **Response**: Status de processamento retornado ao usuÃ¡rio

### ğŸ” Consulta RAG
1. **Query**: UsuÃ¡rio faz pergunta no chat
2. **Retrieval**: RAG Service busca documentos relevantes no Qdrant
3. **Context**: Monta contexto estruturado com chunks + metadados
4. **Generation**: Mistral 7B gera resposta contextualizada
5. **Response**: Resposta + fontes + mÃ©tricas retornadas

### âš¡ Monitoramento
- **Health Checks**: VerificaÃ§Ã£o automÃ¡tica dos serviÃ§os
- **Metrics**: Tempo de resposta, tokens utilizados, taxa de sucesso
- **Logs**: Rastreamento estruturado de todas as operaÃ§Ãµes
- **Performance**: Benchmarks automÃ¡ticos de concorrÃªncia

---

## ğŸ”§ Setup de Desenvolvimento

### ğŸ› ï¸ PrÃ©-requisitos
```bash
# Instalar Docker
curl -fsSL https://get.docker.com -o get-docker.sh
sh get-docker.sh

# Instalar Ollama
curl -fsSL https://ollama.ai/install.sh | sh
ollama serve
ollama pull mistral:latest

# Instalar Node.js 20+
curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash -
sudo apt-get install -y nodejs
```

### ğŸš€ InÃ­cio RÃ¡pido
```bash
# Clone com submodules
git clone --recursive https://github.com/venysssssssssss/knowledge-base-refac.git
cd knowledge-base-refac

# Setup AI Services
docker-compose up --build -d

# Setup Frontend
cd frontend
npm install
npm run dev

# Verificar saÃºde dos serviÃ§os
cd ../scripts
node test-connection.js
```

### ğŸ§ª Executar Testes
```bash
# Teste de conectividade
node scripts/test-connection.js

# Teste de acurÃ¡cia com perguntas ICATU
node scripts/icatu-accuracy-test.js

# Teste de performance (10-70 usuÃ¡rios simultÃ¢neos)
node scripts/advanced-performance-test.js
```

---

## ğŸ“š Recursos e DocumentaÃ§Ã£o

### ğŸ”— Links Ãšteis
- **[Ollama Documentation](https://ollama.ai/docs)** - Setup e configuraÃ§Ã£o
- **[Qdrant Docs](https://qdrant.tech/documentation/)** - Vector database
- **[FastAPI Guide](https://fastapi.tiangolo.com/)** - API framework
- **[Next.js 15](https://nextjs.org/docs)** - Frontend framework
- **[Sentence Transformers](https://www.sbert.net/)** - Embeddings

### ğŸ“– Arquivos de ConfiguraÃ§Ã£o
- `docker-compose.yml` - OrquestraÃ§Ã£o dos serviÃ§os
- `pyproject.toml` - DependÃªncias Python
- `frontend/package.json` - DependÃªncias Node.js
- `scripts/` - Ferramentas de teste e monitoramento

### ğŸ¯ MÃ©tricas de Performance
Com base nos testes realizados, o sistema mantÃ©m:
- **âœ… 100% taxa de sucesso** em todos os nÃ­veis de concorrÃªncia
- **âš¡ 2.6 req/s throughput Ã³timo** com 30 usuÃ¡rios simultÃ¢neos  
- **ğŸ¯ 71% similaridade mÃ©dia** nas respostas RAG
- **â±ï¸ 2-15s latÃªncia** dependendo da concorrÃªncia

---

*DocumentaÃ§Ã£o tÃ©cnica criada em 05/08/2025 - Sistema Knowledge Base ICATU v1.0*
