"""
Test script to verify Mistral 7B model loading
Run this before starting the full service
"""

import sys
import time
import logging
from pathlib import Path

# Add current directory to path
sys.path.append(str(Path(__file__).parent.parent))

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def test_model_loading():
    """Test if Mistral model can be loaded successfully"""
    
    try:
        from vllm import LLM, SamplingParams
        
        logger.info("ğŸ”„ Testing Mistral 7B model loading...")
        
        # Model path (adjust if needed)
        model_path = "models/mistral-7b-instruct-v0.2"
        
        # Check if model exists
        if not Path(model_path).exists():
            logger.error(f"âŒ Model not found at {model_path}")
            logger.info("Run the download script first: ./scripts/download_model.sh")
            return False
        
        logger.info(f"ğŸ“‚ Model found at: {model_path}")
        
        # Initialize LLM with AWQ quantization
        logger.info("ğŸš€ Initializing vLLM engine...")
        llm = LLM(
            model=model_path,
            quantization="awq",
            gpu_memory_utilization=0.8,  # Conservative for testing
            max_model_len=2048,  # Smaller for testing
            trust_remote_code=True
        )
        
        logger.info("âœ… Model loaded successfully!")
        
        # Test generation
        logger.info("ğŸ§ª Testing generation...")
        
        sampling_params = SamplingParams(
            temperature=0.7,
            max_tokens=100,
            stop=["</s>"]
        )
        
        test_prompt = "<s>[INST] OlÃ¡! Como vocÃª pode me ajudar? [/INST]"
        
        start_time = time.time()
        outputs = llm.generate([test_prompt], sampling_params)
        generation_time = time.time() - start_time
        
        for output in outputs:
            generated_text = output.outputs[0].text
            logger.info(f"ğŸ¤– Generated response: {generated_text}")
            logger.info(f"â±ï¸  Generation time: {generation_time:.2f}s")
        
        logger.info("ğŸ‰ All tests passed! Model is ready for production.")
        return True
        
    except ImportError as e:
        logger.error(f"âŒ Import error: {e}")
        logger.info("Install dependencies: pip install -r ai-services/requirements.txt")
        return False
        
    except Exception as e:
        logger.error(f"âŒ Model loading failed: {e}")
        return False

def test_dependencies():
    """Test if all required dependencies are installed"""
    
    logger.info("ğŸ” Checking dependencies...")
    
    required_packages = [
        'vllm',
        'torch',
        'transformers',
        'fastapi',
        'qdrant_client'
    ]
    
    missing_packages = []
    
    for package in required_packages:
        try:
            __import__(package)
            logger.info(f"âœ… {package}")
        except ImportError:
            logger.error(f"âŒ {package}")
            missing_packages.append(package)
    
    if missing_packages:
        logger.error(f"Missing packages: {missing_packages}")
        logger.info("Install with: pip install -r ai-services/requirements.txt")
        return False
    
    logger.info("âœ… All dependencies installed!")
    return True

if __name__ == "__main__":
    logger.info("ğŸš€ Starting Mistral 7B test suite...")
    
    # Test dependencies first
    if not test_dependencies():
        sys.exit(1)
    
    # Test model loading
    if not test_model_loading():
        sys.exit(1)
    
    logger.info("ğŸ¯ Ready to proceed with full deployment!")
    logger.info("Next: docker compose up -d")
