"""
RAG (Retrieval-Augmented Generation) Service
Combines document search with Mistral 7B for contextual answers
"""

import asyncio
import logging
import re
import time
from contextlib import asynccontextmanager
from typing import Any, Dict, List, Optional

import httpx
import uvicorn
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

# Configura√ß√£o b√°sica de logging estruturado
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
)

# Princ√≠pios SOLID aplicados
# SRP: Cada classe/fun√ß√£o tem responsabilidade √∫nica
# OCP: Classes abertas para extens√£o, fechadas para modifica√ß√£o
# LSP: Subclasses podem substituir superclasses
# ISP: Interfaces espec√≠ficas para cada opera√ß√£o
# DIP: Dependa de abstra√ß√µes


class RagLogger:
    def __init__(self, name: str):
        self.logger = logging.getLogger(name)

    def info(self, msg):
        self.logger.info(msg)

    def error(self, msg):
        self.logger.error(msg)

    def debug(self, msg):
        self.logger.debug(msg)

    def warning(self, msg):
        self.logger.warning(msg)

    def warn(self, msg):
        self.logger.warning(msg)


# Exemplo de uso do logger
rag_logger = RagLogger(__name__)
rag_logger.info('RAG Service iniciado.')

# Service URLs
MISTRAL_SERVICE_URL = (
    'http://10.117.0.19:8003'  # Updated to avoid port conflict
)
DOCUMENT_PROCESSOR_URL = 'http://10.117.0.19:8001'


class RAGRequest(BaseModel):
    question: str
    max_tokens: int = 512
    temperature: float = 0.7
    search_limit: int = 8
    score_threshold: float = 0.5
    document_id: str = None  # Novo campo opcional para filtrar por documento


class FullContextRAGRequest(BaseModel):
    question: str
    max_tokens: int = 1024
    temperature: float = 0.3
    document_id: str = None  # Opcional: usar documento espec√≠fico
    use_full_manual: bool = True  # Se deve usar todo o manual como contexto


class RAGResponse(BaseModel):
    question: str
    answer: str
    sources: List[Dict[str, Any]]
    tokens_used: int
    processing_time: float
    search_time: float
    generation_time: float


class RAGService:
    def __init__(self):
        self.http_client = None

    async def initialize(self):
        """Initialize HTTP client"""
        self.http_client = httpx.AsyncClient(timeout=60.0)
        rag_logger.info('‚úÖ RAG service initialized')

    async def cleanup(self):
        """Cleanup resources"""
        if self.http_client:
            await self.http_client.aclose()

    async def get_full_manual_content(self, document_id: str = None) -> str:
        """
        Extrai todo o conte√∫do do manual ICATU para usar como contexto completo.
        Se document_id for fornecido, extrai apenas desse documento espec√≠fico.
        """
        try:
            # Preparar payload para buscar todos os chunks do documento
            search_payload = {
                'query': 'altera√ß√£o cadastral procedimento documento',  # Query ampla para pegar tudo
                'limit': 1000,  # Limite alto para pegar todos os chunks
                'score_threshold': 0.1,  # Threshold muito baixo para incluir tudo
            }
            if document_id:
                search_payload['document_id'] = document_id

            response = await self.http_client.post(
                f'{DOCUMENT_PROCESSOR_URL}/search', json=search_payload
            )

            if response.status_code != 200:
                rag_logger.error(
                    f'Erro ao buscar conte√∫do completo: {response.status_code}'
                )
                return ''

            response_data = response.json()
            if isinstance(response_data, dict) and 'chunks' in response_data:
                all_chunks = response_data['chunks']
            else:
                all_chunks = response_data

            if not all_chunks:
                rag_logger.warning(
                    'Nenhum chunk encontrado para contexto completo'
                )
                return ''

            # Organizar chunks por ordem de documento/p√°gina se poss√≠vel
            sorted_chunks = sorted(
                all_chunks,
                key=lambda x: (
                    x.get('metadata', {}).get('page_number', 0),
                    x.get('metadata', {}).get('chunk_index', 0),
                ),
            )

            # Construir texto completo do manual
            manual_sections = []
            current_section = ''
            last_page = -1

            for chunk in sorted_chunks:
                content = chunk.get('content', '').strip()
                if not content:
                    continue

                metadata = chunk.get('metadata', {})
                page_number = metadata.get('page_number', 0)
                section_title = metadata.get('section_title', '')

                # Adicionar quebra de se√ß√£o se mudou de p√°gina ou h√° t√≠tulo de se√ß√£o
                if page_number != last_page and page_number > 0:
                    if current_section:
                        manual_sections.append(current_section)
                    current_section = f'\n--- P√ÅGINA {page_number} ---\n'
                    last_page = page_number

                # Adicionar t√≠tulo de se√ß√£o se dispon√≠vel
                if section_title and section_title not in current_section:
                    current_section += f'\n## {section_title}\n'

                # Adicionar conte√∫do
                current_section += f'{content}\n'

            # Adicionar √∫ltima se√ß√£o
            if current_section:
                manual_sections.append(current_section)

            full_manual = '\n'.join(manual_sections)

            rag_logger.info(
                f'üìñ Conte√∫do completo extra√≠do: {len(full_manual)} caracteres de {len(sorted_chunks)} chunks'
            )

            return full_manual

        except Exception as e:
            rag_logger.error(
                f'Erro ao extrair conte√∫do completo do manual: {e}'
            )
            return ''

    async def search_documents(
        self,
        query: str,
        limit: int = 3,
        score_threshold: float = 0.5,
        document_id: str = None,
    ) -> List[Dict[str, Any]]:
        """Search for relevant documents with enhanced multi-query strategy"""
        try:
            # 1. Expandir consulta com varia√ß√µes sem√¢nticas
            expanded_queries = self.expand_search_query(query)
            rag_logger.info(
                f'üîç Busca expandida: {len(expanded_queries)} varia√ß√µes da consulta'
            )

            all_results = []

            # 2. Buscar com m√∫ltiplas varia√ß√µes
            for search_query in expanded_queries:
                search_payload = {
                    'query': search_query,
                    'limit': min(limit * 2, 10),  # Mais resultados por query
                    'score_threshold': max(
                        score_threshold - 0.1, 0.3
                    ),  # Threshold mais permissivo
                }
                if document_id:
                    search_payload['document_id'] = document_id

                response = await self.http_client.post(
                    f'{DOCUMENT_PROCESSOR_URL}/search', json=search_payload
                )

                if response.status_code == 200:
                    response_data = response.json()
                    if (
                        isinstance(response_data, dict)
                        and 'chunks' in response_data
                    ):
                        results = response_data['chunks']
                    else:
                        results = response_data
                    all_results.extend(results)

            # 3. Deduplificar e ranquear resultados
            deduplicated_results = self.deduplicate_and_rank_results(
                all_results, query
            )

            # 4. Retornar melhores resultados
            final_results = deduplicated_results[:limit]
            rag_logger.info(
                f'üìä Busca finalizada: {len(final_results)} chunks selecionados de {len(all_results)} encontrados'
            )

            return final_results

        except Exception as e:
            rag_logger.error(f'Error searching documents: {e}')
            return []

    def expand_search_query(self, query: str) -> List[str]:
        """Expande a consulta com varia√ß√µes sem√¢nticas e sin√¥nimos"""
        expanded_queries = [query]  # Consulta original

        # Mapeamento de sin√¥nimos espec√≠ficos do dom√≠nio ICATU
        synonym_mapping = {
            'quem pode': [
                'quem consegue',
                'quem tem permiss√£o',
                'autorizado',
                'habilitado',
            ],
            'solicitar': ['pedir', 'requerer', 'realizar', 'fazer'],
            'altera√ß√£o': ['mudan√ßa', 'modifica√ß√£o', 'atualiza√ß√£o', 'corre√ß√£o'],
            'cadastral': ['cadastro', 'dados pessoais', 'informa√ß√µes'],
            'documento': ['documenta√ß√£o', 'pap√©is', 'comprovante'],
            'procedimento': [
                'processo',
                'fluxo',
                'passo a passo',
                'como fazer',
            ],
            'prazo': ['tempo', 'per√≠odo', 'dura√ß√£o', 'quando'],
            'titular': ['portador', 'segurado', 'cliente'],
            'cpf': ['documento federal', 'receita federal'],
            'endere√ßo': ['resid√™ncia', 'correspond√™ncia', 'localiza√ß√£o'],
            'telefone': ['contato', 'celular', 'n√∫mero'],
            'email': ['correio eletr√¥nico', 'e-mail'],
        }

        query_lower = query.lower()

        # Adicionar varia√ß√µes com sin√¥nimos
        for term, synonyms in synonym_mapping.items():
            if term in query_lower:
                for synonym in synonyms:
                    variant = query_lower.replace(term, synonym)
                    if variant != query_lower:
                        expanded_queries.append(variant)

        # Adicionar consultas focadas em palavras-chave
        keywords = self.extract_query_keywords(query)
        if len(keywords) > 1:
            # Consulta com apenas palavras-chave principais
            expanded_queries.append(' '.join(keywords[:3]))

            # Consultas individuais por palavra-chave
            for keyword in keywords[:2]:
                expanded_queries.append(keyword)

        # Adicionar varia√ß√µes espec√≠ficas por tipo de pergunta
        if any(word in query_lower for word in ['quem', 'pode', 'solicitar']):
            expanded_queries.extend(
                [
                    'titular pode solicitar',
                    'procurador autorizado',
                    'quem est√° habilitado',
                ]
            )
        elif any(
            word in query_lower
            for word in ['como', 'procedimento', 'processo']
        ):
            expanded_queries.extend(
                ['passo a passo', 'fluxo procedimento', 'como realizar']
            )
        elif any(word in query_lower for word in ['prazo', 'tempo', 'quando']):
            expanded_queries.extend(
                ['dias √∫teis', 'tempo necess√°rio', 'dura√ß√£o processo']
            )
        elif any(
            word in query_lower
            for word in ['documento', 'necess√°rio', 'exigido']
        ):
            expanded_queries.extend(
                [
                    'documentos obrigat√≥rios',
                    'pap√©is necess√°rios',
                    'comprovantes exigidos',
                ]
            )

        # Remover duplicatas e limitar
        unique_queries = []
        for q in expanded_queries:
            if q not in unique_queries:
                unique_queries.append(q)

        return unique_queries[:8]  # M√°ximo 8 varia√ß√µes

    def extract_query_keywords(self, query: str) -> List[str]:
        """Extrai palavras-chave importantes da consulta"""
        # Palavras irrelevantes
        stop_words = {
            'o',
            'a',
            'os',
            'as',
            'de',
            'da',
            'do',
            'em',
            'na',
            'no',
            'para',
            'por',
            'com',
            'como',
            'que',
            '√©',
            's√£o',
            'tem',
            'ter',
            'pode',
            'posso',
        }

        words = re.findall(r'\b[a-z√°√©√≠√≥√∫√ß√£√µ√¢√™√Æ]{3,}\b', query.lower())
        keywords = [word for word in words if word not in stop_words]

        return keywords

    def deduplicate_and_rank_results(
        self, results: List[Dict], original_query: str
    ) -> List[Dict]:
        """Remove duplicatas e ranqueia resultados por relev√¢ncia"""
        if not results:
            return []

        # 1. Remover duplicatas por conte√∫do
        seen_content = set()
        unique_results = []

        for result in results:
            content_key = result.get('content', '')[
                :100
            ]  # Primeiros 100 chars como chave
            if content_key not in seen_content:
                seen_content.add(content_key)
                unique_results.append(result)

        # 2. Calcular relev√¢ncia aprimorada
        for result in unique_results:
            relevance_score = self.calculate_enhanced_relevance(
                result, original_query
            )
            result['enhanced_score'] = relevance_score

        # 3. Ordenar por relev√¢ncia aprimorada
        unique_results.sort(
            key=lambda x: x.get('enhanced_score', 0), reverse=True
        )

        return unique_results

    def calculate_enhanced_relevance(self, result: Dict, query: str) -> float:
        """Calcula score de relev√¢ncia aprimorado"""
        base_score = result.get('score', 0)
        content = result.get('content', '').lower()
        metadata = result.get('metadata', {})

        # Fatores de relev√¢ncia
        relevance_factors = []

        # 1. Score base do embedding
        relevance_factors.append(base_score * 0.4)

        # 2. Correspond√™ncia exata de termos
        query_terms = self.extract_query_keywords(query)
        exact_matches = sum(1 for term in query_terms if term in content)
        term_score = (
            (exact_matches / len(query_terms)) * 0.3 if query_terms else 0
        )
        relevance_factors.append(term_score)

        # 3. Presen√ßa de palavras-chave importantes
        important_keywords = metadata.get('keywords', [])
        keyword_overlap = len(set(query_terms) & set(important_keywords))
        keyword_score = (keyword_overlap / max(len(query_terms), 1)) * 0.2
        relevance_factors.append(keyword_score)

        # 4. Tipo de se√ß√£o (algumas s√£o mais importantes)
        section_type = metadata.get('section_type', '')
        section_bonus = 0
        if 'title' in section_type or 'summary' in section_type:
            section_bonus = 0.1
        elif 'main_section' in section_type:
            section_bonus = 0.05
        relevance_factors.append(section_bonus)

        # 5. Comprimento adequado do conte√∫do
        content_length = len(content)
        length_score = 0
        if 200 <= content_length <= 1500:  # Tamanho ideal
            length_score = 0.1
        elif content_length > 50:  # M√≠nimo aceit√°vel
            length_score = 0.05
        relevance_factors.append(length_score)

        return sum(relevance_factors)

    def build_context_intelligent(
        self, search_results: List[Dict[str, Any]], question: str
    ) -> str:
        """
        Constru√ß√£o inteligente de contexto otimizada para m√°xima precis√£o
        Estrat√©gia: Hierarquia sem√¢ntica + Deduplica√ß√£o + Contexto estruturado
        """
        if not search_results:
            return ''

        rag_logger.info(
            f"üß† Construindo contexto inteligente para: '{question[:50]}...'"
        )
        rag_logger.info(
            f'üìä Processando {len(search_results)} chunks encontrados'
        )

        # 1. Filtrar e classificar resultados por qualidade
        high_quality_results = self.filter_high_quality_results(
            search_results, question
        )
        rag_logger.info(
            f'‚úÖ {len(high_quality_results)} chunks de alta qualidade selecionados'
        )

        # 2. Agrupar por relev√¢ncia sem√¢ntica
        grouped_results = self.group_by_semantic_relevance(
            high_quality_results, question
        )

        # 3. Construir contexto hier√°rquico estruturado
        structured_context = self.build_hierarchical_context(
            grouped_results, question
        )

        # 4. Otimizar tamanho do contexto
        final_context = self.optimize_context_size(
            structured_context, max_chars=3000
        )

        rag_logger.info(f'üìù Contexto final: {len(final_context)} caracteres')
        return final_context

    def filter_high_quality_results(
        self, results: List[Dict[str, Any]], question: str
    ) -> List[Dict[str, Any]]:
        """Filtra resultados de alta qualidade"""
        if not results:
            return []

        filtered_results = []
        query_keywords = self.extract_query_keywords(question.lower())

        for result in results:
            content = result.get('content', '').strip()
            score = result.get('score', 0)
            metadata = result.get('metadata', {})

            # Crit√©rios de qualidade
            quality_score = 0

            # 1. Score de embedding base
            if score > 0.8:
                quality_score += 3
            elif score > 0.6:
                quality_score += 2
            elif score > 0.4:
                quality_score += 1

            # 2. Tamanho adequado do conte√∫do
            content_length = len(content)
            if 100 <= content_length <= 2000:  # Tamanho ideal
                quality_score += 2
            elif content_length >= 50:  # M√≠nimo aceit√°vel
                quality_score += 1

            # 3. Correspond√™ncia de palavras-chave
            content_lower = content.lower()
            keyword_matches = sum(
                1 for keyword in query_keywords if keyword in content_lower
            )
            if keyword_matches >= 2:
                quality_score += 3
            elif keyword_matches >= 1:
                quality_score += 1

            # 4. Tipo de se√ß√£o importante
            section_type = metadata.get('section_type', '')
            if any(
                important_type in section_type
                for important_type in ['title', 'summary', 'main_section']
            ):
                quality_score += 2

            # 5. Presen√ßa de informa√ß√µes estruturadas
            if any(
                indicator in content_lower
                for indicator in [
                    'procedimento',
                    'documento',
                    'prazo',
                    'quem pode',
                ]
            ):
                quality_score += 1

            # Aceitar apenas resultados com qualidade suficiente
            if quality_score >= 3:
                result['quality_score'] = quality_score
                filtered_results.append(result)

        # Ordenar por qualidade e score combinados
        filtered_results.sort(
            key=lambda x: (x['quality_score'], x.get('score', 0)), reverse=True
        )

        return filtered_results[:8]  # M√°ximo 8 chunks de alta qualidade

    def group_by_semantic_relevance(
        self, results: List[Dict[str, Any]], question: str
    ) -> Dict[str, List[Dict]]:
        """Agrupa resultados por relev√¢ncia sem√¢ntica"""
        grouped = {
            'direct_answer': [],  # Resposta direta √† pergunta
            'procedure': [],  # Procedimentos e processos
            'requirements': [],  # Requisitos e documentos
            'timeframes': [],  # Prazos e tempos
            'authorization': [],  # Quem pode fazer
            'context': [],  # Contexto geral
        }

        question_lower = question.lower()

        for result in results:
            content = result.get('content', '').lower()
            metadata = result.get('metadata', {})
            section_title = metadata.get('section_title', '').lower()

            # Classificar por tipo de informa√ß√£o
            categorized = False

            # Perguntas sobre autoriza√ß√£o/permiss√£o
            if any(
                word in question_lower
                for word in ['quem', 'pode', 'autorizado', 'permitido']
            ):
                if any(
                    word in content
                    for word in [
                        'titular',
                        'pode',
                        'autorizado',
                        'procurador',
                        'curador',
                    ]
                ):
                    grouped['authorization'].append(result)
                    categorized = True

            # Perguntas sobre procedimentos
            if any(
                word in question_lower
                for word in ['como', 'procedimento', 'processo', 'passo']
            ):
                if any(
                    word in content
                    for word in [
                        'procedimento',
                        'processo',
                        'seguir',
                        'realizar',
                        'fluxo',
                    ]
                ):
                    grouped['procedure'].append(result)
                    categorized = True

            # Perguntas sobre documentos/requisitos
            if any(
                word in question_lower
                for word in [
                    'documento',
                    'necess√°rio',
                    'exigido',
                    'obrigat√≥rio',
                ]
            ):
                if any(
                    word in content
                    for word in [
                        'documento',
                        'necess√°rio',
                        'exigido',
                        'obrigat√≥rio',
                        'formul√°rio',
                    ]
                ):
                    grouped['requirements'].append(result)
                    categorized = True

            # Perguntas sobre prazos
            if any(
                word in question_lower
                for word in ['prazo', 'tempo', 'quando', 'dura√ß√£o']
            ):
                if any(
                    word in content
                    for word in ['prazo', 'dias', 'horas', 'tempo', '√∫teis']
                ):
                    grouped['timeframes'].append(result)
                    categorized = True

            # Se tem alta relev√¢ncia, considerar resposta direta
            if (
                result.get('score', 0) > 0.8
                or result.get('quality_score', 0) >= 5
            ):
                grouped['direct_answer'].append(result)
                categorized = True

            # Caso contr√°rio, adicionar ao contexto geral
            if not categorized:
                grouped['context'].append(result)

        return grouped

    def build_hierarchical_context(
        self, grouped_results: Dict[str, List[Dict]], question: str
    ) -> str:
        """Constr√≥i contexto hier√°rquico baseado na pergunta"""
        context_parts = []

        # Determinar prioridade baseada na pergunta
        question_lower = question.lower()

        if any(
            word in question_lower for word in ['quem', 'pode', 'autorizado']
        ):
            priority_order = [
                'authorization',
                'direct_answer',
                'requirements',
                'procedure',
                'timeframes',
                'context',
            ]
        elif any(
            word in question_lower
            for word in ['como', 'procedimento', 'processo']
        ):
            priority_order = [
                'procedure',
                'direct_answer',
                'requirements',
                'authorization',
                'timeframes',
                'context',
            ]
        elif any(
            word in question_lower
            for word in ['documento', 'necess√°rio', 'obrigat√≥rio']
        ):
            priority_order = [
                'requirements',
                'direct_answer',
                'procedure',
                'authorization',
                'timeframes',
                'context',
            ]
        elif any(
            word in question_lower for word in ['prazo', 'tempo', 'quando']
        ):
            priority_order = [
                'timeframes',
                'direct_answer',
                'procedure',
                'requirements',
                'authorization',
                'context',
            ]
        else:
            priority_order = [
                'direct_answer',
                'procedure',
                'requirements',
                'authorization',
                'timeframes',
                'context',
            ]

        # Construir contexto seguindo a prioridade
        section_headers = {
            'authorization': 'üë• QUEM PODE SOLICITAR',
            'procedure': 'üìã PROCEDIMENTOS',
            'requirements': 'üìÑ DOCUMENTOS NECESS√ÅRIOS',
            'timeframes': '‚è∞ PRAZOS',
            'direct_answer': 'üéØ INFORMA√á√ÉO PRINCIPAL',
            'context': 'üìå CONTEXTO ADICIONAL',
        }

        used_content = set()
        total_chars = 0
        max_chars_per_section = {
            'direct_answer': 800,
            'authorization': 600,
            'procedure': 700,
            'requirements': 600,
            'timeframes': 400,
            'context': 400,
        }

        for category in priority_order:
            if total_chars >= 2500:  # Limite global
                break

            results = grouped_results.get(category, [])
            if not results:
                continue

            section_content = []
            section_chars = 0
            max_section_chars = max_chars_per_section.get(category, 500)

            for result in results[:3]:  # M√°ximo 3 itens por categoria
                content = result.get('content', '').strip()
                content_key = content[:100]  # Chave para deduplica√ß√£o

                if content_key in used_content:
                    continue

                if section_chars + len(content) > max_section_chars:
                    # Truncar conte√∫do se necess√°rio
                    remaining_chars = max_section_chars - section_chars
                    if (
                        remaining_chars > 200
                    ):  # S√≥ truncar se sobrar espa√ßo razo√°vel
                        content = content[:remaining_chars] + '...'
                    else:
                        break

                used_content.add(content_key)
                score = result.get('score', 0)
                filename = result.get('metadata', {}).get('filename', 'ICATU')

                formatted_content = f'‚Ä¢ {content} [Relev√¢ncia: {score:.2f}]'
                section_content.append(formatted_content)
                section_chars += len(formatted_content)

                if section_chars >= max_section_chars:
                    break

            if section_content:
                header = section_headers.get(category, f'ÔøΩ {category.upper()}')
                section_text = f'\n{header}:\n' + '\n'.join(section_content)
                context_parts.append(section_text)
                total_chars += len(section_text)

        if not context_parts:
            return 'Informa√ß√£o espec√≠fica n√£o encontrada nos documentos dispon√≠veis.'

        # Adicionar cabe√ßalho contextual
        context_header = f"""CONTEXTO ICATU - Altera√ß√£o Cadastral
Pergunta: {question}
Fontes encontradas: {len([r for results in grouped_results.values() for r in results])} documentos

INFORMA√á√ïES RELEVANTES:"""

        full_context = context_header + '\n' + '\n'.join(context_parts)

        return full_context

    def optimize_context_size(
        self, context: str, max_chars: int = 3000
    ) -> str:
        """Otimiza o tamanho do contexto mantendo as informa√ß√µes mais importantes"""
        if len(context) <= max_chars:
            return context

        # Dividir em se√ß√µes
        sections = context.split('\n\n')

        # Priorizar se√ß√µes por import√¢ncia (baseado nos emojis/headers)
        priority_order = ['üéØ', 'üë•', 'üìã', 'üìÑ', '‚è∞', 'üìå']

        important_sections = []
        remaining_sections = []

        for section in sections:
            is_important = any(
                emoji in section for emoji in priority_order[:3]
            )
            if is_important:
                important_sections.append(section)
            else:
                remaining_sections.append(section)

        # Construir contexto otimizado
        optimized_context = ''

        # Adicionar se√ß√µes importantes primeiro
        for section in important_sections:
            if len(optimized_context) + len(section) <= max_chars:
                optimized_context += section + '\n\n'
            else:
                # Truncar se√ß√£o se necess√°rio
                remaining_space = max_chars - len(optimized_context) - 50
                if remaining_space > 200:
                    optimized_context += section[:remaining_space] + '...\n\n'
                break

        # Adicionar se√ß√µes restantes se houver espa√ßo
        for section in remaining_sections:
            if len(optimized_context) + len(section) <= max_chars:
                optimized_context += section + '\n\n'
            else:
                break

        return optimized_context.strip()

    async def generate_answer_enhanced(
        self,
        question: str,
        context: str,
        max_tokens: int = 500,
        temperature: float = 0.1,
    ) -> dict:
        """Generate enhanced answer using Mistral with advanced prompting strategy"""
        try:
            # Prompt engineering otimizado para m√°xima precis√£o
            enhanced_prompt = self.build_optimal_prompt(question, context)

            mistral_payload = {
                'question': question,
                'context': '',  # Deixar vazio para for√ßar uso do manual hardcoded
                'max_tokens': max_tokens,
                'temperature': min(
                    temperature, 0.15
                ),  # Temperatura muito baixa para m√°xima precis√£o
                'instructions': '',  # Sem instru√ß√µes adicionais - usar apenas o manual hardcoded
            }

            response = await self.http_client.post(
                f'{MISTRAL_SERVICE_URL}/query', json=mistral_payload
            )

            if response.status_code != 200:
                raise HTTPException(
                    status_code=500,
                    detail=f'Mistral service error: {response.text}',
                )

            result = response.json()
            rag_logger.debug(f'Mistral response: {result}')

            # P√≥s-processamento da resposta
            processed_answer = self.post_process_answer(
                result.get('answer', ''), question, context
            )
            result['answer'] = processed_answer

            return result

        except Exception as e:
            rag_logger.error(f'Error generating answer: {e}')
            raise HTTPException(
                status_code=500, detail=f'Answer generation failed: {str(e)}'
            )

    def build_optimal_prompt(self, question: str, context: str) -> str:
        """Constr√≥i o prompt otimizado para o Mistral"""

        # Determinar tipo de pergunta para personalizar o prompt
        question_type = self.classify_question_type(question)

        # Prompts especializados por tipo de pergunta
        specialized_instructions = {
            'authorization': """
Voc√™ √© um especialista em regulamenta√ß√µes ICATU. Responda com precis√£o sobre QUEM pode solicitar altera√ß√µes cadastrais.
FOQUE EM: autoriza√ß√£o, permiss√µes, quem est√° habilitado, limita√ß√µes.
FORMATO: Liste claramente quem pode e quem n√£o pode, com base APENAS no contexto.
""",
            'procedure': """
Voc√™ √© um especialista em processos ICATU. Explique COMO realizar procedimentos de altera√ß√£o cadastral.
FOQUE EM: passos, fluxos, sequ√™ncia de a√ß√µes, orienta√ß√µes pr√°ticas.
FORMATO: Liste os passos de forma ordenada e clara, baseado APENAS no contexto.
""",
            'requirements': """
Voc√™ √© um especialista em documenta√ß√£o ICATU. Liste QUAIS documentos s√£o necess√°rios.
FOQUE EM: documentos obrigat√≥rios, formul√°rios, comprovantes necess√°rios.
FORMATO: Liste todos os documentos exigidos, baseado APENAS no contexto.
""",
            'timeframes': """
Voc√™ √© um especialista em prazos ICATU. Informe QUANDO/QUANTO TEMPO leva cada processo.
FOQUE EM: prazos espec√≠ficos, dura√ß√£o, tempos de processamento.
FORMATO: Indique prazos claros e espec√≠ficos, baseado APENAS no contexto.
""",
            'general': """
Voc√™ √© um especialista em procedimentos ICATU. Responda de forma abrangente e precisa.
FOQUE EM: informa√ß√£o mais relevante para a pergunta espec√≠fica.
FORMATO: Resposta clara e direta, baseado APENAS no contexto.
""",
        }

        instruction = specialized_instructions.get(
            question_type, specialized_instructions['general']
        )

        # Prompt estruturado final
        enhanced_prompt = f"""SISTEMA: {instruction}

REGRAS CR√çTICAS:
1. Use APENAS informa√ß√µes do contexto fornecido abaixo
2. Se a informa√ß√£o n√£o estiver no contexto: "A informa√ß√£o solicitada n√£o est√° dispon√≠vel nos documentos fornecidos"
3. Seja espec√≠fico e objetivo
4. Mantenha terminologia t√©cnica ICATU
5. Cite procedimentos e prazos exatos quando dispon√≠veis
6. N√ÉO invente ou extrapole informa√ß√µes

CONTEXTO ICATU OFICIAL:
{context}

PERGUNTA ESPEC√çFICA: {question}

RESPOSTA T√âCNICA (baseada exclusivamente no contexto acima):"""

        return enhanced_prompt

    def classify_question_type(self, question: str) -> str:
        """Classifica o tipo de pergunta para personalizar o prompt"""
        question_lower = question.lower()

        if any(
            word in question_lower
            for word in [
                'quem',
                'pode',
                'autorizado',
                'permitido',
                'habilitado',
            ]
        ):
            return 'authorization'
        elif any(
            word in question_lower
            for word in [
                'como',
                'procedimento',
                'processo',
                'passo',
                'realizar',
            ]
        ):
            return 'procedure'
        elif any(
            word in question_lower
            for word in [
                'documento',
                'necess√°rio',
                'obrigat√≥rio',
                'exigido',
                'formul√°rio',
            ]
        ):
            return 'requirements'
        elif any(
            word in question_lower
            for word in ['prazo', 'tempo', 'quando', 'dura√ß√£o', 'demora']
        ):
            return 'timeframes'
        else:
            return 'general'

    def post_process_answer(
        self, answer: str, question: str, context: str
    ) -> str:
        """P√≥s-processa a resposta para melhorar qualidade e precis√£o"""
        if not answer or len(answer.strip()) < 10:
            return 'N√£o foi poss√≠vel gerar uma resposta adequada com base no contexto fornecido.'

        # Limpar resposta
        processed_answer = answer.strip()

        # Remover repeti√ß√µes desnecess√°rias
        lines = processed_answer.split('\n')
        unique_lines = []
        seen_content = set()

        for line in lines:
            line_clean = line.strip().lower()
            if line_clean and line_clean not in seen_content:
                seen_content.add(line_clean)
                unique_lines.append(line.strip())

        processed_answer = '\n'.join(unique_lines)

        # Adicionar formata√ß√£o se necess√°rio
        question_type = self.classify_question_type(question)

        if (
            question_type == 'authorization'
            and 'titular' in processed_answer.lower()
        ):
            if not processed_answer.startswith('**'):
                processed_answer = (
                    f'**QUEM PODE SOLICITAR:**\n{processed_answer}'
                )

        elif question_type == 'procedure' and any(
            word in processed_answer.lower()
            for word in ['procedimento', 'processo']
        ):
            if not processed_answer.startswith('**'):
                processed_answer = f'**PROCEDIMENTO:**\n{processed_answer}'

        elif (
            question_type == 'requirements'
            and 'documento' in processed_answer.lower()
        ):
            if not processed_answer.startswith('**'):
                processed_answer = (
                    f'**DOCUMENTOS NECESS√ÅRIOS:**\n{processed_answer}'
                )

        elif question_type == 'timeframes' and any(
            word in processed_answer.lower()
            for word in ['prazo', 'dias', 'horas']
        ):
            if not processed_answer.startswith('**'):
                processed_answer = f'**PRAZOS:**\n{processed_answer}'

        # Garantir que n√£o seja muito longo
        if len(processed_answer) > 800:
            # Truncar preservando frases completas
            sentences = processed_answer.split('.')
            truncated = ''
            for sentence in sentences:
                if len(truncated + sentence + '.') <= 750:
                    truncated += sentence + '.'
                else:
                    break
            if truncated:
                processed_answer = (
                    truncated
                    + '\n\n[Resposta truncada - consulte o documento completo para mais detalhes]'
                )

        return processed_answer

    async def generate_answer_with_full_context(
        self,
        question: str,
        full_manual_content: str,
        max_tokens: int = 1024,
        temperature: float = 0.3,
    ) -> dict:
        """Generate answer using full manual content as context"""
        try:
            mistral_payload = {
                'question': question,
                'full_document_topics': full_manual_content,
                'max_tokens': max_tokens,
                'temperature': temperature,
            }

            response = await self.http_client.post(
                f'{MISTRAL_SERVICE_URL}/query-full-context',
                json=mistral_payload,
            )

            if response.status_code != 200:
                raise HTTPException(
                    status_code=500,
                    detail=f'Mistral service error: {response.text}',
                )

            result = response.json()
            rag_logger.debug(f'Full context Mistral response: {result}')

            return result

        except Exception as e:
            rag_logger.error(f'Error generating answer with full context: {e}')
            raise HTTPException(
                status_code=500,
                detail=f'Full context answer generation failed: {str(e)}',
            )

    async def process_full_context_rag_query(
        self, request: FullContextRAGRequest
    ) -> RAGResponse:
        """Process a RAG query using the full manual as context"""
        start_time = time.time()

        rag_logger.info(
            f'üîÑ Processando consulta com contexto completo: {request.question}'
        )

        # Step 1: Get full manual content
        search_start = time.time()
        full_manual_content = await self.get_full_manual_content(
            request.document_id
        )
        search_time = time.time() - search_start

        if not full_manual_content:
            rag_logger.warning('Nenhum conte√∫do do manual encontrado')
            return RAGResponse(
                question=request.question,
                answer='N√£o foi poss√≠vel acessar o conte√∫do do manual para responder √† pergunta.',
                sources=[],
                tokens_used=0,
                processing_time=time.time() - start_time,
                search_time=search_time,
                generation_time=0.0,
            )

        rag_logger.info(
            f'üìñ Manual completo carregado: {len(full_manual_content)} caracteres'
        )

        # Step 2: Generate answer with full context
        generation_start = time.time()
        mistral_response = await self.generate_answer_with_full_context(
            question=request.question,
            full_manual_content=full_manual_content,
            max_tokens=request.max_tokens,
            temperature=request.temperature,
        )
        generation_time = time.time() - generation_start
        total_time = time.time() - start_time

        # Step 3: Prepare sources (indicating full manual was used)
        sources = [
            {
                'content_preview': 'Manual completo ICATU - Altera√ß√£o Cadastral utilizado como contexto',
                'score': 1.0,
                'metadata': {
                    'source_type': 'full_manual',
                    'content_length': len(full_manual_content),
                    'document_id': request.document_id or 'all_documents',
                },
            }
        ]

        rag_logger.info(
            f'‚úÖ Consulta com contexto completo processada em {total_time:.2f}s'
        )

        return RAGResponse(
            question=request.question,
            answer=mistral_response.get('answer', ''),
            sources=sources,
            tokens_used=mistral_response.get('tokens_used', 0),
            processing_time=total_time,
            search_time=search_time,
            generation_time=generation_time,
        )

    async def generate_answer(
        self,
        question: str,
        context: str,
        max_tokens: int = 500,
        temperature: float = 0.1,
    ) -> dict:
        """Generate answer using the enhanced method - compatibility wrapper"""
        response = await self.generate_answer_enhanced(
            question, context, max_tokens, temperature
        )

        # If response is a dict (expected), return it directly
        if isinstance(response, dict):
            return response

        # If response is a string, wrap it in the expected format
        return {
            'answer': str(response),
            'tokens_used': len(str(response).split()) if response else 0,
            'processing_time': 0.0,
        }

    async def process_rag_query(self, request: RAGRequest) -> RAGResponse:
        """Process a complete RAG query, optionally filtering by document_id"""
        start_time = time.time()
        # Step 1: Search for relevant documents
        search_start = time.time()
        search_results = await self.search_documents(
            query=request.question,
            limit=min(request.search_limit, 8),
            score_threshold=max(
                request.score_threshold, 0.2
            ),  # Threshold mais baixo
            document_id=request.document_id,
        )
        search_time = time.time() - search_start
        rag_logger.info(
            f'Found {len(search_results)} relevant documents in {search_time:.2f}s'
        )

        # Step 2: Build context
        context = self.build_context_intelligent(
            search_results, request.question
        )

        # Log the context length for debugging
        rag_logger.debug(f'Built context with {len(context)} characters')

        # Step 3: Generate answer with Mistral
        generation_start = time.time()
        if context:
            mistral_response = await self.generate_answer(
                question=request.question,
                context=context,
                max_tokens=request.max_tokens,
                temperature=request.temperature,
            )
        else:
            # Fallback: answer without context
            rag_logger.warning(
                'No relevant documents found, answering without context'
            )
            mistral_response = await self.generate_answer(
                question=request.question,
                context='',
                max_tokens=request.max_tokens,
                temperature=request.temperature,
            )
        generation_time = time.time() - generation_start
        total_time = time.time() - start_time
        # Step 4: Prepare sources information
        sources = []
        for result in search_results:
            source = {
                'content_preview': result.get('content', '')[:200] + '...'
                if len(result.get('content', '')) > 200
                else result.get('content', ''),
                'score': result.get('score', 0),
                'metadata': result.get('metadata', {}),
            }
            sources.append(source)
        return RAGResponse(
            question=request.question,
            answer=mistral_response.get('answer', ''),
            sources=sources,
            tokens_used=mistral_response.get('tokens_used', 0),
            processing_time=total_time,
            search_time=search_time,
            generation_time=generation_time,
        )


# Global service instance
rag_service = RAGService()


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan"""
    # Startup
    await rag_service.initialize()

    # Test connections
    try:
        async with httpx.AsyncClient() as client:
            # Test Mistral service
            mistral_response = await client.get(
                f'{MISTRAL_SERVICE_URL}/health'
            )
            if mistral_response.status_code == 200:
                rag_logger.info('‚úÖ Mistral service is accessible')
            else:
                rag_logger.warning('‚ö†Ô∏è Mistral service not accessible')

            # Test Document processor
            doc_response = await client.get(f'{DOCUMENT_PROCESSOR_URL}/health')
            if doc_response.status_code == 200:
                rag_logger.info('‚úÖ Document processor is accessible')
            else:
                rag_logger.warning('‚ö†Ô∏è Document processor not accessible')

    except Exception as e:
        rag_logger.error(f'Service connection test failed: {e}')

    yield

    # Shutdown
    await rag_service.cleanup()


# FastAPI app
app = FastAPI(
    title='RAG Knowledge Base API',
    description='Retrieval-Augmented Generation for document-based Q&A',
    version='1.0.0',
    lifespan=lifespan,
)


@app.get('/health')
async def health_check():
    """Health check endpoint"""
    return {
        'status': 'healthy',
        'service': 'rag-service',
        'dependencies': {
            'mistral_service': MISTRAL_SERVICE_URL,
            'document_processor': DOCUMENT_PROCESSOR_URL,
        },
    }


@app.post('/ask', response_model=RAGResponse)
async def ask_question(request: RAGRequest):
    """Ask a question with RAG (Retrieval-Augmented Generation)"""
    try:
        rag_logger.info(f'Recebida pergunta: {request.question}')
        response = await rag_service.process_rag_query(request)
        rag_logger.info(f'Resposta gerada para pergunta: {request.question}')
        return response
    except Exception as e:
        rag_logger.error(f'RAG query failed: {e}')
        raise HTTPException(
            status_code=500, detail=f'Query processing failed: {str(e)}'
        )


@app.post('/ask-full-context', response_model=RAGResponse)
async def ask_question_full_context(request: FullContextRAGRequest):
    """Ask a question using the full manual as context for more comprehensive answers"""
    try:
        rag_logger.info(
            f'Recebida pergunta com contexto completo: {request.question}'
        )
        response = await rag_service.process_full_context_rag_query(request)
        rag_logger.info(
            f'Resposta com contexto completo gerada para: {request.question}'
        )
        return response
    except Exception as e:
        rag_logger.error(f'Full context RAG query failed: {e}')
        raise HTTPException(
            status_code=500,
            detail=f'Full context query processing failed: {str(e)}',
        )


@app.get('/services/status')
async def get_services_status():
    """Check status of all dependent services"""
    status = {
        'mistral_service': {'url': MISTRAL_SERVICE_URL, 'status': 'unknown'},
        'document_processor': {
            'url': DOCUMENT_PROCESSOR_URL,
            'status': 'unknown',
        },
    }

    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            # Check Mistral service
            try:
                response = await client.get(f'{MISTRAL_SERVICE_URL}/health')
                status['mistral_service']['status'] = (
                    'healthy' if response.status_code == 200 else 'unhealthy'
                )
                status['mistral_service']['details'] = (
                    response.json()
                    if response.status_code == 200
                    else response.text
                )
            except Exception as e:
                status['mistral_service']['status'] = 'error'
                status['mistral_service']['error'] = str(e)

            # Check Document processor
            try:
                response = await client.get(f'{DOCUMENT_PROCESSOR_URL}/health')
                status['document_processor']['status'] = (
                    'healthy' if response.status_code == 200 else 'unhealthy'
                )
                status['document_processor']['details'] = (
                    response.json()
                    if response.status_code == 200
                    else response.text
                )
            except Exception as e:
                status['document_processor']['status'] = 'error'
                status['document_processor']['error'] = str(e)

    except Exception as e:
        rag_logger.error(f'Error checking services: {e}')

    return status


if __name__ == '__main__':
    uvicorn.run('rag_service:app', host='0.0.0.0', port=8002, log_level='info')
