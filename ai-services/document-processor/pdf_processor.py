"""
PDF Document Processor with Dolphin (ByteDance) for parsing and Sentence Transformers for embeddings
"""

import asyncio
import hashlib
import io
import logging
import os
import re
import time
import uuid
from typing import Any, Dict, List, Optional, Tuple

import httpx
import numpy as np
import PyPDF2
import torch

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
)

# SOLID principles applied
# SRP: Each class/function has a single responsibility
# OCP: Classes open for extension, closed for modification
# LSP: Subclasses can replace superclasses
# ISP: Specific interfaces for each operation
# DIP: Depend on abstractions


def preprocess_text_advanced(text: str) -> str:
    """Pr√©-processamento avan√ßado de texto com quebras estruturais"""
    # Preservar quebras importantes antes da normaliza√ß√£o
    text = text.replace('\n\n', '\n¬ßPARAGRAPH_BREAK¬ß\n')

    # Normalizar espa√ßos mas preservar quebras estruturais
    text = re.sub(r'[ \t]+', ' ', text)  # Espa√ßos horizontais

    # Detectar e criar quebras em se√ß√µes numeradas
    text = re.sub(
        r'(\d+\.)([A-Z])', r'\1\n\2', text
    )  # "1.QuemPode" -> "1.\nQuemPode"
    text = re.sub(
        r'([a-z])(\d+\.[A-Z])', r'\1\n\2', text
    )  # Quebra antes de numera√ß√£o

    # Quebras em letras de subdivis√£o (a), b), c))
    text = re.sub(r'([.!?])\s*([a-z]\))', r'\1\n\2', text)

    # Quebras em t√≠tulos em mai√∫sculas (m√≠nimo 3 palavras mai√∫sculas)
    text = re.sub(r'([.!?])\s*([A-Z][A-Z\s]{10,})', r'\1\n\2', text)

    # Quebras espec√≠ficas para ICATU
    icatu_breaks = [
        'Objetivo',
        'QuemPodeSolicitar',
        'TiposdeAltera√ß√µes',
        'Documentosnecess√°riosparaIdentifica√ß√£o',
        'Procedimentos',
        'ImportanteObservar',
        'Aten√ß√£o',
        'Observa√ß√£o',
        'Nota:',
    ]

    for break_word in icatu_breaks:
        # Adicionar quebra antes dessas palavras-chave
        pattern = r'([.!?:])\s*(' + re.escape(break_word) + ')'
        text = re.sub(pattern, r'\1\n\2', text, flags=re.IGNORECASE)

        # Tamb√©m quebrar se vier ap√≥s texto corrido
        pattern2 = r'([a-z])\s*(' + re.escape(break_word) + ')'
        text = re.sub(pattern2, r'\1\n\2', text, flags=re.IGNORECASE)

    # Quebras em listas com marcadores
    text = re.sub(r'([.!?])\s*([‚Ä¢¬∑*-]\s*)', r'\1\n\2', text)
    text = re.sub(r'([a-z])\s*([‚Ä¢¬∑*-]\s*[A-Z])', r'\1\n\2', text)

    # Quebras em final de senten√ßas longas seguidas de palavras-chave
    text = re.sub(
        r'([.!?])\s*(Para|Se|Caso|Quando|Ap√≥s|Durante)', r'\1\n\2', text
    )

    # Restaurar quebras de par√°grafo originais
    text = text.replace('\n¬ßPARAGRAPH_BREAK¬ß\n', '\n\n')

    # Normalizar quebras m√∫ltiplas
    text = re.sub(r'\n{3,}', '\n\n', text)  # M√°ximo 2 quebras consecutivas
    text = re.sub(r'\n\s+\n', '\n\n', text)  # Remove espa√ßos entre quebras

    # Preservar estrutura de listas e numera√ß√£o
    text = re.sub(r'\n\s*[‚Ä¢¬∑*-]\s*', '\n‚Ä¢ ', text)
    text = re.sub(r'\n\s*(\d+)[.):]\s*', r'\n\1. ', text)

    # Normalizar pontua√ß√£o espec√≠fica ICATU
    text = text.replace('ICATU Seguros', 'ICATU')
    text = text.replace('altera√ß√£o cadastral', 'altera√ß√£o cadastral')

    # Remover caracteres problem√°ticos
    text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\xff]', '', text)

    return text.strip()


class DocumentProcessorLogger:
    def __init__(self, name: str):
        self.logger = logging.getLogger(name)

    def info(self, msg):
        self.logger.info(msg)

    def error(self, msg):
        self.logger.error(msg)

    def debug(self, msg):
        self.logger.debug(msg)

    def warning(self, msg):
        self.logger.warning(msg)

    def warn(self, msg):
        self.logger.warning(msg)


# Example usage of the logger
doc_logger = DocumentProcessorLogger(__name__)
doc_logger.info('Document processor started.')
import logging
import uuid
from pathlib import Path
from typing import Any, Dict, List, Optional

import PyPDF2
import torch
import uvicorn
from fastapi import FastAPI, File, HTTPException, UploadFile
from pdfminer.high_level import extract_text
from PIL import Image
from pydantic import BaseModel
from qdrant_client import QdrantClient, models
from qdrant_client.models import Distance, PointStruct, VectorParams
from sentence_transformers import SentenceTransformer
from transformers import AutoModel, AutoTokenizer

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configuration
QDRANT_URL = 'http://qdrant:6333'
COLLECTION_NAME = 'knowledge_base_v2'
# EMBEDDING_MODEL = "sentence-transformers/all-mpnet-base-v2"  # Atual - bom para sem√¢ntica geral
EMBEDDING_MODEL = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'  # MELHOR para portugu√™s brasileiro
CHUNK_SIZE = 1024  # REDUZIDO para gerar mais chunks menores e espec√≠ficos
CHUNK_OVERLAP = 256  # PROPORCIONALMENTE ajustado
MIN_CHUNK_SIZE = 150  # REDUZIDO para permitir chunks menores mais granulares


class DocumentChunk(BaseModel):
    chunk_id: str
    text: str
    embedding: List[float]
    metadata: Dict[str, Any] = {}


class EmbeddingService:
    """Service for generating embeddings using SentenceTransformers"""

    def __init__(self, model_name: str = EMBEDDING_MODEL):
        self.model_name = model_name
        self.model = None
        self.embedding_dim = 768

    async def initialize(self):
        """Initialize the embedding model"""
        try:
            self.model = SentenceTransformer(self.model_name)
            logger.info(
                f'‚úÖ Embedding model {self.model_name} loaded successfully'
            )
        except Exception as e:
            logger.critical(
                f'‚ùå Failed to load embedding model: {e}. Embeddings will NOT be generated. Aborting.'
            )
            raise RuntimeError(f'Failed to load embedding model: {e}')

    async def embed_texts(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for a list of texts"""
        if not self.model:
            logger.critical(
                '‚ùå Embedding model is not loaded. Aborting embedding generation.'
            )
            raise RuntimeError('Embedding model is not loaded.')
        try:
            embeddings = self.model.encode(texts, convert_to_tensor=True)
            # Convert tensor to list if needed
            if torch.is_tensor(embeddings):
                embeddings = embeddings.cpu().numpy()

            # Ensure embeddings are valid
            if isinstance(embeddings, np.ndarray):
                # Convert numpy array to list of lists
                embeddings_list = embeddings.tolist()
            else:
                # If already list-like
                embeddings_list = embeddings

            # Validate each embedding
            for i, emb in enumerate(embeddings_list):
                if (
                    not isinstance(emb, (list, tuple))
                    or len(emb) != self.embedding_dim
                ):
                    logger.warning(f'‚ö†Ô∏è Fixing invalid embedding at index {i}')
                    # Create a fallback embedding if invalid
                    embeddings_list[i] = self._fallback_embedding(texts[i])

            return embeddings_list
        except Exception as e:
            logger.critical(
                f'‚ùå SentenceTransformer encoding failed: {e}. Using fallback.'
            )
            # Use fallback embeddings for all texts
            return [self._fallback_embedding(text) for text in texts]

    def _fallback_embedding(self, text: str) -> List[float]:
        """Fallback method to generate simple embeddings using text hashing"""
        import hashlib

        # Use multiple hash functions for more robust embedding
        hashes = [
            hashlib.md5(text.encode()).hexdigest(),
            hashlib.sha1(text.encode()).hexdigest()[:32],
            hashlib.sha256(text.encode()).hexdigest()[:32],
        ]

        embedding = []
        for hash_str in hashes:
            for i in range(0, len(hash_str), 2):
                hex_pair = hash_str[i : i + 2]
                embedding.append(int(hex_pair, 16) / 255.0)

        # Ensure fixed size
        if len(embedding) > self.embedding_dim:
            embedding = embedding[: self.embedding_dim]
        elif len(embedding) < self.embedding_dim:
            embedding.extend([0.0] * (self.embedding_dim - len(embedding)))

        return embedding


class ProcessingResponse(BaseModel):
    document_id: str
    filename: str
    chunks_count: int
    processing_time: float


class SearchRequest(BaseModel):
    query: str
    limit: int = 5
    score_threshold: float = 0.7


class SearchResult(BaseModel):
    content: str
    score: float
    metadata: Dict[str, Any]


class DocumentProcessor:
    def __init__(self):
        self.embedding_service = EmbeddingService()
        # Add compatibility mode for older Qdrant server versions
        self.qdrant = QdrantClient(url=QDRANT_URL, check_compatibility=False)

    async def initialize(self):
        """Initialize Qdrant client and embedding model"""
        # Initialize embedding model first
        logger.info(
            f'Initializing SentenceTransformer embeddings: {EMBEDDING_MODEL}'
        )
        self.embedding_model = EmbeddingService(EMBEDDING_MODEL)
        await self.embedding_model.initialize()

        # Then initialize Qdrant client
        try:
            self.qdrant_client = QdrantClient(
                url=QDRANT_URL, check_compatibility=False
            )
            await self.ensure_collection_exists()
            logger.info('‚úÖ Qdrant client initialized')
        except Exception as e:
            logger.error(f'Failed to initialize Qdrant client: {e}')
            raise

    async def ensure_collection_exists(self):
        """Create Qdrant collection if it doesn't exist, using the correct embedding dimension from the model."""
        try:
            collections = self.qdrant_client.get_collections()
            collection_names = [col.name for col in collections.collections]
            if COLLECTION_NAME not in collection_names:
                logger.info(f'Creating collection: {COLLECTION_NAME}')
                # Descobre a dimens√£o do modelo carregado
                embedding_dim = (
                    self.embedding_model.embedding_dim
                    if hasattr(self.embedding_model, 'embedding_dim')
                    else 768
                )
                self.qdrant_client.create_collection(
                    collection_name=COLLECTION_NAME,
                    vectors_config=models.VectorParams(
                        size=embedding_dim, distance=models.Distance.COSINE
                    ),
                )
                logger.info(
                    f'‚úÖ Collection {COLLECTION_NAME} created with dim {embedding_dim}'
                )
            else:
                # Verifica se a dimens√£o est√° correta
                info = self.qdrant_client.get_collection(COLLECTION_NAME)
                current_dim = info.config.params.vectors.size
                expected_dim = (
                    self.embedding_model.embedding_dim
                    if hasattr(self.embedding_model, 'embedding_dim')
                    else 768
                )
                if current_dim != expected_dim:
                    logger.error(
                        f'‚ùå Dimens√£o da cole√ß√£o Qdrant ({current_dim}) n√£o corresponde ao modelo ({expected_dim}). Exclua a cole√ß√£o manualmente ou altere o nome da cole√ß√£o.'
                    )
                    raise Exception(
                        f'Dimens√£o da cole√ß√£o Qdrant ({current_dim}) n√£o corresponde ao modelo ({expected_dim}). Exclua a cole√ß√£o manualmente ou altere o nome da cole√ß√£o.'
                    )
                logger.info(
                    f'‚úÖ Collection {COLLECTION_NAME} already exists with correct dim {current_dim}'
                )
        except Exception as e:
            logger.error(f'Error creating collection: {e}')
            raise

    def extract_text_with_docling(self, pdf_file: bytes) -> Optional[str]:
        """Extract text from PDF using Docling with proper error handling"""
        try:
            # Tentativa 1: Usar DocumentConverter (m√©todo mais recente)
            try:
                import io

                from docling.datamodel.base_models import DocumentStream
                from docling.document_converter import DocumentConverter

                logger.info('üîÑ Trying Docling DocumentConverter...')
                converter = DocumentConverter()

                # Criar stream do PDF
                pdf_stream = DocumentStream(
                    name='document.pdf', stream=io.BytesIO(pdf_file)
                )

                # Converter documento
                result = converter.convert(pdf_stream)

                if result and hasattr(result, 'document'):
                    # Extrair texto do documento convertido
                    if hasattr(result.document, 'export_to_markdown'):
                        text = result.document.export_to_markdown()
                        logger.info(
                            f'‚úÖ Docling DocumentConverter: {len(text)} chars extracted'
                        )
                        return text
                    elif hasattr(result.document, 'export_to_text'):
                        text = result.document.export_to_text()
                        logger.info(
                            f'‚úÖ Docling DocumentConverter: {len(text)} chars extracted'
                        )
                        return text
                    else:
                        # Tentar extrair texto de p√°ginas
                        pages_text = []
                        if hasattr(result.document, 'pages'):
                            for page in result.document.pages:
                                if hasattr(page, 'text'):
                                    pages_text.append(page.text)
                                elif hasattr(page, 'content'):
                                    pages_text.append(str(page.content))

                        if pages_text:
                            text = '\n'.join(pages_text)
                            logger.info(
                                f'‚úÖ Docling DocumentConverter (pages): {len(text)} chars extracted'
                            )
                            return text

            except ImportError as e:
                logger.info(f'üìã DocumentConverter not available: {e}')
            except Exception as e:
                logger.warning(f'‚ö†Ô∏è DocumentConverter failed: {e}')

            # Tentativa 2: Usar m√©todo direto do docling
            try:
                import io

                import docling

                logger.info('üîÑ Trying direct Docling import...')

                # Verificar se h√° uma classe Document dispon√≠vel
                if hasattr(docling, 'Document'):
                    doc = docling.Document.from_pdf(io.BytesIO(pdf_file))
                    if hasattr(doc, 'pages') and doc.pages:
                        texts = [
                            page.text
                            for page in doc.pages
                            if hasattr(page, 'text')
                        ]
                        if texts:
                            text = '\n'.join(texts)
                            logger.info(
                                f'‚úÖ Docling direct: {len(text)} chars extracted'
                            )
                            return text

                # Tentar outros m√©todos dispon√≠veis
                available_methods = [
                    attr for attr in dir(docling) if not attr.startswith('_')
                ]
                logger.info(
                    f'üìã Available Docling methods: {available_methods}'
                )

            except Exception as e:
                logger.warning(f'‚ö†Ô∏è Direct Docling failed: {e}')

            # Tentativa 3: Usar docling-parse se dispon√≠vel
            try:
                import io

                from docling_parse.pdf_parser import PdfParser

                logger.info('üîÑ Trying docling-parse...')
                parser = PdfParser()

                # Parse do PDF
                parsed_doc = parser.parse(io.BytesIO(pdf_file))

                if parsed_doc:
                    # Extrair texto de todas as p√°ginas
                    all_text = []

                    if hasattr(parsed_doc, 'pages'):
                        for page_num, page in enumerate(parsed_doc.pages):
                            page_text = ''

                            # Tentar diferentes m√©todos de extra√ß√£o
                            if hasattr(page, 'text'):
                                page_text = page.text
                            elif hasattr(page, 'get_text'):
                                page_text = page.get_text()
                            elif hasattr(page, 'content'):
                                page_text = str(page.content)

                            if page_text:
                                all_text.append(page_text)
                                logger.info(
                                    f'üìÑ Docling-parse page {page_num + 1}: {len(page_text)} chars'
                                )

                    if all_text:
                        text = '\n'.join(all_text)
                        logger.info(
                            f'‚úÖ Docling-parse: {len(text)} chars total'
                        )
                        return text

            except ImportError:
                logger.info('üìã docling-parse not available')
            except Exception as e:
                logger.warning(f'‚ö†Ô∏è docling-parse failed: {e}')

            # Tentativa 4: Usar outras bibliotecas docling dispon√≠veis
            try:
                # Verificar se h√° outros m√≥dulos docling dispon√≠veis
                import importlib
                import pkgutil

                logger.info('üîÑ Scanning for available docling modules...')

                # Tentar encontrar m√≥dulos docling
                docling_modules = []
                try:
                    import docling

                    for importer, modname, ispkg in pkgutil.iter_modules(
                        docling.__path__, docling.__name__ + '.'
                    ):
                        docling_modules.append(modname)
                        logger.info(f'üìã Found docling module: {modname}')
                except:
                    pass

                # Tentar usar docling_core se dispon√≠vel (opcional)
                try:
                    try:
                        from docling_core.types.doc import DoclingDocument

                        logger.info(
                            'üîÑ docling_core available but no direct PDF parser found'
                        )
                    except ImportError:
                        logger.info(
                            'üìã docling_core not available in this environment'
                        )
                    except Exception as e:
                        logger.warning(f'‚ùå docling_core import failed: {e}')

                except ImportError:
                    logger.info('üìã docling_core not available')
                except Exception as e:
                    logger.warning(f'‚ö†Ô∏è docling_core failed: {e}')

            except Exception as e:
                logger.warning(f'‚ö†Ô∏è Docling module scanning failed: {e}')

            logger.warning('‚ö†Ô∏è All Docling extraction methods failed')
            return None

        except Exception as e:
            logger.warning(f'‚ùå Docling extraction completely failed: {e}')
            return None

    def extract_text_from_pdf(self, pdf_file: bytes) -> str:
        """Extract text from PDF file with enhanced debugging and multiple fallbacks"""
        logger.info(
            f'üîç Starting PDF text extraction, file size: {len(pdf_file)} bytes'
        )

        # Try Docling first (most advanced)
        docling_text = self.extract_text_with_docling(pdf_file)
        if docling_text and len(docling_text.strip()) > 50:
            cleaned_text = self.validate_and_clean_extracted_text(
                docling_text, 'Docling'
            )
            if len(cleaned_text.strip()) > 50:
                logger.info(
                    f'‚úÖ Docling extraction successful: {len(cleaned_text)} characters'
                )
                logger.info(f'üìÑ Docling preview: {cleaned_text[:200]}...')
                return cleaned_text
        else:
            logger.info(
                f'‚ö†Ô∏è Docling extraction failed or insufficient text: {len(docling_text) if docling_text else 0} chars'
            )

        # Fallback para PyPDF2 com extra√ß√£o melhorada
        logger.info('üîÑ Trying enhanced PyPDF2 extraction...')
        try:
            import io

            pdf_reader = PyPDF2.PdfReader(io.BytesIO(pdf_file))
            text = ''
            page_count = len(pdf_reader.pages)
            logger.info(f'üìñ PDF has {page_count} pages')

            for i, page in enumerate(pdf_reader.pages):
                try:
                    # Tentar m√∫ltiplos m√©todos de extra√ß√£o por p√°gina
                    page_text = ''

                    # M√©todo 1: extract_text() padr√£o
                    try:
                        page_text = page.extract_text()
                        if page_text:
                            logger.debug(
                                f'üìÑ Page {i+1}: Standard extraction: {len(page_text)} chars'
                            )
                    except Exception as e:
                        logger.warning(
                            f'‚ö†Ô∏è Page {i+1}: Standard extraction failed: {e}'
                        )

                    # M√©todo 2: extract_text() com configura√ß√µes customizadas
                    if not page_text or len(page_text) < 50:
                        try:
                            # Tentar com diferentes configura√ß√µes
                            page_text_alt = page.extract_text(
                                extraction_mode='layout',
                                layout_mode_space_vertically=False,
                            )
                            if page_text_alt and len(page_text_alt) > len(
                                page_text
                            ):
                                page_text = page_text_alt
                                logger.debug(
                                    f'üìÑ Page {i+1}: Layout extraction better: {len(page_text)} chars'
                                )
                        except Exception as e:
                            logger.debug(
                                f'Page {i+1}: Layout extraction failed: {e}'
                            )

                    # M√©todo 3: Visitor pattern para extra√ß√£o mais detalhada
                    if not page_text or len(page_text) < 50:
                        try:

                            class TextVisitor:
                                def __init__(self):
                                    self.text = []

                                def visit_string(self, string_obj, *args):
                                    if hasattr(string_obj, 'get_data'):
                                        self.text.append(string_obj.get_data())
                                    elif hasattr(string_obj, '_data'):
                                        self.text.append(string_obj._data)
                                    else:
                                        self.text.append(str(string_obj))

                            visitor = TextVisitor()
                            if hasattr(page, 'extract_text'):
                                # Usar visitor se dispon√≠vel
                                try:
                                    page.extract_text(
                                        visitor=visitor.visit_string
                                    )
                                    visitor_text = ''.join(visitor.text)
                                    if visitor_text and len(
                                        visitor_text
                                    ) > len(page_text):
                                        page_text = visitor_text
                                        logger.debug(
                                            f'üìÑ Page {i+1}: Visitor extraction better: {len(page_text)} chars'
                                        )
                                except:
                                    pass
                        except Exception as e:
                            logger.debug(
                                f'Page {i+1}: Visitor extraction failed: {e}'
                            )

                    if page_text:
                        text += page_text + '\n'
                        logger.info(
                            f'üìÑ Page {i+1}: {len(page_text)} characters extracted'
                        )
                    else:
                        logger.warning(f'‚ö†Ô∏è Page {i+1}: No text extracted')

                except Exception as e:
                    logger.warning(
                        f'‚ö†Ô∏è Page {i+1}: Complete extraction error: {e}'
                    )

            if len(text.strip()) > 50:
                cleaned_text = self.validate_and_clean_extracted_text(
                    text.strip(), 'Enhanced PyPDF2'
                )
                if len(cleaned_text.strip()) > 50:
                    logger.info(
                        f'‚úÖ Enhanced PyPDF2 extraction successful: {len(cleaned_text)} characters total'
                    )
                    logger.info(
                        f'üìÑ Enhanced PyPDF2 preview: {cleaned_text[:200]}...'
                    )
                    return cleaned_text
            else:
                logger.warning(
                    f'‚ö†Ô∏è Enhanced PyPDF2 extracted insufficient text: {len(text)} chars'
                )
        except Exception as e:
            logger.warning(f'‚ùå Enhanced PyPDF2 parsing failed: {e}')

        # Fallback para pdfminer com configura√ß√µes melhoradas
        logger.info('üîÑ Trying enhanced pdfminer extraction...')
        try:
            import io

            from pdfminer.high_level import extract_text
            from pdfminer.layout import LAParams

            # Configura√ß√µes otimizadas para o pdfminer
            laparams = LAParams(
                boxes_flow=0.5,  # Melhor detec√ß√£o de colunas
                word_margin=0.1,  # Espa√ßamento entre palavras
                char_margin=2.0,  # Espa√ßamento entre caracteres
                line_margin=0.5,  # Espa√ßamento entre linhas
                all_texts=True,  # Extrair todo o texto dispon√≠vel
            )

            text = extract_text(
                io.BytesIO(pdf_file),
                laparams=laparams,
                maxpages=0,  # Processar todas as p√°ginas
                password='',
                caching=True,
                check_extractable=True,
            )

            if text and len(text.strip()) > 50:
                cleaned_text = self.validate_and_clean_extracted_text(
                    text.strip(), 'Enhanced pdfminer'
                )
                if len(cleaned_text.strip()) > 50:
                    logger.info(
                        f'‚úÖ Enhanced pdfminer extraction successful: {len(cleaned_text)} characters'
                    )
                    logger.info(
                        f'üìÑ Enhanced pdfminer preview: {cleaned_text[:200]}...'
                    )
                    return cleaned_text
            else:
                logger.warning(
                    f'‚ö†Ô∏è Enhanced pdfminer extracted insufficient text: {len(text) if text else 0} chars'
                )

        except Exception as e:
            logger.error(f'‚ùå Enhanced pdfminer parsing failed: {e}')

        # √öltimo recurso: pdfplumber se dispon√≠vel
        logger.info('üîÑ Trying pdfplumber as last resort...')
        try:
            try:
                import io

                import pdfplumber

                text = ''
                with pdfplumber.open(io.BytesIO(pdf_file)) as pdf:
                    logger.info(
                        f'üìñ Pdfplumber: PDF has {len(pdf.pages)} pages'
                    )

                    for i, page in enumerate(pdf.pages):
                        try:
                            page_text = page.extract_text()
                            if page_text:
                                text += page_text + '\n'
                                logger.info(
                                    f'üìÑ Pdfplumber page {i+1}: {len(page_text)} chars'
                                )
                        except Exception as e:
                            logger.warning(f'‚ö†Ô∏è Pdfplumber page {i+1}: {e}')

                if text and len(text.strip()) > 50:
                    cleaned_text = self.validate_and_clean_extracted_text(
                        text.strip(), 'Pdfplumber'
                    )
                    if len(cleaned_text.strip()) > 50:
                        logger.info(
                            f'‚úÖ Pdfplumber extraction successful: {len(cleaned_text)} characters'
                        )
                        logger.info(
                            f'üìÑ Pdfplumber preview: {cleaned_text[:200]}...'
                        )
                        return cleaned_text
                else:
                    logger.warning(
                        f'‚ö†Ô∏è Pdfplumber extracted insufficient text: {len(text) if text else 0} chars'
                    )

            except ImportError:
                logger.info(
                    'üìã pdfplumber not available - install with: poetry add pdfplumber'
                )
            except Exception as e:
                logger.warning(f'‚ùå pdfplumber extraction failed: {e}')

        except Exception as outer_e:
            logger.warning(f'‚ùå pdfplumber setup failed: {outer_e}')

        # If all methods fail
        logger.error('‚ùå All PDF extraction methods failed!')
        raise HTTPException(
            status_code=400,
            detail='Failed to extract text from PDF using all available methods (Docling, PyPDF2, pdfminer, pdfplumber)',
        )

    def validate_and_clean_extracted_text(
        self, text: str, filename: str
    ) -> str:
        """Validate and clean extracted text to ensure quality"""
        if not text:
            logger.warning(f'‚ö†Ô∏è Empty text extracted from {filename}')
            return ''

        original_length = len(text)

        # 1. Remove caracteres de controle problem√°ticos
        text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]', '', text)

        # 2. Normalizar espa√ßos em branco excessivos
        text = re.sub(r'\s{4,}', ' ', text)  # M√°ximo 3 espa√ßos consecutivos

        # 3. Preservar quebras de linha importantes mas remover excessivas
        text = re.sub(r'\n{4,}', '\n\n\n', text)  # M√°ximo 3 quebras

        # 4. Remover caracteres Unicode problem√°ticos que podem afetar tokeniza√ß√£o
        text = re.sub(
            r'[\ufeff\u200b-\u200d\u2060]', '', text
        )  # Zero-width chars

        # 5. Verificar se h√° conte√∫do significativo
        meaningful_chars = re.sub(r'[\s\n\r\t]', '', text)
        if len(meaningful_chars) < 50:
            logger.warning(
                f'‚ö†Ô∏è Very little meaningful content in {filename}: {len(meaningful_chars)} chars'
            )

        # 6. Detectar poss√≠veis problemas de encoding
        if text.count('ÔøΩ') > 5:  # Muitos caracteres de replacement
            logger.warning(
                f"‚ö†Ô∏è Possible encoding issues in {filename}: {text.count('ÔøΩ')} replacement chars"
            )

        # 7. Verificar se o texto parece ser portugu√™s/ingl√™s v√°lido
        alphabet_chars = re.findall(r'[a-zA-Z√Ä-√ø]', text)
        total_visible_chars = re.findall(r'[^\s\n\r\t]', text)

        if (
            total_visible_chars
            and len(alphabet_chars) / len(total_visible_chars) < 0.5
        ):
            logger.warning(
                f'‚ö†Ô∏è Text might contain many non-alphabetic characters in {filename}'
            )

        # 8. Log estat√≠sticas de limpeza
        cleaned_length = len(text)
        if cleaned_length != original_length:
            logger.info(
                f'üßπ Text cleaned: {original_length} -> {cleaned_length} chars ({filename})'
            )

        # 9. Garantir que o texto termina adequadamente
        text = text.strip()
        if text and not text.endswith(('.', '!', '?', ':')):
            text += '.'

        return text

    def smart_structural_chunking(
        self, text: str, filename: str
    ) -> List[DocumentChunk]:
        """
        Chunking inteligente otimizado para preservar TODO o contexto do documento
        Estrat√©gia: Divis√£o sem√¢ntica com overlap inteligente e estrutura hier√°rquica
        """
        logger.info(
            f'üß† Iniciando chunking estrutural inteligente para {filename}'
        )
        logger.info(f'üìä Tamanho do texto de entrada: {len(text)} caracteres')

        # 1. Limpeza e normaliza√ß√£o avan√ßada preservando estrutura
        clean_text = self.advanced_text_cleaning_preserving_structure(text)
        logger.info(f'üßπ Ap√≥s limpeza estrutural: {len(clean_text)} caracteres')

        # 2. Detec√ß√£o de estrutura hier√°rquica do documento
        document_structure = self.detect_hierarchical_structure(clean_text)
        logger.info(
            f"üìã Estrutura detectada: {len(document_structure['sections'])} se√ß√µes principais"
        )

        # 3. Cria√ß√£o de chunks com estrat√©gia multi-n√≠vel
        chunks = self.create_hierarchical_chunks(document_structure, filename)

        # 4. Adi√ß√£o de chunks de contexto global
        global_chunks = self.create_global_context_chunks(
            clean_text, filename, len(chunks)
        )
        chunks.extend(global_chunks)

        # 5. Valida√ß√£o final e otimiza√ß√£o
        optimized_chunks = self.optimize_chunks_for_retrieval(chunks)

        logger.info(f'‚úÖ Chunking conclu√≠do para {filename}:')
        logger.info(f'   üìä {len(optimized_chunks)} chunks gerados')
        logger.info(
            f'   üìè Cobertura: {self.calculate_coverage(optimized_chunks, clean_text):.1f}% do documento'
        )

        if len(optimized_chunks) > 0:
            avg_size = sum(len(c.text) for c in optimized_chunks) // len(
                optimized_chunks
            )
            logger.info(f'   üìù Tamanho m√©dio: {avg_size} caracteres/chunk')
        else:
            logger.warning('   ‚ö†Ô∏è Nenhum chunk v√°lido foi gerado!')
            return []

        return optimized_chunks

    def advanced_text_cleaning_preserving_structure(self, text: str) -> str:
        """Limpeza avan√ßada que preserva a estrutura sem√¢ntica do documento"""
        logger.info('üßπ Iniciando limpeza preservando estrutura...')

        # 1. Normalizar encoding e caracteres problem√°ticos
        text = text.encode('utf-8', errors='ignore').decode('utf-8')

        # 2. Preservar quebras de se√ß√£o importantes
        text = re.sub(
            r'(#{1,6}\s+[^\n]+)', r'\n\1\n', text
        )  # Headers Markdown
        text = re.sub(
            r'(\d+\.\s+[A-Z][^\n]+)', r'\n\1\n', text
        )  # Se√ß√µes numeradas

        # 3. Corrigir caracteres com problemas de encoding
        encoding_fixes = {
            'Alterao': 'Altera√ß√£o',
            'Capitalizao': 'Capitaliza√ß√£o',
            'Informaes': 'Informa√ß√µes',
            'Solicitao': 'Solicita√ß√£o',
            'Identificao': 'Identifica√ß√£o',
            'Manifestao': 'Manifesta√ß√£o',
            'Orientao': 'Orienta√ß√£o',
            'Concluso': 'Conclus√£o',
            'Atualizao': 'Atualiza√ß√£o',
            'Incluso': 'Inclus√£o',
            'Expediao': 'Expedi√ß√£o',
            'Aplicao': 'Aplica√ß√£o',
            'Validao': 'Valida√ß√£o',
            'Autenticao': 'Autentica√ß√£o',
            'Manifestaes': 'Manifesta√ß√µes',
            'Orientaes': 'Orienta√ß√µes',
            'Solicitaes': 'Solicita√ß√µes',
            'Procurao': 'Procura√ß√£o',
            'Telefone/E-mail': 'Telefone / E-mail',
            'amp;': '&',
        }

        for wrong, correct in encoding_fixes.items():
            text = text.replace(wrong, correct)

        # 4. Normalizar espa√ßamentos mantendo estrutura
        text = re.sub(r'\n{4,}', '\n\n\n', text)  # Max 3 quebras
        text = re.sub(r' {3,}', '  ', text)  # Max 2 espa√ßos

        # 5. Garantir pontua√ß√£o adequada
        text = re.sub(
            r'([a-zA-Z])(\n#{1,6})', r'\1.\n\2', text
        )  # Ponto antes de headers

        logger.info(
            f'üßπ Limpeza conclu√≠da: {len(encoding_fixes)} corre√ß√µes aplicadas'
        )
        return text.strip()

    def detect_hierarchical_structure(self, text: str) -> Dict[str, Any]:
        """Detecta a estrutura hier√°rquica completa do documento"""
        logger.info('üìã Detectando estrutura hier√°rquica...')

        lines = text.split('\n')
        structure = {
            'title': '',
            'sections': [],
            'metadata': {
                'total_lines': len(lines),
                'estimated_reading_time': len(text) // 1000,  # ~1000 chars/min
                'document_type': 'manual',
            },
        }

        current_section = None
        current_subsection = None
        section_patterns = [
            (r'^#{1,2}\s+(.+)', 'main_section', 1),
            (r'^#{3,4}\s+(.+)', 'subsection', 2),
            (r'^(\d+)\.\s+(.+)', 'numbered_section', 1),
            (r'^([a-z])\)\s+(.+)', 'lettered_subsection', 2),
            (r'^([A-Z][A-Z\s]{3,}):?\s*$', 'emphasis_section', 1),
            (r'^(.+):$', 'colon_section', 2),
        ]

        for line_num, line in enumerate(lines):
            line = line.strip()
            if not line:
                continue

            # Detectar t√≠tulo principal
            if line_num < 5 and (
                'manual' in line.lower() or 'icatu' in line.lower()
            ):
                structure['title'] = line
                continue

            # Detectar se√ß√µes
            section_detected = False
            for pattern, section_type, level in section_patterns:
                match = re.match(pattern, line, re.IGNORECASE)
                if match:
                    if level == 1:  # Se√ß√£o principal
                        if current_section:
                            structure['sections'].append(current_section)
                        current_section = {
                            'title': match.group(1)
                            if len(match.groups()) == 1
                            else match.group(2),
                            'type': section_type,
                            'level': level,
                            'content': [],
                            'subsections': [],
                            'start_line': line_num,
                            'keywords': [],
                            'topics': [],
                        }
                        current_subsection = None
                    elif level == 2 and current_section:  # Subse√ß√£o
                        if current_subsection:
                            current_section['subsections'].append(
                                current_subsection
                            )
                        current_subsection = {
                            'title': match.group(1)
                            if len(match.groups()) == 1
                            else match.group(2),
                            'type': section_type,
                            'level': level,
                            'content': [],
                            'start_line': line_num,
                            'keywords': [],
                            'topics': [],
                        }
                    section_detected = True
                    break

            if not section_detected:
                # Adicionar conte√∫do √† se√ß√£o/subse√ß√£o atual
                if current_subsection:
                    current_subsection['content'].append(line)
                elif current_section:
                    current_section['content'].append(line)

        # Finalizar √∫ltima se√ß√£o
        if current_subsection and current_section:
            current_section['subsections'].append(current_subsection)
        if current_section:
            structure['sections'].append(current_section)

        # Enriquecer se√ß√µes com metadados
        for section in structure['sections']:
            section['keywords'] = self.extract_section_keywords(section)
            section['topics'] = self.extract_section_topics(section)
            section['end_line'] = (
                section['start_line']
                + len(section['content'])
                + len(section['subsections'])
            )

            for subsection in section['subsections']:
                subsection['keywords'] = self.extract_section_keywords(
                    subsection
                )
                subsection['topics'] = self.extract_section_topics(subsection)

        logger.info(
            f"üìã Estrutura detectada: {len(structure['sections'])} se√ß√µes, t√≠tulo: '{structure['title'][:50]}...'"
        )
        return structure

    def create_hierarchical_chunks(
        self, structure: Dict[str, Any], filename: str
    ) -> List[DocumentChunk]:
        """Cria chunks preservando a hierarquia e contexto sem√¢ntico com tamanho adequado"""
        logger.info('üì¶ Criando chunks hier√°rquicos...')

        chunks = []
        chunk_index = 0

        # 1. Chunk do t√≠tulo e contexto geral
        if structure['title']:
            title_context = f"""# {structure['title']}

Este √© um manual da ICATU sobre procedimentos de altera√ß√£o cadastral.

PRINCIPAIS T√ìPICOS ABORDADOS:
‚Ä¢ Quem pode solicitar altera√ß√µes cadastrais
‚Ä¢ Tipos de altera√ß√µes permitidas 
‚Ä¢ Documentos necess√°rios
‚Ä¢ Procedimentos e prazos
‚Ä¢ Canais de atendimento

Este documento √© essencial para operadores que processam solicita√ß√µes de altera√ß√£o cadastral na ICATU."""

            title_chunk = DocumentChunk(
                chunk_id=f'{filename}_title_{chunk_index:03d}',
                text=title_context,
                embedding=[],
                metadata={
                    'filename': filename,
                    'section_title': 'T√≠tulo Principal',
                    'section_type': 'title',
                    'section_index': chunk_index,
                    'is_title': True,
                    'topics': [
                        'manual',
                        'icatu',
                        'alteracao_cadastral',
                        'procedimentos',
                    ],
                    'keywords': [
                        'manual',
                        'icatu',
                        'altera√ß√£o',
                        'cadastral',
                        'procedimentos',
                    ],
                    'context_summary': 'Documento principal sobre altera√ß√µes cadastrais na ICATU',
                },
            )
            chunks.append(title_chunk)
            chunk_index += 1

        # 2. Agrupar se√ß√µes pequenas para formar chunks maiores
        current_group = []
        current_group_size = 0
        target_chunk_size = 800  # Tamanho alvo para cada chunk
        min_chunk_size = 300    # Tamanho m√≠nimo aceit√°vel

        for section_idx, section in enumerate(structure['sections']):
            section_content = self.build_section_content(section)
            section_size = len(section_content)

            # Se a se√ß√£o √© muito grande, criar chunk individual
            if section_size > target_chunk_size * 1.5:
                # Finalizar grupo atual se existir
                if current_group:
                    combined_chunk = self.create_combined_chunk(
                        current_group, filename, chunk_index
                    )
                    if combined_chunk:
                        chunks.append(combined_chunk)
                        chunk_index += 1
                    current_group = []
                    current_group_size = 0

                # Dividir se√ß√£o grande em chunks com overlap
                section_chunks = self.create_overlapping_chunks(
                    section_content, section, filename, chunk_index
                )
                chunks.extend(section_chunks)
                chunk_index += len(section_chunks)

            # Se adicionar esta se√ß√£o n√£o excede o limite, adicionar ao grupo
            elif current_group_size + section_size <= target_chunk_size * 1.2:
                current_group.append((section, section_content))
                current_group_size += section_size

            # Se excede, finalizar grupo atual e iniciar novo
            else:
                if current_group:
                    combined_chunk = self.create_combined_chunk(
                        current_group, filename, chunk_index
                    )
                    if combined_chunk:
                        chunks.append(combined_chunk)
                        chunk_index += 1

                # Iniciar novo grupo
                current_group = [(section, section_content)]
                current_group_size = section_size

        # Finalizar √∫ltimo grupo
        if current_group:
            combined_chunk = self.create_combined_chunk(
                current_group, filename, chunk_index
            )
            if combined_chunk:
                chunks.append(combined_chunk)
                chunk_index += 1

        return chunks

    def create_combined_chunk(
        self, section_group: List, filename: str, chunk_index: int
    ) -> Optional[DocumentChunk]:
        """Combina m√∫ltiplas se√ß√µes em um chunk substancial"""
        if not section_group:
            return None

        combined_content = []
        combined_topics = []
        combined_keywords = []
        main_section_title = section_group[0][0]['title']

        for section, content in section_group:
            combined_content.append(content)
            combined_topics.extend(section.get('topics', []))
            combined_keywords.extend(section.get('keywords', []))

        full_content = '\n\n'.join(combined_content)

        # Verificar se o chunk tem tamanho adequado
        if len(full_content.strip()) < 200:
            return None

        # Remover duplicatas e limitar listas
        unique_topics = list(set(combined_topics))[:10]
        unique_keywords = list(set(combined_keywords))[:15]

        chunk = DocumentChunk(
            chunk_id=f'{filename}_combined_{chunk_index:03d}',
            text=full_content,
            embedding=[],
            metadata={
                'filename': filename,
                'section_title': main_section_title,
                'section_type': 'combined_section',
                'section_index': chunk_index,
                'hierarchical_level': 1,
                'topics': unique_topics,
                'keywords': unique_keywords,
                'context_summary': self.generate_context_summary(full_content),
                'sections_count': len(section_group),
                'combined_sections': [s[0]['title'] for s, _ in section_group],
            },
        )

        return chunk

    def build_section_content(self, section: Dict[str, Any]) -> str:
        """Constr√≥i o conte√∫do completo de uma se√ß√£o com contexto"""
        content_parts = []

        # T√≠tulo da se√ß√£o
        content_parts.append(f"## {section['title']}")

        # Conte√∫do principal da se√ß√£o
        if section['content']:
            content_parts.append('\n'.join(section['content']))

        # Subse√ß√µes
        for subsection in section['subsections']:
            content_parts.append(f"\n### {subsection['title']}")
            if subsection['content']:
                content_parts.append('\n'.join(subsection['content']))

        return '\n\n'.join(content_parts)

    def create_overlapping_chunks(
        self,
        content: str,
        section: Dict[str, Any],
        filename: str,
        start_index: int,
    ) -> List[DocumentChunk]:
        """Cria chunks com overlap inteligente para se√ß√µes grandes"""
        chunks = []
        max_chunk_size = 1500
        overlap_size = 200

        # Dividir por par√°grafos preservando contexto
        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]

        current_chunk = ''
        chunk_paragraphs = []
        chunk_count = 0

        for i, paragraph in enumerate(paragraphs):
            # Verificar se adicionar este par√°grafo excederia o limite
            potential_chunk = (
                current_chunk + '\n\n' + paragraph
                if current_chunk
                else paragraph
            )

            if len(potential_chunk) > max_chunk_size and current_chunk:
                # Criar chunk atual
                chunk_text = self.add_section_context(current_chunk, section)
                chunk = DocumentChunk(
                    chunk_id=f'{filename}_section_{start_index + chunk_count:03d}',
                    text=chunk_text,
                    embedding=[],
                    metadata={
                        'filename': filename,
                        'section_title': section['title'],
                        'section_type': f"{section['type']}_part",
                        'section_index': start_index + chunk_count,
                        'part_number': chunk_count + 1,
                        'hierarchical_level': section['level'],
                        'topics': section['topics'],
                        'keywords': self.extract_keywords_from_text(
                            current_chunk
                        ),
                        'context_summary': self.generate_context_summary(
                            current_chunk
                        ),
                        'is_continuation': chunk_count > 0,
                        'has_next_part': i < len(paragraphs) - 1,
                    },
                )
                chunks.append(chunk)

                # Preparar pr√≥ximo chunk com overlap
                overlap_text = self.get_overlap_text(
                    current_chunk, overlap_size
                )
                current_chunk = (
                    overlap_text + '\n\n' + paragraph
                    if overlap_text
                    else paragraph
                )
                chunk_count += 1
            else:
                current_chunk = potential_chunk

        # √öltimo chunk
        if current_chunk:
            chunk_text = self.add_section_context(current_chunk, section)
            chunk = DocumentChunk(
                chunk_id=f'{filename}_section_{start_index + chunk_count:03d}',
                text=chunk_text,
                embedding=[],
                metadata={
                    'filename': filename,
                    'section_title': section['title'],
                    'section_type': f"{section['type']}_final",
                    'section_index': start_index + chunk_count,
                    'part_number': chunk_count + 1,
                    'hierarchical_level': section['level'],
                    'topics': section['topics'],
                    'keywords': self.extract_keywords_from_text(current_chunk),
                    'context_summary': self.generate_context_summary(
                        current_chunk
                    ),
                    'is_continuation': chunk_count > 0,
                    'has_next_part': False,
                },
            )
            chunks.append(chunk)

        return chunks

    def add_section_context(
        self, content: str, section: Dict[str, Any]
    ) -> str:
        """Adiciona contexto da se√ß√£o ao chunk"""
        context_prefix = f"[CONTEXTO: {section['title']}]\n\n"
        return context_prefix + content

    def get_overlap_text(self, text: str, overlap_size: int) -> str:
        """Extrai texto de overlap do final do chunk atual"""
        sentences = text.split('.')
        overlap_text = ''
        for sentence in reversed(sentences):
            potential_overlap = sentence.strip() + '. ' + overlap_text
            if len(potential_overlap) <= overlap_size:
                overlap_text = potential_overlap
            else:
                break
        return overlap_text.strip()

    def create_global_context_chunks(
        self, text: str, filename: str, existing_count: int
    ) -> List[DocumentChunk]:
        """Cria chunks de contexto global substanciais para consultas gerais"""
        logger.info('üåê Criando chunks de contexto global...')

        chunks = []

        # 1. Chunk de resumo executivo expandido
        summary = self.generate_comprehensive_executive_summary(text)
        summary_chunk = DocumentChunk(
            chunk_id=f'{filename}_summary_{existing_count:03d}',
            text=summary,
            embedding=[],
            metadata={
                'filename': filename,
                'section_title': 'Resumo Executivo',
                'section_type': 'executive_summary',
                'section_index': existing_count,
                'is_summary': True,
                'topics': [
                    'resumo',
                    'geral',
                    'overview',
                    'alteracao_cadastral',
                    'procedimentos',
                ],
                'keywords': [
                    'resumo',
                    'geral',
                    'principal',
                    'importante',
                    'altera√ß√£o',
                    'cadastral',
                    'procedimentos',
                ],
                'context_summary': 'Resumo executivo completo de todo o documento',
            },
        )
        chunks.append(summary_chunk)

        # 2. Chunk de procedimentos principais
        procedures_summary = self.generate_procedures_summary(text)
        procedures_chunk = DocumentChunk(
            chunk_id=f'{filename}_procedures_{existing_count + 1:03d}',
            text=procedures_summary,
            embedding=[],
            metadata={
                'filename': filename,
                'section_title': 'Procedimentos Principais',
                'section_type': 'procedures_summary',
                'section_index': existing_count + 1,
                'is_procedures': True,
                'topics': [
                    'procedimentos',
                    'como_fazer',
                    'passo_a_passo',
                    'fluxo',
                ],
                'keywords': [
                    'procedimento',
                    'como',
                    'fazer',
                    'passo',
                    'fluxo',
                    'processo',
                ],
                'context_summary': 'Compila√ß√£o de todos os procedimentos do documento',
            },
        )
        chunks.append(procedures_chunk)

        return chunks

    def generate_comprehensive_executive_summary(self, text: str) -> str:
        """Gera um resumo executivo abrangente do documento"""
        lines = text.split('\n')

        # Construir resumo estruturado
        summary_parts = [
            '# MANUAL DE ALTERA√á√ÉO CADASTRAL - ICATU',
            '',
            '## RESUMO EXECUTIVO',
            '',
            'Este manual da ICATU Capitaliza√ß√£o e Vida apresenta os procedimentos completos para altera√ß√£o cadastral de clientes.',
            '',
            '## PRINCIPAIS T√ìPICOS ABORDADOS:',
            '',
            '### 1. QUEM PODE SOLICITAR',
            '‚Ä¢ Somente o titular da ap√≥lice pode solicitar altera√ß√µes cadastrais',
            '‚Ä¢ Para inclus√£o de nome social: tamb√©m permitido procurador, curador ou tutor',
            '‚Ä¢ Respons√°vel legal para menores de idade',
            '',
            '### 2. TIPOS DE ALTERA√á√ïES',
            '‚Ä¢ Documento de identifica√ß√£o',
            '‚Ä¢ Nome completo',
            '‚Ä¢ Estado civil',
            '‚Ä¢ Nome social',
            '‚Ä¢ Endere√ßo, telefone e e-mail',
            '‚Ä¢ CPF e data de nascimento',
            '',
            '### 3. DOCUMENTOS NECESS√ÅRIOS',
            '‚Ä¢ Formul√°rio de altera√ß√£o de dados',
            '‚Ä¢ Documento de identifica√ß√£o com foto',
            '‚Ä¢ Certid√µes espec√≠ficas (casamento, div√≥rcio, √≥bito conforme altera√ß√£o)',
            '‚Ä¢ Comprovante de endere√ßo quando aplic√°vel',
            '',
            '### 4. PROCEDIMENTOS',
            '‚Ä¢ Valida√ß√£o no site da Receita Federal (CPF/data nascimento)',
            '‚Ä¢ Preenchimento de formul√°rio espec√≠fico',
            '‚Ä¢ Assinatura com reconhecimento de firma (quando exigido)',
            '‚Ä¢ Envio via correios ou canais digitais',
            '',
            '### 5. PRAZOS',
            '‚Ä¢ Processamento: at√© 7 dias √∫teis',
            '‚Ä¢ Reflex√£o no sistema: at√© 24 horas',
            '‚Ä¢ Atualiza√ß√£o no Zendesk: at√© 1 hora',
            '',
            '### 6. CANAIS DE ENVIO',
            '‚Ä¢ √Årea do Cliente (para altera√ß√µes simples)',
            '‚Ä¢ Formul√°rio com assinatura digital',
            '‚Ä¢ Correios (caixa postal espec√≠fica)',
            '‚Ä¢ Parceiros espec√≠ficos (conforme orienta√ß√£o)',
            '',
            'Este documento √© essencial para operadores de atendimento que processam solicita√ß√µes de altera√ß√£o cadastral na ICATU.',
        ]

        return '\n'.join(summary_parts)

    def generate_procedures_summary(self, text: str) -> str:
        """Gera resumo detalhado dos procedimentos"""

        procedures_parts = [
            '# PROCEDIMENTOS DE ALTERA√á√ÉO CADASTRAL - ICATU',
            '',
            '## FLUXO GERAL DE ATENDIMENTO',
            '',
            '### PASSO 1: IDENTIFICA√á√ÉO E VALIDA√á√ÉO',
            '‚Ä¢ Confirmar identidade do solicitante',
            '‚Ä¢ Verificar se √© titular ou representante autorizado',
            '‚Ä¢ Validar dados no sistema ICATU',
            '',
            '### PASSO 2: TIPO DE ALTERA√á√ÉO',
            '‚Ä¢ Identificar que tipo de altera√ß√£o ser√° realizada',
            '‚Ä¢ Verificar documentos necess√°rios',
            '‚Ä¢ Orientar sobre procedimentos espec√≠ficos',
            '',
            '### PASSO 3: VALIDA√á√ïES NECESS√ÅRIAS',
            '‚Ä¢ CPF/Data Nascimento: validar na Receita Federal',
            '‚Ä¢ Documentos: verificar autenticidade e validade',
            '‚Ä¢ Assinatura: confirmar necessidade de reconhecimento',
            '',
            '### PASSO 4: PROCESSAMENTO',
            '‚Ä¢ Registrar solicita√ß√£o no sistema',
            '‚Ä¢ Anexar documentos digitalizados',
            '‚Ä¢ Definir prazo de conclus√£o',
            '',
            '### PASSO 5: FINALIZA√á√ÉO',
            '‚Ä¢ Atualizar dados nos sistemas',
            '‚Ä¢ Confirmar altera√ß√£o com cliente',
            '‚Ä¢ Registrar conclus√£o do atendimento',
            '',
            '## PROCEDIMENTOS ESPEC√çFICOS',
            '',
            '### ALTERA√á√ÉO DE NOME',
            '‚Ä¢ Documento necess√°rio: RG ou certid√£o com novo nome',
            '‚Ä¢ Erros simples: corre√ß√£o direta no sistema',
            '‚Ä¢ Altera√ß√µes formais: formul√°rio com reconhecimento',
            '',
            '### ALTERA√á√ÉO DE CPF/DATA NASCIMENTO',
            '‚Ä¢ Sempre validar na Receita Federal',
            '‚Ä¢ Inserir print da valida√ß√£o no sistema',
            "‚Ä¢ Registrar como 'Altera√ß√£o Cadastral Pendente'",
            '',
            '### ALTERA√á√ÉO DE ENDERE√áO/TELEFONE/EMAIL',
            '‚Ä¢ Pode ser feita diretamente no sistema',
            '‚Ä¢ Registro autom√°tico gerado',
            '‚Ä¢ Sincroniza√ß√£o com Zendesk em at√© 24h',
            '',
            '### NOME SOCIAL',
            '‚Ä¢ Base legal: Circular SUSEP 001/2024',
            '‚Ä¢ N√£o necess√°rio documento comprobat√≥rio',
            '‚Ä¢ Pode ser solicitado a qualquer momento',
            '‚Ä¢ Transfer√™ncia assistida para ramal espec√≠fico (V&P)',
            '',
            '## SISTEMAS ENVOLVIDOS',
            '‚Ä¢ ZENDESK: atendimento e registros',
            '‚Ä¢ SISPREV: previd√™ncia',
            '‚Ä¢ MUMPS/SISVIDA: seguros de vida',
            '‚Ä¢ SISCAP: capitaliza√ß√£o',
            '',
            'Este guia serve como refer√™ncia r√°pida para todos os procedimentos de altera√ß√£o cadastral.',
        ]

        return '\n'.join(procedures_parts)

    def generate_executive_summary(self, text: str) -> str:
        """Gera um resumo executivo do documento"""
        # Extrair primeiros par√°grafos significativos
        paragraphs = [
            p.strip() for p in text.split('\n\n') if p.strip() and len(p) > 50
        ]

        summary_parts = [
            'Este manual da ICATU Capitaliza√ß√£o e Vida apresenta os procedimentos para altera√ß√£o cadastral de clientes.',
            '\nPRINCIPAIS T√ìPICOS ABORDADOS:',
        ]

        # Identificar t√≥picos principais
        main_topics = []
        for paragraph in paragraphs[:20]:  # Primeiros 20 par√°grafos
            if any(
                keyword in paragraph.lower()
                for keyword in [
                    'quem pode',
                    'tipos de',
                    'documentos',
                    'procedimento',
                ]
            ):
                clean_para = paragraph.replace('\n', ' ').strip()
                if len(clean_para) < 200:
                    main_topics.append(f'‚Ä¢ {clean_para}')

        summary_parts.extend(main_topics[:8])  # M√°ximo 8 t√≥picos
        summary_parts.append(
            '\nEste documento √© essencial para operadores que processam solicita√ß√µes de altera√ß√£o cadastral.'
        )

        return '\n'.join(summary_parts)

    def generate_keywords_summary(self, text: str) -> str:
        """Gera resumo de palavras-chave e t√≥picos"""
        keywords = self.extract_keywords_from_text(text)

        categorized_keywords = {
            'Solicitantes': ['titular', 'procurador', 'curador', 'tutor'],
            'Documentos': [
                'cpf',
                'rg',
                'certid√£o',
                'formul√°rio',
                'assinatura',
            ],
            'Procedimentos': [
                'altera√ß√£o',
                'atualiza√ß√£o',
                'valida√ß√£o',
                'registro',
            ],
            'Prazos': ['dias', '√∫teis', 'horas', 'prazo'],
            'Sistemas': ['zendesk', 'sisprev', 'mumps', 'sistema'],
            'Canais': ['correio', 'email', 'formul√°rio', 'site'],
        }

        summary_parts = ['PRINCIPAIS CATEGORIAS E TERMOS:\n']

        for category, category_keywords in categorized_keywords.items():
            found_keywords = [
                k
                for k in keywords
                if any(ck in k.lower() for ck in category_keywords)
            ]
            if found_keywords:
                summary_parts.append(
                    f"{category}: {', '.join(found_keywords[:8])}"
                )

        return '\n'.join(summary_parts)

    def extract_keywords_from_text(self, text: str) -> List[str]:
        """Extrai palavras-chave relevantes do texto"""
        # Palavras importantes do dom√≠nio
        domain_keywords = [
            'altera√ß√£o',
            'cadastral',
            'titular',
            'documento',
            'cpf',
            'rg',
            'nome',
            'endere√ßo',
            'telefone',
            'email',
            'formul√°rio',
            'assinatura',
            'prazo',
            'sistema',
            'zendesk',
            'sisprev',
            'procurador',
            'curador',
            'tutor',
        ]

        found_keywords = []
        text_lower = text.lower()

        for keyword in domain_keywords:
            if keyword in text_lower:
                found_keywords.append(keyword)

        # Adicionar palavras frequentes espec√≠ficas
        words = re.findall(r'\b[a-z√°√©√≠√≥√∫√ß√£√µ√¢√™√Æ]{4,}\b', text_lower)
        word_freq = {}
        for word in words:
            if word not in [
                'para',
                'com',
                's√£o',
                'est√°',
                'ter',
                'como',
                'mais',
            ]:
                word_freq[word] = word_freq.get(word, 0) + 1

        frequent_words = [
            word
            for word, freq in sorted(
                word_freq.items(), key=lambda x: x[1], reverse=True
            )[:10]
            if freq > 2
        ]
        found_keywords.extend(frequent_words)

        return list(set(found_keywords))

    def extract_section_keywords(self, section: Dict[str, Any]) -> List[str]:
        """Extrai palavras-chave espec√≠ficas de uma se√ß√£o"""
        content = section['title'] + ' ' + ' '.join(section['content'])
        return self.extract_keywords_from_text(content)

    def extract_section_topics(self, section: Dict[str, Any]) -> List[str]:
        """Extrai t√≥picos espec√≠ficos de uma se√ß√£o"""
        title_lower = section['title'].lower()

        topic_mapping = {
            'solicitantes': ['quem pode', 'titular', 'procurador'],
            'documentos': ['documento', 'cpf', 'rg', 'identifica√ß√£o'],
            'procedimentos': ['procedimento', 'como', 'processo'],
            'prazos': ['prazo', 'tempo', 'dias'],
            'alteracao_cadastral': ['altera√ß√£o', 'cadastral', 'atualiza√ß√£o'],
        }

        topics = []
        for topic, keywords in topic_mapping.items():
            if any(keyword in title_lower for keyword in keywords):
                topics.append(topic)

        return topics

    def generate_context_summary(self, content: str) -> str:
        """Gera resumo contextual do conte√∫do"""
        lines = [l.strip() for l in content.split('\n') if l.strip()]
        if not lines:
            return 'Conte√∫do vazio'

        # Primeira linha significativa
        first_line = lines[0] if lines else ''

        # Identificar pontos-chave
        key_points = []
        for line in lines:
            if any(
                keyword in line.lower()
                for keyword in [
                    'deve',
                    'necess√°rio',
                    'obrigat√≥rio',
                    'importante',
                ]
            ):
                key_points.append(
                    line[:100] + '...' if len(line) > 100 else line
                )

        summary = (
            first_line[:100] + '...' if len(first_line) > 100 else first_line
        )
        if key_points:
            summary += f' | Pontos-chave: {len(key_points)} itens'

        return summary

    def optimize_chunks_for_retrieval(
        self, chunks: List[DocumentChunk]
    ) -> List[DocumentChunk]:
        """Otimiza chunks para melhor recupera√ß√£o pelo RAG com crit√©rios ajustados"""
        logger.info('üîß Otimizando chunks para recupera√ß√£o...')

        optimized_chunks = []

        for chunk in chunks:
            # Crit√©rios mais flex√≠veis para o tamanho m√≠nimo
            chunk_text = chunk.text.strip()

            # Aceitar chunks com pelo menos 100 caracteres OU que contenham informa√ß√µes importantes
            is_important_chunk = any(
                keyword in chunk_text.lower()
                for keyword in [
                    'titular',
                    'solicitar',
                    'documento',
                    'procedimento',
                    'prazo',
                    'altera√ß√£o',
                    'cadastral',
                    'icatu',
                    'formul√°rio',
                    'assinatura',
                    'reconhecimento',
                    'sistema',
                ]
            )

            # Aceitar se tem tamanho adequado OU √© importante
            if len(chunk_text) >= 100 or (
                len(chunk_text) >= 50 and is_important_chunk
            ):
                # Adicionar metadados de busca
                chunk.metadata['search_keywords'] = ' '.join(
                    chunk.metadata.get('keywords', [])
                )
                chunk.metadata['search_topics'] = ' '.join(
                    chunk.metadata.get('topics', [])
                )

                # Normalizar texto para busca
                normalized_text = self.normalize_for_search(chunk.text)
                chunk.metadata['normalized_content'] = normalized_text[
                    :500
                ]  # Primeiros 500 chars

                # Adicionar indicadores de qualidade
                chunk.metadata['has_important_keywords'] = is_important_chunk
                chunk.metadata['content_length'] = len(chunk_text)

                optimized_chunks.append(chunk)
            else:
                logger.debug(
                    f'Chunk pequeno ignorado: {chunk.chunk_id} ({len(chunk_text)} chars)'
                )

        logger.info(
            f'üîß Otimiza√ß√£o conclu√≠da: {len(optimized_chunks)} chunks v√°lidos de {len(chunks)} originais'
        )
        return optimized_chunks

    def normalize_for_search(self, text: str) -> str:
        """Normaliza texto para melhor busca"""
        # Remover caracteres especiais e normalizar
        normalized = re.sub(r'[^\w\s]', ' ', text.lower())
        normalized = re.sub(r'\s+', ' ', normalized)
        return normalized.strip()

    def calculate_coverage(
        self, chunks: List[DocumentChunk], original_text: str
    ) -> float:
        """Calcula a cobertura dos chunks em rela√ß√£o ao texto original"""
        total_chunk_chars = sum(len(chunk.text) for chunk in chunks)
        original_chars = len(original_text)
        return (
            (total_chunk_chars / original_chars) * 100
            if original_chars > 0
            else 0
        )

    def analyze_document_structure(self, text: str) -> List[Dict]:
        """Analisa a estrutura l√≥gica do documento e identifica se√ß√µes"""
        import re

        sections = []
        lines = text.split('\n')
        current_section = {
            'title': 'Introdu√ß√£o',
            'type': 'intro',
            'content': '',
            'topics': [],
            'keywords': [],
            'summary': '',
        }

        title_patterns = [
            r'^#{1,3}\s+(.+)',  # Markdown headers
            r'^(\d+\.)\s+(.+)',  # Numbered sections
            r'^([A-Z][^a-z]*):?\s*$',  # ALL CAPS titles
            r'^(.+):\s*$',  # Title with colon
            r'^([a-z]\))\s+(.+)',  # Letter enumeration
        ]

        for line_num, line in enumerate(lines):
            line = line.strip()
            if not line:
                continue

            # Detectar in√≠cio de nova se√ß√£o
            is_new_section = False
            section_title = ''
            section_type = 'content'

            for pattern in title_patterns:
                match = re.match(pattern, line, re.IGNORECASE)
                if match:
                    if pattern.startswith(r'^#{1,3}'):  # Markdown header
                        section_title = match.group(1).strip()
                        section_type = 'header'
                    elif pattern.startswith(r'^(\d+\.)'):  # Numbered
                        section_title = (
                            f'{match.group(1)} {match.group(2)}'.strip()
                        )
                        section_type = 'numbered'
                    elif pattern.startswith(r'^([A-Z]'):  # ALL CAPS
                        section_title = line.strip(':')
                        section_type = 'emphasis'
                    elif pattern.startswith(r'^(.+):'):  # With colon
                        section_title = match.group(1).strip()
                        section_type = 'topic'
                    elif pattern.startswith(r'^([a-z]\))'):  # Letter enum
                        section_title = (
                            f'{match.group(1)} {match.group(2)}'.strip()
                        )
                        section_type = 'enumeration'

                    is_new_section = True
                    break

            if is_new_section and current_section['content'].strip():
                # Finalizar se√ß√£o anterior
                current_section['summary'] = self.generate_section_summary(
                    current_section['content']
                )
                current_section['keywords'] = self.extract_keywords(
                    current_section['content']
                )
                current_section['topics'] = self.extract_topics(
                    current_section['content']
                )
                sections.append(current_section)

                # Iniciar nova se√ß√£o
                current_section = {
                    'title': section_title,
                    'type': section_type,
                    'content': line + '\n',
                    'topics': [],
                    'keywords': [],
                    'summary': '',
                }
            else:
                # Adicionar linha √† se√ß√£o atual
                current_section['content'] += line + '\n'

        # Adicionar √∫ltima se√ß√£o
        if current_section['content'].strip():
            current_section['summary'] = self.generate_section_summary(
                current_section['content']
            )
            current_section['keywords'] = self.extract_keywords(
                current_section['content']
            )
            current_section['topics'] = self.extract_topics(
                current_section['content']
            )
            sections.append(current_section)

        return sections

    def split_large_section(
        self, section: Dict, filename: str, section_index: int
    ) -> List[DocumentChunk]:
        """Divide se√ß√µes muito grandes mantendo contexto"""
        content = section['content']
        chunks = []

        # Dividir por par√°grafos l√≥gicos
        paragraphs = content.split('\n\n')
        current_chunk = ''
        chunk_count = 0

        for para in paragraphs:
            if (
                len(current_chunk + para) > 1800
            ):  # Limite menor para garantir contexto
                if current_chunk.strip():
                    # Criar chunk com contexto da se√ß√£o
                    chunk_id = f'{filename}_section_{section_index:03d}_part_{chunk_count:02d}'
                    chunk = DocumentChunk(
                        chunk_id=chunk_id,
                        text=f"# {section['title']}\n\n{current_chunk.strip()}",
                        embedding=[],
                        metadata={
                            'filename': filename,
                            'section_title': section['title'],
                            'section_type': section['type'],
                            'section_index': section_index,
                            'part_index': chunk_count,
                            'is_continuation': chunk_count > 0,
                            'topics': section['topics'],
                            'keywords': self.extract_keywords(current_chunk),
                            'context_summary': self.generate_section_summary(
                                current_chunk
                            ),
                        },
                    )
                    chunks.append(chunk)
                    chunk_count += 1
                    current_chunk = para + '\n\n'
            else:
                current_chunk += para + '\n\n'

        # √öltimo chunk
        if current_chunk.strip():
            chunk_id = f'{filename}_section_{section_index:03d}_part_{chunk_count:02d}'
            chunk = DocumentChunk(
                chunk_id=chunk_id,
                text=f"# {section['title']}\n\n{current_chunk.strip()}",
                embedding=[],
                metadata={
                    'filename': filename,
                    'section_title': section['title'],
                    'section_type': section['type'],
                    'section_index': section_index,
                    'part_index': chunk_count,
                    'is_continuation': chunk_count > 0,
                    'topics': section['topics'],
                    'keywords': self.extract_keywords(current_chunk),
                    'context_summary': self.generate_section_summary(
                        current_chunk
                    ),
                },
            )
            chunks.append(chunk)

        return chunks

    def generate_section_summary(self, content: str) -> str:
        """Gera resumo inteligente da se√ß√£o para facilitar busca"""
        lines = content.strip().split('\n')
        if not lines:
            return ''

        # Pegar primeiras e √∫ltimas linhas significativas
        significant_lines = [
            l.strip() for l in lines if l.strip() and len(l.strip()) > 10
        ]

        if len(significant_lines) <= 3:
            return ' '.join(significant_lines)

        # Resumo baseado em conte√∫do
        summary_parts = []

        # Primeira linha (geralmente t√≠tulo ou contexto)
        if significant_lines:
            summary_parts.append(significant_lines[0])

        # Identificar pontos-chave
        keywords = [
            'necess√°rio',
            'obrigat√≥rio',
            'deve',
            'pode',
            'n√£o',
            'sim',
            'importante',
            'aten√ß√£o',
        ]
        key_lines = []
        for line in significant_lines[1:]:
            if any(keyword in line.lower() for keyword in keywords):
                key_lines.append(line)

        if key_lines:
            summary_parts.extend(key_lines[:2])  # M√°ximo 2 linhas-chave

        return ' | '.join(summary_parts)

    def extract_keywords(self, content: str) -> List[str]:
        """Extrai palavras-chave relevantes para busca"""
        import re

        # Palavras importantes do dom√≠nio ICATU
        domain_keywords = [
            'altera√ß√£o',
            'cadastral',
            'cliente',
            'documento',
            'cpf',
            'rg',
            'nome',
            'endere√ßo',
            'telefone',
            'email',
            'social',
            'titular',
            'ap√≥lice',
            'formul√°rio',
            'assinatura',
            'reconhecimento',
            'firma',
            'protocolo',
            'prazo',
            'dias',
            '√∫teis',
            'correio',
            'envelope',
            'caixa postal',
            'sistema',
            'qdrant',
            'zendesk',
            'sisprev',
            'mumps',
            'sisvida',
            'receita federal',
            'valida√ß√£o',
            'autentica√ß√£o',
            'token',
            'score',
            'pendente',
            'conclu√≠do',
            'manifesta√ß√£o',
            'solicita√ß√£o',
            'atualiza√ß√£o',
        ]

        found_keywords = []
        content_lower = content.lower()

        for keyword in domain_keywords:
            if keyword in content_lower:
                found_keywords.append(keyword)

        # Adicionar palavras espec√≠ficas do conte√∫do
        words = re.findall(r'\b[a-z√°√©√≠√≥√∫√ß√£√µ√¢√™√Æ]{4,}\b', content_lower)
        word_freq = {}
        for word in words:
            if word not in [
                'para',
                'como',
                'deve',
                'ser√°',
                'onde',
                'pelo',
                'pela',
                'esta',
                'este',
            ]:
                word_freq[word] = word_freq.get(word, 0) + 1

        # Adicionar palavras mais frequentes
        frequent_words = sorted(
            word_freq.items(), key=lambda x: x[1], reverse=True
        )[:5]
        found_keywords.extend(
            [word for word, freq in frequent_words if freq > 1]
        )

        return list(set(found_keywords))

    def extract_topics(self, content: str) -> List[str]:
        """Extrai t√≥picos principais da se√ß√£o"""
        topics = []

        # Mapear conte√∫do para t√≥picos
        topic_mapping = {
            'cpf': ['cpf', 'receita federal', 'valida√ß√£o'],
            'documento': ['rg', 'documento', 'identifica√ß√£o', 'certid√£o'],
            'nome': ['nome', 'social', 'altera√ß√£o'],
            'endere√ßo': ['endere√ßo', 'correspond√™ncia', 'resid√™ncia'],
            'telefone': ['telefone', 'celular', 'm√≥vel'],
            'email': ['email', 'eletr√¥nico'],
            'procedimento': ['formul√°rio', 'assinatura', 'reconhecimento'],
            'prazo': ['prazo', 'dias', '√∫teis', 'horas'],
            'sistema': ['sistema', 'zendesk', 'sisprev', 'mumps'],
            'envio': ['correio', 'envelope', 'caixa postal'],
            'solicitante': ['titular', 'procurador', 'curador', 'tutor'],
            'valida√ß√£o': ['autentica√ß√£o', 'token', 'score', 'rating'],
        }

        content_lower = content.lower()
        for topic, keywords in topic_mapping.items():
            if any(keyword in content_lower for keyword in keywords):
                topics.append(topic)

        return topics

    def advanced_text_cleaning(self, text: str) -> str:
        """Limpeza b√°sica de texto - mantida por compatibilidade"""
        return self.advanced_text_cleaning_preserving_structure(text)

    def chunk_text(self, text: str, filename: str) -> List[DocumentChunk]:
        """
        CHUNKING INTELIGENTE BASEADO EM ESTRUTURA L√ìGICA

        NOVA ESTRAT√âGIA IMPLEMENTADA:
        1. üß† An√°lise estrutural do documento (t√≠tulos, se√ß√µes, t√≥picos)
        2. üßπ Limpeza avan√ßada de texto e corre√ß√£o de encoding
        3. ÔøΩ Chunking baseado em se√ß√µes l√≥gicas completas
        4. üè∑Ô∏è Metadados ricos com keywords e sum√°rios
        5. ÔøΩ Facilita√ß√£o da busca pelo MISTRAL
        6. ‚úÖ Preserva√ß√£o completa do contexto
        """
        logger.info(
            f'üß† Starting intelligent structural chunking for {filename}'
        )
        logger.info(f'ÔøΩ Input text length: {len(text)} characters')
        logger.info(f'üìÑ Text preview: {text[:300]}...')

        # Usar novo sistema de chunking estrutural
        chunks = self.smart_structural_chunking(text, filename)

        if not chunks:
            logger.warning('‚ö†Ô∏è Structural chunking failed, using fallback...')
            # Fallback para m√©todo anterior se necess√°rio
            chunks = self.fallback_chunking(text, filename)

        logger.info(
            f'‚úÖ Chunking complete: {len(chunks)} intelligent chunks created'
        )

        return chunks

    def fallback_chunking(
        self, text: str, filename: str
    ) -> List[DocumentChunk]:
        """M√©todo de fallback se o chunking estrutural falhar"""
        logger.info('ÔøΩ Using fallback chunking method')

        # Limpeza b√°sica
        clean_text = self.advanced_text_cleaning(text)

        # Divis√£o simples por par√°grafos
        paragraphs = [p.strip() for p in clean_text.split('\n\n') if p.strip()]

        chunks = []
        current_chunk = ''
        chunk_index = 0

        for para in paragraphs:
            if len(current_chunk + para) > 1500:
                if current_chunk.strip():
                    chunk_id = f'{filename}_fallback_{chunk_index:03d}'
                    chunk = DocumentChunk(
                        chunk_id=chunk_id,
                        text=current_chunk.strip(),
                        embedding=[],
                        metadata={
                            'filename': filename,
                            'chunk_index': chunk_index,
                            'method': 'fallback',
                            'keywords': self.extract_keywords(current_chunk),
                            'topics': self.extract_topics(current_chunk),
                        },
                    )
                    chunks.append(chunk)
                    chunk_index += 1
                    current_chunk = para + '\n\n'
            else:
                current_chunk += para + '\n\n'

        # √öltimo chunk
        if current_chunk.strip():
            chunk_id = f'{filename}_fallback_{chunk_index:03d}'
            chunk = DocumentChunk(
                chunk_id=chunk_id,
                text=current_chunk.strip(),
                embedding=[],
                metadata={
                    'filename': filename,
                    'chunk_index': chunk_index,
                    'method': 'fallback',
                    'keywords': self.extract_keywords(current_chunk),
                    'topics': self.extract_topics(current_chunk),
                },
            )
            chunks.append(chunk)

        return chunks

    async def generate_embeddings(
        self, chunks: List[str]
    ) -> List[List[float]]:
        """Generate embeddings for text chunks using SentenceTransformers"""
        logger.info(f'Gerando embeddings para {len(chunks)} chunks...')
        embeddings = await self.embedding_model.embed_texts(chunks)
        logger.info(f'Embeddings gerados com sucesso.')
        return embeddings

    async def store_in_qdrant(
        self,
        chunks: List[DocumentChunk],
        embeddings: List[List[float]],
        document_id: str,
    ):
        """Store chunks and embeddings in Qdrant with correct format"""
        points = []
        point_ids = []

        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            # Validar embedding
            if (
                not isinstance(embedding, (list, tuple))
                or len(embedding) != self.embedding_model.embedding_dim
            ):
                logger.error(
                    f"‚ùå Embedding inv√°lido para chunk {chunk.chunk_id}: dimens√£o {len(embedding) if embedding else 'None'}"
                )
                continue

            # Gerar ID √∫nico se necess√°rio
            point_id = (
                str(uuid.uuid4()) if not chunk.chunk_id else chunk.chunk_id
            )
            point_ids.append(point_id)

            # Preparar payload com metadados serializ√°veis
            payload = {
                'content': chunk.text,
                'document_id': document_id,
                'filename': chunk.metadata.get('filename', ''),
                'section_title': chunk.metadata.get('section_title', ''),
                'section_type': chunk.metadata.get('section_type', ''),
                'section_index': chunk.metadata.get('section_index', 0),
                'hierarchical_level': chunk.metadata.get(
                    'hierarchical_level', 1
                ),
                'character_count': len(chunk.text),
                'token_count': len(chunk.text.split()),
                'source': 'pdf',
                'content_type': 'document',
                'processing_timestamp': time.time(),
            }

            # Adicionar keywords e topics como strings
            if 'keywords' in chunk.metadata and chunk.metadata['keywords']:
                payload['keywords'] = (
                    ','.join(chunk.metadata['keywords'])
                    if isinstance(chunk.metadata['keywords'], list)
                    else str(chunk.metadata['keywords'])
                )

            if 'topics' in chunk.metadata and chunk.metadata['topics']:
                payload['topics'] = (
                    ','.join(chunk.metadata['topics'])
                    if isinstance(chunk.metadata['topics'], list)
                    else str(chunk.metadata['topics'])
                )

            # Adicionar outros metadados importantes
            for key in [
                'context_summary',
                'search_keywords',
                'search_topics',
                'normalized_content',
            ]:
                if key in chunk.metadata and chunk.metadata[key]:
                    payload[key] = str(chunk.metadata[key])

            # Criar ponto no formato correto para Qdrant
            point = models.PointStruct(
                id=point_id, vector=embedding, payload=payload
            )
            points.append(point)

        if not points:
            raise ValueError(
                'Nenhum ponto v√°lido foi criado para armazenamento'
            )

        # Armazenar no Qdrant
        try:
            self.qdrant_client.upsert(
                collection_name=COLLECTION_NAME, points=points, wait=True
            )
            logger.info(
                f'‚úÖ Stored {len(points)} chunks in Qdrant for document {document_id}. IDs: {point_ids}'
            )
        except Exception as e:
            logger.error(f'‚ùå Erro ao armazenar no Qdrant: {e}')
            raise

        # Padr√µes para identificar se√ß√µes e estruturas
        section_pattern = re.compile(
            r'^(\d+[\.\)]\s*|[IVX]+[\.\)]\s*|[A-Z][A-Z\s\d\-]{10,}:?\s*|\w+\s*:)$|^[a-z][\)\.]',
            re.MULTILINE,
        )
        title_pattern = re.compile(r'^[A-Z][A-Z\s\d\-]{8,}$')
        list_pattern = re.compile(r'^\s*[‚Ä¢¬∑*-]\s*|^\s*\d+[\.\)]\s*')

        def categorize_content(content: str) -> List[str]:
            """Categoriza o conte√∫do do chunk de forma mais precisa"""
            categories = []
            content_lower = content.lower()

            for category, pattern in patterns.items():
                if pattern.search(content_lower):
                    categories.append(category)

            # Categoriza√ß√£o adicional baseada em estrutura
            if list_pattern.search(content):
                categories.append('lista')
            if any(
                word in content_lower
                for word in ['passo', 'etapa', 'primeiro', 'segundo']
            ):
                categories.append('sequencial')
            if any(
                word in content_lower
                for word in ['aten√ß√£o', 'importante', 'observa√ß√£o', 'nota']
            ):
                categories.append('destaque')

            if not categories:
                categories.append('geral')

            return categories

        def create_chunk_with_context(
            text: str,
            section: str,
            categories: List[str],
            chunk_idx: int,
            section_idx: int = 0,
            para_idx: int = 0,
            has_overlap: bool = False,
        ) -> DocumentChunk:
            """Cria chunk com metadados ricos e contexto estrutural"""
            token_count = len(tokenizer.encode(text, add_special_tokens=False))

            return DocumentChunk(
                chunk_id=str(uuid.uuid4()),
                text=text.strip(),
                embedding=[],
                metadata={
                    'filename': filename,
                    'section': section,
                    'categories': categories,
                    'section_index': section_idx,
                    'paragraph_index': para_idx,
                    'chunk_index': chunk_idx,
                    'source': 'pdf',
                    'content_type': 'document',
                    'token_count': token_count,
                    'character_count': len(text),
                    'has_overlap': has_overlap,
                    'processing_timestamp': time.time(),
                    'priority_score': len(categories) * 0.1
                    + (1.0 if 'importante' in text.lower() else 0.0),
                    'density_score': token_count
                    / max(len(text), 1),  # Densidade de informa√ß√£o
                },
            )

        # ESTRAT√âGIA 1: Processamento por par√°grafos com overlap inteligente
        paragraphs = text.split('\n\n')
        logger.info(f'üìù Split into {len(paragraphs)} paragraphs')

        # Se n√£o h√° par√°grafos bem definidos, dividir por quebras simples
        if len(paragraphs) == 1:
            paragraphs = text.split('\n')
            logger.info(f'üìù Fallback: Split into {len(paragraphs)} lines')

            # Se ainda h√° poucas quebras, for√ßar mais divis√µes
            if len(paragraphs) <= 3:
                logger.info(f'üîß For√ßando divis√µes adicionais...')
                # Dividir por padr√µes estruturais
                additional_splits = []
                for para in paragraphs:
                    # Dividir por numera√ß√£o
                    parts = re.split(r'(\d+\.[A-Z])', para)
                    for part in parts:
                        if part.strip():
                            # Dividir por subdivis√µes a), b), c)
                            subparts = re.split(r'([a-z]\))', part)
                            additional_splits.extend(
                                [p.strip() for p in subparts if p.strip()]
                            )

                if len(additional_splits) > len(paragraphs):
                    paragraphs = additional_splits
                    logger.info(f'üìù Structural split: {len(paragraphs)} parts')

        # Se ainda assim h√° muito pouco conte√∫do, usar chunking mais agressivo
        if len(paragraphs) <= 2 and len(text) > MIN_CHUNK_SIZE:
            logger.warning(
                f'‚ö†Ô∏è Very few paragraphs ({len(paragraphs)}), using aggressive sentence-based chunking'
            )
            # Dividir por senten√ßas com chunks menores
            import re

            sentences = re.split(r'(?<=[.!?])\s+', text)
            logger.info(f'üìù Split into {len(sentences)} sentences')

            # Reagrupar senten√ßas em chunks MENORES para mais granularidade
            chunks = []
            current_chunk = ''
            chunk_index = 0
            target_chunk_size = CHUNK_SIZE // 2  # Chunks menores: 1024 tokens

            for sent in sentences:
                sent = sent.strip()
                if not sent:
                    continue

                # Verificar se adicionar a senten√ßa n√£o excede o limite MENOR
                test_chunk = (
                    current_chunk + ' ' + sent if current_chunk else sent
                )
                tokens = len(
                    tokenizer.encode(test_chunk, add_special_tokens=False)
                )

                if tokens > target_chunk_size and current_chunk:
                    # Salvar chunk atual
                    if len(current_chunk.strip()) >= MIN_CHUNK_SIZE:
                        categories = categorize_content(current_chunk)
                        chunk = create_chunk_with_context(
                            current_chunk,
                            'SENTENCE_BASED_SMALL',
                            categories,
                            chunk_index,
                            0,
                            0,
                        )
                        chunks.append(chunk)
                        logger.info(
                            f'‚úÖ Created small sentence-based chunk {chunk_index}: {len(current_chunk)} chars, {tokens} tokens'
                        )
                        chunk_index += 1
                    current_chunk = sent
                else:
                    current_chunk = test_chunk

            # Adicionar √∫ltimo chunk
            if current_chunk and len(current_chunk.strip()) >= MIN_CHUNK_SIZE:
                categories = categorize_content(current_chunk)
                chunk = create_chunk_with_context(
                    current_chunk,
                    'SENTENCE_BASED_SMALL',
                    categories,
                    chunk_index,
                    0,
                    0,
                )
                chunks.append(chunk)
                logger.info(
                    f'‚úÖ Created final small sentence-based chunk {chunk_index}: {len(current_chunk)} chars'
                )

            # Se ainda temos poucos chunks, dividir os maiores
            if len(chunks) <= 3:
                logger.warning(
                    f'‚ö†Ô∏è Still few chunks ({len(chunks)}), splitting large ones...'
                )
                expanded_chunks = []
                for i, chunk in enumerate(chunks):
                    if len(chunk.text) > CHUNK_SIZE:
                        # Dividir chunk grande em peda√ßos menores
                        words = chunk.text.split()
                        words_per_chunk = (
                            len(words) // 3
                        )  # Dividir em 3 partes

                        for j in range(0, len(words), words_per_chunk):
                            sub_words = words[j : j + words_per_chunk]
                            if sub_words:
                                sub_text = ' '.join(sub_words)
                                if len(sub_text.strip()) >= MIN_CHUNK_SIZE:
                                    sub_categories = categorize_content(
                                        sub_text
                                    )
                                    sub_chunk = create_chunk_with_context(
                                        sub_text,
                                        f'SPLIT_CHUNK_{i}_{j//words_per_chunk}',
                                        sub_categories,
                                        len(expanded_chunks),
                                        0,
                                        0,
                                    )
                                    expanded_chunks.append(sub_chunk)
                                    logger.info(
                                        f'‚úÖ Split chunk {i} part {j//words_per_chunk}: {len(sub_text)} chars'
                                    )
                    else:
                        expanded_chunks.append(chunk)

                if len(expanded_chunks) > len(chunks):
                    chunks = expanded_chunks
                    logger.info(
                        f'üîß Chunk splitting successful: {len(chunks)} total chunks'
                    )

            logger.info(
                f'üéØ Aggressive sentence-based chunking created {len(chunks)} chunks'
            )
            return chunks

        chunks = []
        current_section = 'DOCUMENTO_PRINCIPAL'
        section_index = 0
        chunk_index = 0

        # Buffer para constru√ß√£o de chunks
        chunk_buffer = ''
        chunk_tokens = 0
        previous_chunk_text = ''  # Para overlap

        doc_logger.info(
            f'üîç Starting paragraph-based chunking for {filename}: {len(paragraphs)} paragraphs'
        )

        non_empty_paragraphs = 0
        for para_idx, paragraph in enumerate(paragraphs):
            paragraph = paragraph.strip()
            if not paragraph:
                continue

            non_empty_paragraphs += 1
            if para_idx < 5:  # Log first 5 paragraphs for debugging
                logger.info(
                    f'üìÑ Paragraph {para_idx}: {len(paragraph)} chars - {paragraph[:100]}...'
                )

            # Detecta nova se√ß√£o baseada em padr√µes
            if section_pattern.match(paragraph) or title_pattern.match(
                paragraph
            ):
                logger.info(f'üè∑Ô∏è Detected new section: {paragraph[:50]}...')
                # Finaliza chunk atual se existir
                if (
                    chunk_buffer
                    and len(chunk_buffer.strip()) >= MIN_CHUNK_SIZE
                ):
                    categories = categorize_content(chunk_buffer)
                    chunk = create_chunk_with_context(
                        chunk_buffer,
                        current_section,
                        categories,
                        chunk_index,
                        section_index,
                        para_idx,
                    )
                    chunks.append(chunk)
                    logger.info(
                        f'‚úÖ Created section-end chunk {chunk_index}: {len(chunk_buffer)} chars, categories: {categories}'
                    )
                    previous_chunk_text = (
                        chunk_buffer[-CHUNK_OVERLAP:]
                        if len(chunk_buffer) > CHUNK_OVERLAP
                        else chunk_buffer
                    )
                    chunk_index += 1

                # Inicia nova se√ß√£o
                current_section = (
                    paragraph[:100] + '...'
                    if len(paragraph) > 100
                    else paragraph
                )
                section_index += 1
                chunk_buffer = ''
                chunk_tokens = 0
                continue

            # Calcula tokens do par√°grafo
            para_tokens = len(
                tokenizer.encode(paragraph, add_special_tokens=False)
            )

            # ESTRAT√âGIA 2: Gerenciamento inteligente de tamanho de chunk
            if chunk_tokens + para_tokens > CHUNK_SIZE:
                # Salva chunk atual
                if (
                    chunk_buffer
                    and len(chunk_buffer.strip()) >= MIN_CHUNK_SIZE
                ):
                    # Adiciona overlap da chunk anterior se dispon√≠vel
                    full_chunk_text = chunk_buffer
                    if previous_chunk_text and not chunk_buffer.startswith(
                        previous_chunk_text[-100:]
                    ):
                        overlap_text = previous_chunk_text[
                            -CHUNK_OVERLAP // 2 :
                        ].strip()
                        if overlap_text:
                            full_chunk_text = (
                                overlap_text + ' [CONTINUA√á√ÉO] ' + chunk_buffer
                            )

                    categories = categorize_content(full_chunk_text)
                    chunk = create_chunk_with_context(
                        full_chunk_text,
                        current_section,
                        categories,
                        chunk_index,
                        section_index,
                        para_idx,
                        has_overlap=bool(previous_chunk_text),
                    )
                    chunks.append(chunk)

                    # Prepara overlap para pr√≥ximo chunk
                    previous_chunk_text = chunk_buffer
                    chunk_index += 1

                # ESTRAT√âGIA 3: Divis√£o inteligente de par√°grafos grandes
                if para_tokens > CHUNK_SIZE:
                    # Divide par√°grafo por senten√ßas
                    sentences = re.split(r'(?<=[.!?])\s+', paragraph)
                    sub_chunk = ''
                    sub_tokens = 0

                    for sent in sentences:
                        sent = sent.strip()
                        if not sent:
                            continue

                        sent_tokens = len(
                            tokenizer.encode(sent, add_special_tokens=False)
                        )

                        if sub_tokens + sent_tokens > CHUNK_SIZE and sub_chunk:
                            # Salva sub-chunk
                            if len(sub_chunk.strip()) >= MIN_CHUNK_SIZE:
                                # Adiciona overlap se dispon√≠vel
                                full_sub_chunk = sub_chunk
                                if previous_chunk_text:
                                    overlap_text = previous_chunk_text[
                                        -CHUNK_OVERLAP // 3 :
                                    ].strip()
                                    if overlap_text:
                                        full_sub_chunk = (
                                            overlap_text
                                            + ' [CONTINUA√á√ÉO] '
                                            + sub_chunk
                                        )

                                categories = categorize_content(full_sub_chunk)
                                chunk = create_chunk_with_context(
                                    full_sub_chunk,
                                    current_section,
                                    categories,
                                    chunk_index,
                                    section_index,
                                    para_idx,
                                    has_overlap=True,
                                )
                                chunks.append(chunk)
                                previous_chunk_text = sub_chunk
                                chunk_index += 1

                            # Inicia novo sub-chunk com overlap
                            overlap_text = (
                                sub_chunk[-CHUNK_OVERLAP // 2 :]
                                if len(sub_chunk) > CHUNK_OVERLAP // 2
                                else ''
                            )
                            sub_chunk = (
                                overlap_text + ' ' + sent
                                if overlap_text
                                else sent
                            )
                            sub_tokens = len(
                                tokenizer.encode(
                                    sub_chunk, add_special_tokens=False
                                )
                            )
                        else:
                            sub_chunk = (
                                sub_chunk + ' ' + sent if sub_chunk else sent
                            )
                            sub_tokens += sent_tokens

                    # Salva √∫ltimo sub-chunk
                    if sub_chunk and len(sub_chunk.strip()) >= MIN_CHUNK_SIZE:
                        categories = categorize_content(sub_chunk)
                        chunk = create_chunk_with_context(
                            sub_chunk,
                            current_section,
                            categories,
                            chunk_index,
                            section_index,
                            para_idx,
                        )
                        chunks.append(chunk)
                        previous_chunk_text = sub_chunk
                        chunk_index += 1

                    chunk_buffer = ''
                    chunk_tokens = 0
                else:
                    # Inicia novo chunk com o par√°grafo atual
                    chunk_buffer = paragraph
                    chunk_tokens = para_tokens
            else:
                # Adiciona par√°grafo ao chunk atual
                chunk_buffer = (
                    chunk_buffer + '\n\n' + paragraph
                    if chunk_buffer
                    else paragraph
                )
                chunk_tokens += para_tokens

        # ESTRAT√âGIA 4: Processa √∫ltimo chunk restante
        if chunk_buffer and len(chunk_buffer.strip()) >= MIN_CHUNK_SIZE:
            # Adiciona overlap se dispon√≠vel
            full_final_chunk = chunk_buffer
            if previous_chunk_text and not chunk_buffer.startswith(
                previous_chunk_text[-100:]
            ):
                overlap_text = previous_chunk_text[
                    -CHUNK_OVERLAP // 2 :
                ].strip()
                if overlap_text:
                    full_final_chunk = (
                        overlap_text + ' [CONTINUA√á√ÉO] ' + chunk_buffer
                    )

            categories = categorize_content(full_final_chunk)
            chunk = create_chunk_with_context(
                full_final_chunk,
                current_section,
                categories,
                chunk_index,
                section_index,
                len(paragraphs),
                has_overlap=bool(previous_chunk_text),
            )
            chunks.append(chunk)

        # ESTRAT√âGIA 5: An√°lise de cobertura e estat√≠sticas
        total_chars = len(text)
        covered_chars = sum(len(chunk.text) for chunk in chunks)
        coverage_ratio = covered_chars / total_chars if total_chars > 0 else 0

        category_stats = {}
        for chunk in chunks:
            for cat in chunk.metadata.get('categories', []):
                category_stats[cat] = category_stats.get(cat, 0) + 1

        # Log detalhado do chunking
        doc_logger.info(f'‚úÖ Chunking hier√°rquico conclu√≠do para {filename}:')
        doc_logger.info(f'   üìä {len(chunks)} chunks gerados')
        doc_logger.info(
            f'   üìè Cobertura: {coverage_ratio:.2%} do documento original'
        )
        doc_logger.info(
            f'   üìù Tamanho m√©dio: {covered_chars // len(chunks) if chunks else 0} caracteres/chunk'
        )
        doc_logger.info(f'   üè∑Ô∏è Categorias: {category_stats}')

        # ESTRAT√âGIA 6: Valida√ß√£o cr√≠tica e fallback robusto
        if len(chunks) < 3 or coverage_ratio < 0.90:
            logger.warning(
                f'‚ö†Ô∏è PROBLEMA CR√çTICO: {len(chunks)} chunks, cobertura {coverage_ratio:.2%}'
            )
            logger.warning(f'üîß Ativando fallback: chunking por for√ßa bruta...')

            # FALLBACK: Chunking por for√ßa bruta - dividir o texto em chunks de tamanho fixo
            fallback_chunks = []
            words = text.split()
            current_chunk_words = []
            current_chunk_tokens = 0
            fallback_chunk_index = 0

            for word in words:
                # Estimar tokens (aproximadamente 1 token = 0.75 palavras para portugu√™s)
                word_tokens = max(1, len(word) // 4)

                if (
                    current_chunk_tokens + word_tokens > CHUNK_SIZE
                    and current_chunk_words
                ):
                    # Criar chunk atual
                    chunk_text = ' '.join(current_chunk_words)
                    if len(chunk_text.strip()) >= MIN_CHUNK_SIZE:
                        categories = categorize_content(chunk_text)
                        fallback_chunk = DocumentChunk(
                            chunk_id=str(uuid.uuid4()),
                            text=chunk_text,
                            embedding=[],
                            metadata={
                                'filename': filename,
                                'section': f'FALLBACK_CHUNK_{fallback_chunk_index}',
                                'categories': categories,
                                'chunk_index': fallback_chunk_index,
                                'source': 'pdf',
                                'content_type': 'document',
                                'token_count': current_chunk_tokens,
                                'character_count': len(chunk_text),
                                'is_fallback': True,
                                'processing_timestamp': time.time(),
                            },
                        )
                        fallback_chunks.append(fallback_chunk)
                        logger.info(
                            f'‚úÖ Fallback chunk {fallback_chunk_index}: {len(chunk_text)} chars'
                        )
                        fallback_chunk_index += 1

                    # Come√ßar novo chunk com overlap
                    overlap_size = min(50, len(current_chunk_words) // 4)
                    current_chunk_words = current_chunk_words[
                        -overlap_size:
                    ] + [word]
                    current_chunk_tokens = word_tokens + overlap_size
                else:
                    current_chunk_words.append(word)
                    current_chunk_tokens += word_tokens

            # Adicionar √∫ltimo chunk
            if current_chunk_words:
                chunk_text = ' '.join(current_chunk_words)
                if len(chunk_text.strip()) >= MIN_CHUNK_SIZE:
                    categories = categorize_content(chunk_text)
                    fallback_chunk = DocumentChunk(
                        chunk_id=str(uuid.uuid4()),
                        text=chunk_text,
                        embedding=[],
                        metadata={
                            'filename': filename,
                            'section': f'FALLBACK_CHUNK_{fallback_chunk_index}',
                            'categories': categories,
                            'chunk_index': fallback_chunk_index,
                            'source': 'pdf',
                            'content_type': 'document',
                            'token_count': current_chunk_tokens,
                            'character_count': len(chunk_text),
                            'is_fallback': True,
                            'processing_timestamp': time.time(),
                        },
                    )
                    fallback_chunks.append(fallback_chunk)
                    logger.info(
                        f'‚úÖ Final fallback chunk {fallback_chunk_index}: {len(chunk_text)} chars'
                    )

            if len(fallback_chunks) > len(chunks):
                logger.info(
                    f'üîß Fallback successful: {len(fallback_chunks)} chunks vs {len(chunks)} original'
                )
                chunks = fallback_chunks
            else:
                logger.warning(
                    f'‚ö†Ô∏è Fallback n√£o melhorou: {len(fallback_chunks)} vs {len(chunks)}'
                )

        # ESTRAT√âGIA 7: √öltimo recurso - garantir pelo menos um chunk
        if len(chunks) == 0:
            logger.error(f'‚ùå CR√çTICO: Nenhum chunk gerado para {filename}!')
            logger.info(f'üÜò Criando chunk de emerg√™ncia com todo o texto...')
            emergency_chunk = DocumentChunk(
                chunk_id=str(uuid.uuid4()),
                text=text[
                    : CHUNK_SIZE * 4
                ],  # Limita para evitar chunks gigantes
                embedding=[],
                metadata={
                    'filename': filename,
                    'section': 'EMERGENCY_COMPLETE_DOCUMENT',
                    'categories': ['geral', 'emergency'],
                    'chunk_index': 0,
                    'source': 'pdf',
                    'is_emergency': True,
                    'character_count': len(text),
                    'processing_timestamp': time.time(),
                },
            )
            chunks = [emergency_chunk]
            logger.info(
                f'üÜò Emergency chunk created: {len(emergency_chunk.text)} chars'
            )

        if len(chunks) == 0:
            doc_logger.error(f'‚ùå Nenhum chunk gerado para {filename}!')
            # Fallback: criar um chunk com todo o texto
            fallback_chunk = DocumentChunk(
                chunk_id=str(uuid.uuid4()),
                text=text[: CHUNK_SIZE * 2],  # Limita tamanho
                embedding=[],
                metadata={
                    'filename': filename,
                    'section': 'FALLBACK_COMPLETE_DOCUMENT',
                    'categories': ['geral', 'fallback'],
                    'chunk_index': 0,
                    'source': 'pdf',
                    'is_fallback': True,
                },
            )
            chunks = [fallback_chunk]

        return chunks

    async def generate_embeddings(
        self, chunks: List[str]
    ) -> List[List[float]]:
        """Generate embeddings for text chunks using SentenceTransformers"""
        logger.info(f'Gerando embeddings para {len(chunks)} chunks...')
        embeddings = await self.embedding_model.embed_texts(chunks)
        logger.info(f'Embeddings gerados com sucesso.')
        return embeddings

    async def store_in_qdrant(
        self,
        chunks: List[DocumentChunk],
        embeddings: List[List[float]],
        document_id: str,
    ):
        """Store chunks and embeddings in Qdrant with correct format"""
        points = []
        point_ids = []

        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            # Validar embedding
            if (
                not isinstance(embedding, (list, tuple))
                or len(embedding) != self.embedding_model.embedding_dim
            ):
                logger.error(
                    f"‚ùå Embedding inv√°lido para chunk {chunk.chunk_id}: dimens√£o {len(embedding) if embedding else 'None'}"
                )
                continue

            # Gerar ID √∫nico se necess√°rio
            point_id = (
                str(uuid.uuid4()) if not chunk.chunk_id else chunk.chunk_id
            )
            point_ids.append(point_id)

            # Preparar payload com metadados serializ√°veis
            payload = {
                'content': chunk.text,
                'document_id': document_id,
                'filename': chunk.metadata.get('filename', ''),
                'section_title': chunk.metadata.get('section_title', ''),
                'section_type': chunk.metadata.get('section_type', ''),
                'section_index': chunk.metadata.get('section_index', 0),
                'hierarchical_level': chunk.metadata.get(
                    'hierarchical_level', 1
                ),
                'character_count': len(chunk.text),
                'token_count': len(chunk.text.split()),
                'source': 'pdf',
                'content_type': 'document',
                'processing_timestamp': time.time(),
            }

            # Adicionar keywords e topics como strings
            if 'keywords' in chunk.metadata and chunk.metadata['keywords']:
                payload['keywords'] = (
                    ','.join(chunk.metadata['keywords'])
                    if isinstance(chunk.metadata['keywords'], list)
                    else str(chunk.metadata['keywords'])
                )

            if 'topics' in chunk.metadata and chunk.metadata['topics']:
                payload['topics'] = (
                    ','.join(chunk.metadata['topics'])
                    if isinstance(chunk.metadata['topics'], list)
                    else str(chunk.metadata['topics'])
                )

            # Adicionar outros metadados importantes
            for key in [
                'context_summary',
                'search_keywords',
                'search_topics',
                'normalized_content',
            ]:
                if key in chunk.metadata and chunk.metadata[key]:
                    payload[key] = str(chunk.metadata[key])

            # Criar ponto no formato correto para Qdrant
            point = models.PointStruct(
                id=point_id, vector=embedding, payload=payload
            )
            points.append(point)

        if not points:
            raise ValueError(
                'Nenhum ponto v√°lido foi criado para armazenamento'
            )

        # Armazenar no Qdrant
        try:
            self.qdrant_client.upsert(
                collection_name=COLLECTION_NAME, points=points, wait=True
            )
            logger.info(
                f'‚úÖ Stored {len(points)} chunks in Qdrant for document {document_id}. IDs: {point_ids}'
            )
        except Exception as e:
            logger.error(f'‚ùå Erro ao armazenar no Qdrant: {e}')
            raise

    async def search_similar_chunks_enhanced(
        self,
        query: str,
        limit: int = 15,
        score_threshold: float = 0.2,
        document_id: str = None,
    ) -> List[SearchResult]:
        """
        BUSCA ULTRA-OTIMIZADA para capturar TODOS os chunks relevantes

        OTIMIZA√á√ïES CR√çTICAS IMPLEMENTADAS:
        1. üéØ Threshold ultra-baixo (0.2) para m√°xima cobertura
        2. üîÑ Busca multi-estrat√©gia com diferentes abordagens
        3. üìä Query expansion com sin√¥nimos ICATU espec√≠ficos
        4. üè∑Ô∏è Busca por categorias e metadados
        5. üîç Busca h√≠brida: sem√¢ntica + lexical
        6. üìà Re-ranking inteligente por relev√¢ncia
        7. üéõÔ∏è Fus√£o de resultados de m√∫ltiplas consultas
        """
        doc_logger.info(
            f"üîç Busca ultra-otimizada para: '{query}' (threshold: {score_threshold}, limit: {limit})"
        )

        # 1. Mapeamento de sin√¥nimos espec√≠ficos ICATU
        icatu_synonyms = {
            'altera√ß√£o cadastral': [
                'mudan√ßa cadastral',
                'atualiza√ß√£o cadastral',
                'modifica√ß√£o cadastral',
            ],
            'solicitar': [
                'fazer solicita√ß√£o',
                'requerer',
                'pedir',
                'solicitar',
            ],
            'procurador': [
                'representante legal',
                'curador',
                'tutor',
                'respons√°vel',
            ],
            'menor de idade': ['menor', 'crian√ßa', 'adolescente'],
            'zendesk': ['sistema', 'plataforma', 'atendimento'],
            'documento': ['documenta√ß√£o', 'arquivo', 'anexo', 'certid√£o'],
            'prazo': ['tempo', 'per√≠odo', 'hor√°rio'],
            'titular': ['segurado', 'benefici√°rio', 'contratante'],
            'como': ['procedimento', 'processo', 'forma de'],
            'pode': ['consegue', '√© poss√≠vel', 'tem permiss√£o'],
        }

        # 2. Expandir query com sin√¥nimos
        expanded_queries = [query]
        query_lower = query.lower()

        for term, synonyms in icatu_synonyms.items():
            if term in query_lower:
                for synonym in synonyms:
                    expanded_query = query_lower.replace(term, synonym)
                    expanded_queries.append(expanded_query)

        # Adicionar varia√ß√µes estruturais
        if 'como' in query_lower:
            expanded_queries.append(
                query_lower.replace('como', 'procedimento para')
            )
            expanded_queries.append(query_lower.replace('como', 'forma de'))

        if 'quem pode' in query_lower:
            expanded_queries.append(
                query_lower.replace('quem pode', 'solicitantes autorizados')
            )
            expanded_queries.append(
                query_lower.replace('quem pode', 'pessoas que podem')
            )

        doc_logger.info(
            f'Queries expandidas: {len(expanded_queries)} varia√ß√µes'
        )

        # 3. Gerar embeddings para todas as varia√ß√µes
        doc_logger.info('Gerando embedding da pergunta...')
        all_embeddings = await self.embedding_model.embed_texts(
            expanded_queries
        )

        # 4. Busca multi-vector com diferentes estrat√©gias
        all_results = []

        for i, query_embedding in enumerate(all_embeddings):
            # Estrat√©gia 1: Busca padr√£o
            search_params = {
                'collection_name': COLLECTION_NAME,
                'query_vector': query_embedding,
                'limit': min(
                    limit * 3, 50
                ),  # Buscar mais resultados inicialmente
                'score_threshold': max(
                    score_threshold * 0.8, 0.1
                ),  # Threshold ainda mais permissivo
            }

            if document_id:
                search_params['query_filter'] = models.Filter(
                    must=[
                        models.FieldCondition(
                            key='document_id',
                            match=models.MatchValue(value=document_id),
                        )
                    ]
                )

            doc_logger.info('Buscando chunks mais relevantes no Qdrant...')
            search_results = self.qdrant_client.search(**search_params)

            # Peso baseado na query (original tem peso maior)
            weight = 1.0 if i == 0 else 0.85

            for result in search_results:
                # Calcular score ajustado com peso e metadados
                adjusted_score = result.score * weight

                # Bonus por categoria relevante
                categories = result.payload.get('categories', [])
                if any(
                    cat
                    in ['alteracao_cadastral', 'solicitantes', 'procedimentos']
                    for cat in categories
                ):
                    adjusted_score *= 1.2

                # Bonus por prioridade do chunk
                priority = result.payload.get('priority_score', 0)
                adjusted_score += priority

                result_obj = SearchResult(
                    content=result.payload['content'],
                    score=adjusted_score,
                    metadata={
                        k: v
                        for k, v in result.payload.items()
                        if k != 'content'
                    },
                )
                all_results.append(result_obj)

        # 5. Deduplicar e ordenar resultados
        seen_content = {}
        unique_results = []

        for result in all_results:
            content_key = result.content[
                :100
            ]  # Usar in√≠cio do conte√∫do como chave

            if (
                content_key not in seen_content
                or result.score > seen_content[content_key].score
            ):
                seen_content[content_key] = result

        unique_results = list(seen_content.values())
        unique_results.sort(key=lambda x: x.score, reverse=True)

        # 6. Aplicar threshold final e limitar resultados
        final_results = [
            r for r in unique_results if r.score >= score_threshold
        ][:limit]

        doc_logger.info(
            f'Busca retornou {len(final_results)} chunks relevantes.'
        )

        # 7. Log detalhado dos resultados para debug
        if final_results:
            doc_logger.info(
                f"Melhores resultados: scores = {[f'{r.score:.3f}' for r in final_results[:3]]}"
            )
        else:
            doc_logger.warning(
                f'Nenhum resultado encontrado com threshold {score_threshold}'
            )

            # Busca de emerg√™ncia com threshold muito baixo
            emergency_search = self.qdrant_client.search(
                collection_name=COLLECTION_NAME,
                query_vector=all_embeddings[0],
                limit=5,
                score_threshold=0.1,
            )

            if emergency_search:
                doc_logger.info(
                    f'Busca de emerg√™ncia encontrou {len(emergency_search)} resultados'
                )
                for result in emergency_search:
                    result_obj = SearchResult(
                        content=result.payload['content'],
                        score=result.score,
                        metadata={
                            k: v
                            for k, v in result.payload.items()
                            if k != 'content'
                        },
                    )
                    final_results.append(result_obj)

        return final_results

    async def search_similar_chunks(
        self,
        query: str,
        limit: int = 15,
        score_threshold: float = 0.2,
        document_id: str = None,
    ) -> List[SearchResult]:
        """Compatibility wrapper for search_similar_chunks_enhanced with optimized defaults"""
        return await self.search_similar_chunks_enhanced(
            query, limit, score_threshold, document_id
        )

    async def get_collection_info(self) -> Dict[str, Any]:
        """Get information about the Qdrant collection"""
        try:
            collection_info = self.qdrant_client.get_collection(
                COLLECTION_NAME
            )
            return {
                'collection_name': COLLECTION_NAME,
                'vectors_count': collection_info.vectors_count,
                'points_count': collection_info.points_count,
                'status': collection_info.status,
            }
        except Exception as e:
            doc_logger.error(f'Error getting collection info: {e}')
            return {'error': str(e)}


# Global processor instance
processor = DocumentProcessor()

from contextlib import asynccontextmanager


@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    await processor.initialize()
    yield
    # Shutdown - cleanup if needed


# FastAPI app
app = FastAPI(
    title='Document Processor API',
    description='PDF processing and embedding generation for knowledge base',
    version='1.0.0',
    lifespan=lifespan,
)


@app.get('/health')
async def health_check():
    return {'status': 'healthy', 'service': 'document-processor'}


@app.post('/upload-pdf', response_model=ProcessingResponse)
async def upload_pdf(file: UploadFile = File(...)):
    """Upload and process a PDF file"""
    if not file.filename.lower().endswith('.pdf'):
        raise HTTPException(
            status_code=400, detail='Only PDF files are supported'
        )

    import time

    start_time = time.time()

    try:
        # Read file content
        pdf_content = await file.read()

        # Generate document ID
        file_hash = hashlib.md5(pdf_content).hexdigest()
        document_id = f'doc_{file_hash[:12]}'

        # Extract text
        logger.info(f'Processing PDF: {file.filename}')
        text = processor.extract_text_from_pdf(pdf_content)

        if not text.strip():
            raise HTTPException(
                status_code=400, detail='No text could be extracted from PDF'
            )

        # Chunk text
        chunks = processor.chunk_text(text, file.filename)
        logger.info(f'Created {len(chunks)} chunks for {file.filename}')

        # Generate embeddings
        embeddings = await processor.generate_embeddings(
            [chunk.text for chunk in chunks]
        )

        # Store in Qdrant
        await processor.store_in_qdrant(chunks, embeddings, document_id)

        processing_time = time.time() - start_time

        return ProcessingResponse(
            document_id=document_id,
            filename=file.filename,
            chunks_count=len(chunks),
            processing_time=processing_time,
        )

    except Exception as e:
        logger.error(f'Error processing PDF {file.filename}: {e}')
        raise HTTPException(
            status_code=500, detail=f'Processing failed: {str(e)}'
        )


from fastapi.responses import JSONResponse


@app.post('/search', response_model=List[SearchResult])
async def search_documents(request: SearchRequest):
    """Search for relevant document chunks, retorna tamb√©m contexto concatenado para o Mistral"""
    try:
        results = await processor.search_similar_chunks_enhanced(
            query=request.query,
            limit=request.limit,
            score_threshold=request.score_threshold,
        )
        contexto_completo = '\n'.join([r.content for r in results])
        return JSONResponse(
            content={
                'chunks': [r.dict() for r in results],
                'contexto_completo': contexto_completo,
            }
        )
    except Exception as e:
        logger.error(f'Search error: {e}')
        raise HTTPException(status_code=500, detail=f'Search failed: {str(e)}')


@app.get('/collections/info')
async def get_collection_info():
    """Get information about the Qdrant collection"""
    return await processor.get_collection_info()


@app.get('/debug/chunks/{document_id}')
async def debug_chunks(document_id: str):
    """Debug endpoint to see all chunks for a document"""
    try:
        # Buscar todos os chunks do documento no Qdrant
        search_results = processor.qdrant_client.scroll(
            collection_name=COLLECTION_NAME,
            scroll_filter=models.Filter(
                must=[
                    models.FieldCondition(
                        key='document_id',
                        match=models.MatchValue(value=document_id),
                    )
                ]
            ),
            limit=100,  # Buscar at√© 100 chunks
            with_payload=True,
            with_vectors=False,
        )

        chunks_info = []
        for point in search_results[0]:
            chunk_info = {
                'chunk_id': point.id,
                'content_preview': point.payload.get('content', '')[:200]
                + '...'
                if len(point.payload.get('content', '')) > 200
                else point.payload.get('content', ''),
                'content_length': len(point.payload.get('content', '')),
                'categories': point.payload.get('categories', []),
                'section': point.payload.get('section', ''),
                'chunk_index': point.payload.get('chunk_index', 0),
                'token_count': point.payload.get('token_count', 0),
                'metadata': {
                    k: v
                    for k, v in point.payload.items()
                    if k not in ['content']
                },
            }
            chunks_info.append(chunk_info)

        # Ordenar por chunk_index
        chunks_info.sort(key=lambda x: x.get('chunk_index', 0))

        return {
            'document_id': document_id,
            'total_chunks': len(chunks_info),
            'chunks': chunks_info,
        }

    except Exception as e:
        logger.error(f'Error debugging chunks for {document_id}: {e}')
        raise HTTPException(status_code=500, detail=f'Debug failed: {str(e)}')


@app.get('/debug/text-extraction/{document_hash}')
async def debug_text_extraction(document_hash: str):
    """Debug endpoint to see raw text extraction (requires re-upload for testing)"""
    # Note: This would require storing original PDFs or re-uploading for debug
    return {
        'message': 'Upload a PDF with ?debug=true to see text extraction details'
    }


@app.get('/qdrant/info')
async def get_qdrant_info():
    """Get comprehensive information about the Qdrant database"""
    try:
        # Informa√ß√µes gerais do Qdrant
        collections = processor.qdrant_client.get_collections()

        # Informa√ß√µes detalhadas da collection principal
        try:
            collection_info = processor.qdrant_client.get_collection(
                COLLECTION_NAME
            )
            collection_exists = True
        except Exception:
            collection_info = None
            collection_exists = False

        # Estat√≠sticas b√°sicas
        total_points = 0
        if collection_exists:
            try:
                count_result = processor.qdrant_client.count(
                    collection_name=COLLECTION_NAME
                )
                total_points = count_result.count
            except Exception as e:
                logger.warning(f'Could not count points: {e}')

        return {
            'qdrant_url': QDRANT_URL,
            'collection_name': COLLECTION_NAME,
            'collection_exists': collection_exists,
            'total_collections': len(collections.collections),
            'collections': [col.name for col in collections.collections],
            'total_points': total_points,
            'collection_info': {
                'status': collection_info.status
                if collection_info
                else 'Not found',
                'vector_size': collection_info.config.params.vectors.size
                if collection_info
                else None,
                'distance': collection_info.config.params.vectors.distance
                if collection_info
                else None,
            }
            if collection_info
            else None,
        }
    except Exception as e:
        logger.error(f'Error getting Qdrant info: {e}')
        raise HTTPException(
            status_code=500, detail=f'Failed to get Qdrant info: {str(e)}'
        )


@app.get('/qdrant/documents')
async def get_all_documents():
    """Get all documents stored in Qdrant with statistics"""
    try:
        # Buscar todos os pontos
        all_points = []
        next_page_offset = None

        while True:
            scroll_result = processor.qdrant_client.scroll(
                collection_name=COLLECTION_NAME,
                limit=100,
                offset=next_page_offset,
                with_payload=True,
                with_vectors=False,
            )

            points, next_page_offset = scroll_result
            all_points.extend(points)

            if next_page_offset is None:
                break

        # Agrupar por documento
        documents = {}
        for point in all_points:
            doc_id = point.payload.get('document_id', 'unknown')
            filename = point.payload.get('filename', 'unknown')

            if doc_id not in documents:
                documents[doc_id] = {
                    'document_id': doc_id,
                    'filename': filename,
                    'chunks': [],
                    'total_chunks': 0,
                    'total_characters': 0,
                    'categories': set(),
                    'sections': set(),
                }

            # Adicionar chunk
            chunk_info = {
                'chunk_id': point.id,
                'content_length': len(point.payload.get('content', '')),
                'content_preview': point.payload.get('content', '')[:100]
                + '...'
                if len(point.payload.get('content', '')) > 100
                else point.payload.get('content', ''),
                'categories': point.payload.get('categories', []),
                'section': point.payload.get('section', ''),
                'chunk_index': point.payload.get('chunk_index', 0),
                'token_count': point.payload.get('token_count', 0),
            }

            documents[doc_id]['chunks'].append(chunk_info)
            documents[doc_id]['total_chunks'] += 1
            documents[doc_id]['total_characters'] += len(
                point.payload.get('content', '')
            )
            documents[doc_id]['categories'].update(
                point.payload.get('categories', [])
            )
            documents[doc_id]['sections'].add(point.payload.get('section', ''))

        # Converter sets para lists para JSON
        for doc in documents.values():
            doc['categories'] = list(doc['categories'])
            doc['sections'] = list(doc['sections'])
            doc['chunks'].sort(key=lambda x: x.get('chunk_index', 0))

        return {
            'total_documents': len(documents),
            'total_chunks': len(all_points),
            'documents': list(documents.values()),
        }

    except Exception as e:
        logger.error(f'Error getting all documents: {e}')
        raise HTTPException(
            status_code=500, detail=f'Failed to get documents: {str(e)}'
        )


@app.get('/qdrant/stats')
async def get_qdrant_statistics():
    """Get detailed statistics about the knowledge base"""
    try:
        # Buscar todos os pontos com payload
        all_points = []
        next_page_offset = None

        while True:
            scroll_result = processor.qdrant_client.scroll(
                collection_name=COLLECTION_NAME,
                limit=100,
                offset=next_page_offset,
                with_payload=True,
                with_vectors=False,
            )

            points, next_page_offset = scroll_result
            all_points.extend(points)

            if next_page_offset is None:
                break

        # Calcular estat√≠sticas
        stats = {
            'total_chunks': len(all_points),
            'total_documents': len(
                set(p.payload.get('document_id', '') for p in all_points)
            ),
            'total_characters': sum(
                len(p.payload.get('content', '')) for p in all_points
            ),
            'average_chunk_size': 0,
            'categories': {},
            'sections': {},
            'document_breakdown': {},
            'chunk_size_distribution': {
                'small (0-500)': 0,
                'medium (500-1500)': 0,
                'large (1500+)': 0,
            },
        }

        if stats['total_chunks'] > 0:
            stats['average_chunk_size'] = (
                stats['total_characters'] // stats['total_chunks']
            )

        # Analisar cada ponto
        for point in all_points:
            # Categorias
            for cat in point.payload.get('categories', []):
                stats['categories'][cat] = stats['categories'].get(cat, 0) + 1

            # Se√ß√µes
            section = point.payload.get('section', 'unknown')
            stats['sections'][section] = stats['sections'].get(section, 0) + 1

            # Documentos
            doc_id = point.payload.get('document_id', 'unknown')
            filename = point.payload.get('filename', 'unknown')
            if doc_id not in stats['document_breakdown']:
                stats['document_breakdown'][doc_id] = {
                    'filename': filename,
                    'chunks': 0,
                    'characters': 0,
                }
            stats['document_breakdown'][doc_id]['chunks'] += 1
            stats['document_breakdown'][doc_id]['characters'] += len(
                point.payload.get('content', '')
            )

            # Distribui√ß√£o de tamanhos
            content_length = len(point.payload.get('content', ''))
            if content_length < 500:
                stats['chunk_size_distribution']['small (0-500)'] += 1
            elif content_length < 1500:
                stats['chunk_size_distribution']['medium (500-1500)'] += 1
            else:
                stats['chunk_size_distribution']['large (1500+)'] += 1

        return stats

    except Exception as e:
        logger.error(f'Error getting Qdrant statistics: {e}')
        raise HTTPException(
            status_code=500, detail=f'Failed to get statistics: {str(e)}'
        )


@app.get('/qdrant/search-test')
async def test_qdrant_search(query: str = 'seguro', limit: int = 5):
    """Test search functionality in Qdrant"""
    try:
        # Verificar se o embedding service est√° inicializado
        if (
            not processor.embedding_service
            or not processor.embedding_service.model
        ):
            await processor.initialize()

        # Gerar embedding da query
        query_embedding = await processor.embedding_service.embed_texts(
            [query]
        )

        # Buscar no Qdrant
        search_results = processor.qdrant_client.search(
            collection_name=COLLECTION_NAME,
            query_vector=query_embedding[0],
            limit=limit,
            with_payload=True,
            with_vectors=False,
        )

        results = []
        for result in search_results:
            results.append(
                {
                    'score': result.score,
                    'chunk_id': result.id,
                    'content_preview': result.payload.get('content', '')[:200]
                    + '...'
                    if len(result.payload.get('content', '')) > 200
                    else result.payload.get('content', ''),
                    'categories': result.payload.get('categories', []),
                    'section': result.payload.get('section', ''),
                    'filename': result.payload.get('filename', ''),
                    'document_id': result.payload.get('document_id', ''),
                }
            )

        return {
            'query': query,
            'total_results': len(results),
            'results': results,
        }

    except Exception as e:
        logger.error(f'Error testing search: {e}')
        raise HTTPException(
            status_code=500, detail=f'Search test failed: {str(e)}'
        )


@app.delete('/qdrant/clear')
async def clear_qdrant_collection():
    """Clear all data from the Qdrant collection (CAUTION!)"""
    try:
        # Deletar a collection
        processor.qdrant_client.delete_collection(COLLECTION_NAME)

        # Recriar a collection
        await processor.ensure_collection_exists()

        return {
            'message': f'Collection {COLLECTION_NAME} cleared and recreated successfully',
            'status': 'success',
        }

    except Exception as e:
        logger.error(f'Error clearing collection: {e}')
        raise HTTPException(
            status_code=500, detail=f'Failed to clear collection: {str(e)}'
        )


@app.get('/qdrant/raw-points')
async def get_raw_points(limit: int = 10, offset: int = 0):
    """Get raw points from Qdrant for detailed inspection"""
    try:
        scroll_result = processor.qdrant_client.scroll(
            collection_name=COLLECTION_NAME,
            limit=limit,
            offset=offset if offset > 0 else None,
            with_payload=True,
            with_vectors=True,  # Include vectors for complete view
        )

        points, next_offset = scroll_result

        raw_points = []
        for point in points:
            raw_points.append(
                {
                    'id': point.id,
                    'vector_size': len(point.vector) if point.vector else 0,
                    'vector_preview': point.vector[:5]
                    if point.vector
                    else None,  # First 5 dimensions
                    'payload': point.payload,
                }
            )

        return {
            'limit': limit,
            'offset': offset,
            'next_offset': next_offset,
            'total_returned': len(raw_points),
            'points': raw_points,
        }

    except Exception as e:
        logger.error(f'Error getting raw points: {e}')
        raise HTTPException(
            status_code=500, detail=f'Failed to get raw points: {str(e)}'
        )


@app.post('/upload-pdf-debug', response_model=Dict[str, Any])
async def upload_pdf_debug(file: UploadFile = File(...)):
    """Upload PDF with detailed debugging information"""
    if not file.filename.lower().endswith('.pdf'):
        raise HTTPException(
            status_code=400, detail='Only PDF files are supported'
        )

    import time

    start_time = time.time()

    try:
        # Read file content
        pdf_content = await file.read()

        # Generate document ID
        file_hash = hashlib.md5(pdf_content).hexdigest()
        document_id = f'doc_{file_hash[:12]}'

        # Extract text with debugging
        logger.info(f'üîç DEBUG: Processing PDF: {file.filename}')
        text = processor.extract_text_from_pdf(pdf_content)

        debug_info = {
            'filename': file.filename,
            'document_id': document_id,
            'file_size_bytes': len(pdf_content),
            'extracted_text_length': len(text),
            'extracted_text_preview': text[:500] + '...'
            if len(text) > 500
            else text,
            'text_empty': not text.strip(),
        }

        if not text.strip():
            debug_info['error'] = 'No text could be extracted from PDF'
            return debug_info

        # Chunk text with debugging
        logger.info(f'üîç DEBUG: Starting chunking process...')
        chunks = processor.chunk_text(text, file.filename)

        debug_info.update(
            {'chunks_generated': len(chunks), 'chunks_details': []}
        )

        # Add detailed chunk information
        for i, chunk in enumerate(chunks):
            chunk_detail = {
                'chunk_index': i,
                'chunk_id': chunk.chunk_id,
                'text_length': len(chunk.text),
                'text_preview': chunk.text[:200] + '...'
                if len(chunk.text) > 200
                else chunk.text,
                'categories': chunk.metadata.get('categories', []),
                'section': chunk.metadata.get('section', ''),
                'token_count': chunk.metadata.get('token_count', 0),
                'has_overlap': chunk.metadata.get('has_overlap', False),
            }
            debug_info['chunks_details'].append(chunk_detail)

        # Generate embeddings (optional for debug)
        logger.info(
            f'üîç DEBUG: Generating embeddings for {len(chunks)} chunks...'
        )
        embeddings = await processor.generate_embeddings(
            [chunk.text for chunk in chunks]
        )

        # Store in Qdrant
        await processor.store_in_qdrant(chunks, embeddings, document_id)

        processing_time = time.time() - start_time
        debug_info['processing_time'] = processing_time
        debug_info['success'] = True

        return debug_info

    except Exception as e:
        logger.error(f'üîç DEBUG: Error processing PDF {file.filename}: {e}')
        return {'filename': file.filename, 'error': str(e), 'success': False}
    try:
        collection_info = processor.qdrant_client.get_collection(
            COLLECTION_NAME
        )
        return {
            'collection_name': COLLECTION_NAME,
            'vectors_count': collection_info.vectors_count,
            'status': collection_info.status,
            'config': {
                'distance': collection_info.config.params.vectors.distance.value,
                'size': collection_info.config.params.vectors.size,
            },
        }
    except Exception as e:
        return {'error': str(e)}


if __name__ == '__main__':
    uvicorn.run(
        'pdf_processor:app', host='0.0.0.0', port=8001, log_level='info'
    )
